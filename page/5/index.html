<!DOCTYPE html>












  


<html class="theme-next gemini use-motion" lang="zh-CN">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2"/>
<meta name="theme-color" content="#222"/>






















<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0"/>

<link rel="stylesheet" href="/css/main.css?v=7.2.0"/>


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.2.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.2.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.2.0">


  <link rel="mask-icon" href="/images/logo.svg?v=7.2.0" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '7.2.0',
    sidebar: {"position":"right","display":"post","offset":12,"onmobile":false},
    back2top: {"enable":true,"sidebar":true,"scrollpercent":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    translation: {
      copy_button: '复制',
      copy_success: '复制成功',
      copy_failure: '复制失败'
    }
  };
</script>



  <meta property="og:type" content="website">
<meta property="og:title" content="ZDK&#39;s blog">
<meta property="og:url" content="https://github.com/zdkswd/page/5/index.html">
<meta property="og:site_name" content="ZDK&#39;s blog">
<meta property="og:locale" content="zh-CN">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="ZDK&#39;s blog">



  <link rel="alternate" href="/atom.xml" title="ZDK's blog" type="application/atom+xml"/>



  
  
  <link rel="canonical" href="https://github.com/zdkswd/page/5/"/>



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>ZDK's blog</title>
  












  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <div class="container sidebar-position-right 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">ZDK's blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br/>首页</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
      
    

    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br/>标签</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">

    
    
      
    

    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br/>分类</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br/>归档</a>

  </li>

      
      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br/>搜索</a>
        </li>
      
    </ul>
  

  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
            

          
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://github.com/zdkswd/2019/04/17/PyTorch get started/"/>

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="ZDK"/>
      <meta itemprop="description" content=""/>
      <meta itemprop="image" content="/images/avatar.gif"/>
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ZDK's blog"/>
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/04/17/PyTorch get started/" class="post-title-link" itemprop="url">PyTorch get started</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-04-17 20:04:47 / 修改时间：20:05:52" itemprop="dateCreated datePublished" datetime="2019-04-17T20:04:47+08:00">2019-04-17</time>
            </span>
          

          
            

            
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/知识总结/" itemprop="url" rel="index"><span itemprop="name">知识总结</span></a></span>

                
                
              
            </span>
          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">评论数：</span>
                <a href="/2019/04/17/PyTorch get started/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2019/04/17/PyTorch get started/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
            <span id="/2019/04/17/PyTorch get started/" class="post-meta-item leancloud_visitors" data-flag-title="PyTorch get started">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              
                <span class="post-meta-item-text">阅读次数：</span>
              
                <span class="leancloud-visitors-count"></span>
            </span>
          

          

          

          <br/>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Tensors"><a href="#Tensors" class="headerlink" title="Tensors"></a>Tensors</h1><p>Tensors are similar to NumPy’s ndarrays, with the addition being that Tensors can also be used on a GPU to accelerate computing.<br><img src="/img/media/PyTorch%20get%20started/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-17%20%E4%B8%8B%E5%8D%886.04.33.png" alt=""><br>Construct a 5x3 matrix, uninitialized:<br><img src="/img/media/PyTorch%20get%20started/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-17%20%E4%B8%8B%E5%8D%886.04.46.png" alt=""><br>Construct a randomly initialized matrix:<br><img src="/img/media/PyTorch%20get%20started/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-17%20%E4%B8%8B%E5%8D%886.05.13.png" alt=""><br>Construct a matrix filled zeros and of dtype long:<br><img src="/img/media/PyTorch%20get%20started/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-17%20%E4%B8%8B%E5%8D%886.05.25.png" alt=""><br>Construct a tensor directly from data:<br><img src="/img/media/PyTorch%20get%20started/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-17%20%E4%B8%8B%E5%8D%886.06.00.png" alt=""><br>or create a tensor based on an existing tensor. These methods will reuse properties of the input tensor, e.g. dtype, unless new values are provided by user<br><img src="/img/media/PyTorch%20get%20started/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-17%20%E4%B8%8B%E5%8D%886.07.08.png" alt=""><br>Get its size:<br><img src="/img/media/PyTorch%20get%20started/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-17%20%E4%B8%8B%E5%8D%886.15.15.png" alt=""><br><strong>Note:</strong><br><img src="/img/media/PyTorch%20get%20started/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-17%20%E4%B8%8B%E5%8D%886.32.30.png" alt=""></p>
<h1 id="Operations"><a href="#Operations" class="headerlink" title="Operations"></a>Operations</h1><p>Addition: syntax 1<br><img src="/img/media/PyTorch%20get%20started/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-17%20%E4%B8%8B%E5%8D%886.33.06.png" alt=""><br>Addition: syntax 2<br><img src="/img/media/PyTorch%20get%20started/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-17%20%E4%B8%8B%E5%8D%886.33.26.png" alt=""><br>Addition: providing an output tensor as argument<br><img src="/img/media/PyTorch%20get%20started/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-17%20%E4%B8%8B%E5%8D%886.34.20.png" alt=""><br>Addition: in-place<br><img src="/img/media/PyTorch%20get%20started/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-17%20%E4%B8%8B%E5%8D%886.35.28.png" alt=""><br><strong>Note:</strong><br><img src="/img/media/PyTorch%20get%20started/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-17%20%E4%B8%8B%E5%8D%886.36.19.png" alt=""><br>You can use standard NumPy-like indexing<br><img src="/img/media/PyTorch%20get%20started/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-17%20%E4%B8%8B%E5%8D%886.36.52.png" alt=""><br>Resizing: If you want to resize/reshape tensor, you can use torch.view:<br><img src="/img/media/PyTorch%20get%20started/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-17%20%E4%B8%8B%E5%8D%886.39.51.png" alt=""><br>Out:<br><img src="/img/media/PyTorch%20get%20started/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-17%20%E4%B8%8B%E5%8D%886.40.04.png" alt=""></p>
<p>If you have a one element tensor, use .item() to get the value as a Python number<br><img src="/img/media/PyTorch%20get%20started/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-17%20%E4%B8%8B%E5%8D%886.40.24.png" alt=""></p>
<h1 id="Numpy-Bridge"><a href="#Numpy-Bridge" class="headerlink" title="Numpy Bridge"></a>Numpy Bridge</h1><p>The Torch Tensor and NumPy array will share their underlying memory locations, and changing one will change the other.</p>
<h2 id="Converting-a-Torch-Tensor-to-a-NumPy-Array"><a href="#Converting-a-Torch-Tensor-to-a-NumPy-Array" class="headerlink" title="Converting a Torch Tensor to a NumPy Array"></a>Converting a Torch Tensor to a NumPy Array</h2><p><img src="/img/media/PyTorch%20get%20started/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-17%20%E4%B8%8B%E5%8D%886.42.30.png" alt=""><br><img src="/img/media/PyTorch%20get%20started/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-17%20%E4%B8%8B%E5%8D%886.42.42.png" alt=""><br><img src="/img/media/PyTorch%20get%20started/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-17%20%E4%B8%8B%E5%8D%886.42.37.png" alt=""><br><img src="/img/media/PyTorch%20get%20started/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-17%20%E4%B8%8B%E5%8D%886.43.09.png" alt=""><br><img src="/img/media/PyTorch%20get%20started/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-17%20%E4%B8%8B%E5%8D%886.43.18.png" alt=""><br>Out:<br><img src="/img/media/PyTorch%20get%20started/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-17%20%E4%B8%8B%E5%8D%886.43.41.png" alt=""></p>
<h2 id="Converting-NumPy-Array-to-Torch-Tensor"><a href="#Converting-NumPy-Array-to-Torch-Tensor" class="headerlink" title="Converting NumPy Array to Torch Tensor"></a>Converting NumPy Array to Torch Tensor</h2><p><img src="/img/media/PyTorch%20get%20started/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-17%20%E4%B8%8B%E5%8D%886.44.25.png" alt=""><br>out:<br><img src="/img/media/PyTorch%20get%20started/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-17%20%E4%B8%8B%E5%8D%886.44.38.png" alt=""><br>All the Tensors on the CPU except a CharTensor support converting to NumPy and back.</p>
<h1 id="CUDA-Tensors"><a href="#CUDA-Tensors" class="headerlink" title="CUDA Tensors"></a>CUDA Tensors</h1><p>Tensors can be moved onto any device using the .to method.<br><img src="/img/media/PyTorch%20get%20started/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-17%20%E4%B8%8B%E5%8D%887.43.19.png" alt=""><br>out:<br><img src="/img/media/PyTorch%20get%20started/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-17%20%E4%B8%8B%E5%8D%887.46.16.png" alt=""></p>
<h1 id="examples"><a href="#examples" class="headerlink" title="examples"></a>examples</h1><p><img src="/img/media/PyTorch%20get%20started/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-17%20%E4%B8%8B%E5%8D%887.58.55.png" alt=""><br><img src="/img/media/PyTorch%20get%20started/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-17%20%E4%B8%8B%E5%8D%887.59.10.png" alt=""></p>
<h1 id="PyTorch-Custom-nn-Modules"><a href="#PyTorch-Custom-nn-Modules" class="headerlink" title="PyTorch: Custom nn Modules"></a>PyTorch: Custom nn Modules</h1><p><img src="/img/media/PyTorch%20get%20started/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-17%20%E4%B8%8B%E5%8D%888.01.01.png" alt=""><br><img src="/img/media/PyTorch%20get%20started/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-17%20%E4%B8%8B%E5%8D%888.01.17.png" alt=""></p>

          
        
      
    </div>

    

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://github.com/zdkswd/2019/04/17/tx往届心得/"/>

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="ZDK"/>
      <meta itemprop="description" content=""/>
      <meta itemprop="image" content="/images/avatar.gif"/>
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ZDK's blog"/>
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/04/17/tx往届心得/" class="post-title-link" itemprop="url">tx往届心得</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-04-17 16:49:47 / 修改时间：10:57:36" itemprop="dateCreated datePublished" datetime="2019-04-17T16:49:47+08:00">2019-04-17</time>
            </span>
          

          
            

            
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/知识总结/" itemprop="url" rel="index"><span itemprop="name">知识总结</span></a></span>

                
                
              
            </span>
          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">评论数：</span>
                <a href="/2019/04/17/tx往届心得/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2019/04/17/tx往届心得/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
            <span id="/2019/04/17/tx往届心得/" class="post-meta-item leancloud_visitors" data-flag-title="tx往届心得">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              
                <span class="post-meta-item-text">阅读次数：</span>
              
                <span class="leancloud-visitors-count"></span>
            </span>
          

          

          

          <br/>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="stacking技术分享"><a href="#stacking技术分享" class="headerlink" title="stacking技术分享"></a>stacking技术分享</h1><p>stacking不能称为一种算法，而是一种对模型的集成策略。在给定数据集的情况下，数据内部的空间结构和数据之间的关系是非常复杂得。不同的模型，其实很重要的一点就是在不同的角度去观测数据集。stacking框架就是用来取长补短进行结合的。</p>
<p>假设是五折的stacking，我们有一个train数据集和一个test数据集，那么一个基本的stacking框架会进行如下几个操作：</p>
<ol>
<li>选择基模型。我们可以有xgboost，lightGBM，RandomForest，SVM，ANN，KNN，LR等等你能想到的各种基本算法模型。</li>
<li>把训练集分为不交叉的五份。我们标记为train1到train5。</li>
<li>从train1开始作为预测集，使用train2到train5建模，然后预测train1，并保留结果；然后，以train2作为预测集，使用train1，train3到train5建模，预测train2，并保留结果；如此进行下去，直到把train1到train5各预测一遍；</li>
<li>把预测的结果按照train1到trian5的位置对应填补上，得到对train整个数据集在第一个基模型的一个stacking转换。</li>
<li>在上述建立的五个模型过程中，每个模型分别对test数据集进行预测，并最终保留这五列结果，然后对这五列取平均，作为第一个基模型对test数据的一个stacking转换。</li>
<li>选择第二个基模型，重复以上2-5操作，再次得到train整个数据集在第二个基模型的一个stacking转换。</li>
<li>以此类推。有几个基模型，就会对整个train数据集生成几列新的特征表达。同样，也会对test有几列新的特征表达。</li>
<li>一般使用LR作为第二层的模型进行建模预测。</li>
</ol>
<p><img src="/img/media/tx%E5%BE%80%E5%B1%8A%E5%BF%83%E5%BE%97/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-02%20%E4%B8%8B%E5%8D%881.32.08.png" alt=""><br>上面这个框架说明的是：对训练数据进行无重复的五次划分之后，分别对其中每一部分进行一次预测，而预测的模型就是由其余四部分训练的；并且在预测了预测集之后，还需要对我们的test数据集也进行一次预测，这这样就会得到5个N/5行、1列的对train数据集的特征转换，和5个M行、1列的对test数据集的特征转换，由此进入下一个图。<br><img src="/img/media/tx%E5%BE%80%E5%B1%8A%E5%BF%83%E5%BE%97/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-02%20%E4%B8%8B%E5%8D%881.33.39.png" alt=""></p>
<h2 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h2><p>1.stacking的框架设计比较复杂，对于一个基模型要训练5次，如果你的一个xgb模型要训练2个小时，即使在进行stacking的时候每折减少了五分之一的数据量，你的计算时间仍然是很可观的，加起来应该还是8-9小时，所以耗费时间很长（想像一下一个stacking框架跑一个基模型要大半天，简直太可怕）。所以建议大家在使用的时候要计算时间的耗费，或者可以改为3折，4折等等；</p>
<p>2、我们前面讲过了，stacking框架是集成了不同的算法，充分利用不同算法从不同的数据空间角度和数据结构角度的对数据的不同观测，来取长补短，优化结果。所以，我们的基模型除了是不同参数的相同模型之外，比如不同参数的xgboost，或者不同K值的KNN等等；更重要的是要尽可能的多加一些不同种类的基模型进去，也就是说所谓的模型要“跨越空间”的概念。这样的话我们的集成结果会更加稳健，更加精确。（曾经有一个比赛集成了上百个基模型的stacking框架获奖）</p>
<h2 id="基本变种改进"><a href="#基本变种改进" class="headerlink" title="基本变种改进"></a>基本变种改进</h2><p>在变种改进方面，我们可以不仅对模型进行融合，还可以对特征级进行一些变化，比如选部分特征做stacking；或者对stacking的结果进行再次的stacking，上面介绍的是两层的stacking，可以有3层，或者更多。但是时间复杂度很高，效果并不一定明显。</p>
<h1 id="Kaggle数据挖掘比赛经验分享"><a href="#Kaggle数据挖掘比赛经验分享" class="headerlink" title="Kaggle数据挖掘比赛经验分享"></a>Kaggle数据挖掘比赛经验分享</h1><p><a href="https://mp.weixin.qq.com/s?__biz=MzIzMzgzOTUxNA==&amp;mid=2247483678&amp;idx=1&amp;sn=5f044dabfaa726e292686287a1dd5ca4&amp;chksm=e8fecfebdf8946fdabf71fd5c4c0e019144f105da993c12fa257c64f281ecfb3a7557f16b79e&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">【干货】Kaggle 数据挖掘比赛经验分享</a><br>一个完整的数据挖掘比赛基本流程如下：<br><img src="/img/media/tx%E5%BE%80%E5%B1%8A%E5%BF%83%E5%BE%97/640.png" alt=""></p>
<h2 id="数据分析"><a href="#数据分析" class="headerlink" title="数据分析"></a>数据分析</h2><p>通过对数据进行探索性分析（甚至有些情况下需要肉眼观察样本），还可以有助于启发数据清洗和特征抽取，譬如缺失值和异常值的处理，文本数据是否需要进行拼写纠正等。</p>
<h3 id="分析特征变量的分布"><a href="#分析特征变量的分布" class="headerlink" title="分析特征变量的分布"></a>分析特征变量的分布</h3><ol>
<li><strong>特征变量</strong>为连续值：如果为长尾分布并且考虑使用线性模型，可以对变量进行幂变换或者对数变换。</li>
<li><strong>特征变量</strong>为离散值：观察每个离散值的频率分布，对于频次较低的特征，可以考虑统一编码为“其他”类别。</li>
</ol>
<h3 id="分析目标变量的分布"><a href="#分析目标变量的分布" class="headerlink" title="分析目标变量的分布"></a>分析目标变量的分布</h3><ol>
<li><strong>目标变量</strong>为连续值：查看其值域范围是否较大，如果较大，可以考虑对其进行对数变换，并以变换后的值作为新的目标变量进行建模（<strong>在这种情况下，需要对预测结果进行逆变换</strong>）。一般情况下，可以对连续变量进行<strong>Box-Cox</strong>变换。通过变换可以使得模型更好的优化，通常也会带来效果上的提升。</li>
<li><strong>目标变量</strong>为离散值：如果数据分布不平衡，考虑是否需要上采样/下采样；如果目标变量在某个ID上面分布不平衡，在划分本地训练集和验证集的时候，需要考虑<strong>分层采样（Stratified Sampling）</strong>。</li>
</ol>
<h3 id="分析变量之间两两的分布和相关度"><a href="#分析变量之间两两的分布和相关度" class="headerlink" title="分析变量之间两两的分布和相关度"></a>分析变量之间两两的分布和相关度</h3><ol>
<li>可以用于发现高相关和共线性的特征。</li>
</ol>
<h2 id="数据清洗"><a href="#数据清洗" class="headerlink" title="数据清洗"></a>数据清洗</h2><p>数据清洗是指对提供的原始数据进行一定的加工，使得其方便后续的特征抽取。其与特征抽取的界限有时也没有那么明确。常用的数据清洗一般包括：</p>
<h3 id="数据的拼接"><a href="#数据的拼接" class="headerlink" title="数据的拼接"></a>数据的拼接</h3><ol>
<li>提供的数据散落在多个文件，需要根据相应的键值进行数据的拼接。</li>
</ol>
<h3 id="特征缺失值的处理"><a href="#特征缺失值的处理" class="headerlink" title="特征缺失值的处理"></a>特征缺失值的处理</h3><ol>
<li>特征值为连续值：按不同的分布类型对缺失值进行补全：<strong>偏正态分布</strong>，使用均值代替，可以保持数据的均值；<strong>偏长尾分布</strong>，使用中值代替，避免受 outlier 的影响；</li>
<li>特征值为离散值：使用众数代替。</li>
</ol>
<h3 id="文本数据的清洗"><a href="#文本数据的清洗" class="headerlink" title="文本数据的清洗"></a>文本数据的清洗</h3><ol>
<li>在比赛当中，如果数据包含文本，往往需要进行大量的数据清洗工作。如去除HTML 标签，分词，拼写纠正, 同义词替换，去除停词，抽词干，数字和单位格式统一等。</li>
</ol>
<h2 id="特征工程"><a href="#特征工程" class="headerlink" title="特征工程"></a>特征工程</h2><p>有一种说法是，特征决定了效果的上限，而不同模型只是以不同的方式或不同的程度来逼近这个上限。这样来看，好的特征输入对于模型的效果至关重要，正所谓”Garbage in, garbage out”。要做好特征工程，往往跟领域知识和对问题的理解程度有很大的关系，也跟一个人的经验相关。特征工程的做法也是Case by Case，以下就一些点，谈谈自己的一些看法。</p>
<h3 id="特征变换"><a href="#特征变换" class="headerlink" title="特征变换"></a>特征变换</h3><p>主要针对一些长尾分布的特征，<strong>需要进行幂变换或者对数变换，使得模型（LR或者DNN）能更好的优化</strong>。需要注意的是，Random Forest 和 GBDT 等模型对单调的函数变换不敏感。其原因在于树模型在求解分裂点的时候，只考虑排序分位点。</p>
<h3 id="特征编码"><a href="#特征编码" class="headerlink" title="特征编码"></a>特征编码</h3><p>对于离散的类别特征，往往需要进行必要的特征转换/编码才能将其作为特征输入到模型中。常用的编码方式有 LabelEncoder，OneHotEncoder（sklearn里面的接口）。譬如对于”性别”这个特征（取值为男性和女性），使用这两种方式可以分别编码为{0,1}和{[1,0], [0,1]}。</p>
<p>对于取值较多（如几十万）的类别特征（ID特征），直接进行OneHotEncoder编码会导致特征矩阵非常巨大，影响模型效果。可以使用如下的方式进行处理：<br>◆ 统计每个取值在样本中出现的频率，取 Top N 的取值进行 One-hot 编码，剩下的类别分到“其他“类目下，其中 N 需要根据模型效果进行调优；<br>◆ 统计每个 ID 特征的一些统计量（譬如历史平均点击率，历史平均浏览率）等代替该 ID 取值作为特征，具体可以参考 Avazu 点击率预估比赛第二名的获奖方案；<br>◆ 参考 word2vec 的方式，将每个类别特征的取值映射到一个连续的向量，对这个向量进行初始化，跟模型一起训练。训练结束后，可以同时得到每个ID的Embedding。具体的使用方式，可以参考 Rossmann 销量预估竞赛第三名的获奖方案，<a href="https://github.com/entron/entity-embedding-rossmann。">https://github.com/entron/entity-embedding-rossmann。</a></p>
<p>对于 Random Forest 和 GBDT 等模型，如果类别特征存在较多的取值，可以直接使用 LabelEncoder 后的结果作为特征（这里应该只是将数字来代替类别，数字并不具有实际含义）。注意labelEncoder将文字变换为数字，是虚拟数据，不一定有意义，建模时要注意去除。</p>
<h2 id="模型训练和验证"><a href="#模型训练和验证" class="headerlink" title="模型训练和验证"></a>模型训练和验证</h2><h3 id="模型选择"><a href="#模型选择" class="headerlink" title="模型选择"></a>模型选择</h3><p>在处理好特征后，我们可以进行模型的训练和验证。<br>◆ 对于稀疏型特征（如文本特征，One-hot的ID类特征），我们一般使用线性模型，譬如 Linear Regression 或者 Logistic Regression。Random Forest 和 GBDT 等树模型不太适用于稀疏的特征，但可以先对特征进行降维（如PCA，SVD/LSA等），再使用这些特征。稀疏特征直接输入 DNN 会导致网络 weight 较多，不利于优化，也可以考虑先降维，或者对 ID 类特征使用 Embedding 的方式；<br>◆ 对于稠密型特征，推荐使用 XGBoost 进行建模，简单易用效果好；<br>◆ 数据中既有稀疏特征，又有稠密特征，可以考虑使用线性模型对稀疏特征进行建模，将其输出与稠密特征一起再输入 XGBoost/DNN 建模，具体可以参考Stacking 部分。</p>
<h3 id="调参和模型验证"><a href="#调参和模型验证" class="headerlink" title="调参和模型验证"></a>调参和模型验证</h3><p>对于选定的特征和模型，我们往往还需要对模型进行超参数的调优，才能获得比较理想的效果。调参一般可以概括为以下三个步骤：</p>
<p>1.<strong>训练集和验证集的划分。</strong>根据比赛提供的训练集和测试集，模拟其划分方式对训练集进行划分为本地训练集和本地验证集。划分的方式视具体比赛和数据而定，常用的方式有：<br>a) 随机划分：譬如随机采样 70% 作为训练集，剩余的 30% 作为测试集。在这种情况下，本地可以采用 KFold 或者 Stratified KFold 的方法来构造训练集和验证集。<br>b) 按时间划分：一般对应于时序序列数据，譬如取前 7 天数据作为训练集，后 1 天数据作为测试集。这种情况下，划分本地训练集和验证集也需要按时间先后划分。常见的错误方式是随机划分，这种划分方式可能会导致模型效果被高估。<br>c) 按某些规则划分：在 HomeDepot 搜索相关性比赛中，训练集和测试集中的 Query 集合并非完全重合，两者只有部分交集。而在另外一个相似的比赛中（CrowdFlower 搜索相关性比赛），训练集和测试集具有完全一致的 Query 集合。对于 HomeDepot 这个比赛中，训练集和验证集数据的划分，需要考虑 Query 集合并非完全重合这个情况，其中的一种方法可以参考第三名的获奖方案，<a href="https://github.com/ChenglongChen/Kaggle_HomeDepot。">https://github.com/ChenglongChen/Kaggle_HomeDepot。</a></p>
<p>2.<strong>指定参数空间</strong>。在指定参数空间的时候，需要对模型参数以及其如何影响模型的效果有一定的了解，才能指定出合理的参数空间。譬如DNN或者XGBoost中学习率这个参数，一般就选 0.01 左右就 OK 了（太大可能会导致优化算法错过最优化点，太小导致优化收敛过慢）。再如 Random Forest，一般设定树的棵数范围为 100~200 就能有不错的效果，当然也有人固定数棵数为 500，然后只调整其他的超参数。</p>
<p>3.<strong>按照一定的方法进行参数搜索</strong>。常用的参数搜索方法有，Grid Search，Random Search以及一些自动化的方法（如 Hyperopt）。其中，Hyperopt 的方法，根据历史已经评估过的参数组合的效果，来推测本次评估使用哪个参数组合更有可能获得更好的效果。</p>
<h3 id="适当利用-Public-LB-的反馈"><a href="#适当利用-Public-LB-的反馈" class="headerlink" title="适当利用 Public LB 的反馈"></a>适当利用 Public LB 的反馈</h3><p><img src="/img/media/tx%E5%BE%80%E5%B1%8A%E5%BF%83%E5%BE%97/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-17%20%E4%B8%8A%E5%8D%8810.30.48.png" alt=""></p>
<h2 id="模型集成"><a href="#模型集成" class="headerlink" title="模型集成"></a>模型集成</h2><h3 id="Averaging-和-Voting"><a href="#Averaging-和-Voting" class="headerlink" title="Averaging 和 Voting"></a>Averaging 和 Voting</h3><p>直接对多个模型的预测结果求平均或者投票。对于目标变量为连续值的任务，使用平均；对于目标变量为离散值的任务，使用投票的方式。</p>
<h3 id="Stacking"><a href="#Stacking" class="headerlink" title="Stacking"></a>Stacking</h3><p><img src="/img/media/tx%E5%BE%80%E5%B1%8A%E5%BF%83%E5%BE%97/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-17%20%E4%B8%8A%E5%8D%8810.32.33.png" alt=""></p>
<ol>
<li><strong>数据集划分</strong>。将训练数据按照5-Fold进行划分（如果数据跟时间有关，需要按时间划分）</li>
<li><strong>基础模型训练 I</strong>。按照交叉验证（Cross Validation）的方法，在训练集（Training Fold）上面训练模型（如图灰色部分所示），并在验证集（Validation Fold）上面做预测，得到预测结果（如图黄色部分所示）。最后综合得到整个训练集上面的预测结果（如图第一个黄色部分的CV Prediction所示）。</li>
<li><strong>基础模型训练 II</strong>（如图5第二和三行左半部分所示）。在全量的训练集上训练模型（如图第二行灰色部分所示），并在测试集上面做预测，得到预测结果（如图第三行虚线后绿色部分所示）。</li>
<li><strong>Stage 1 模型集成训练 I</strong>（如图5第一行右半部分所示）。将步骤 2 中得到的 CV Prediction 当作新的训练集，按照步骤 2 可以得到 Stage 1模型集成的 CV Prediction。</li>
<li>Stage 1 模型集成训练 II（如图5第二和三行右半部分所示）。将步骤 2 中得到的 CV Prediction 当作新的训练集和步骤 3 中得到的 Prediction 当作新的测试集，按照步骤 3 可以得到 Stage 1 模型集成的测试集 Prediction。此为 Stage 1 的输出，可以提交至 Kaggle 验证其效果。</li>
</ol>
<p>在图5中，基础模型只展示了一个，而实际应用中，基础模型可以多种多样，如SVM，DNN，XGBoost 等。也可以相同的模型，不同的参数，或者不同的样本权重。重复4和5两个步骤，可以相继叠加 Stage 2, Stage 3 等模型。</p>
<h3 id="Blending"><a href="#Blending" class="headerlink" title="Blending"></a>Blending</h3><p>Blending 与 Stacking 类似，但单独留出一部分数据（如 20%）用于训练 Stage X 模型。</p>
<h3 id="Bagging-Ensemble-Selection"><a href="#Bagging-Ensemble-Selection" class="headerlink" title="Bagging Ensemble Selection"></a>Bagging Ensemble Selection</h3><p>Bagging Ensemble Selection [5] 是我在 CrowdFlower 搜索相关性比赛中使用的方法，其主要的优点在于可以以优化任意的指标来进行模型集成。这些指标可以是可导的（如 LogLoss 等）和不可导的（如正确率，AUC，Quadratic Weighted Kappa等）。它是一个前向贪婪算法，存在过拟合的可能性，作者在文献 [5] 中提出了一系列的方法（如 Bagging）来降低这种风险，稳定集成模型的性能。使用这个方法，需要有成百上千的基础模型。为此，在 CrowdFlower 的比赛中，我把在调参过程中所有的中间模型以及相应的预测结果保留下来，作为基础模型。这样做的好处是，不仅仅能够找到最优的单模型（Best Single Model），而且所有的中间模型还可以参与模型集成，进一步提升效果。</p>
<h2 id="自动化框架"><a href="#自动化框架" class="headerlink" title="自动化框架"></a>自动化框架</h2><p>这份代码开源在 Github 上面，目前是 Github 有关 Kaggle 竞赛解决方案的 Most Stars，地址：<a href="https://github.com/ChenglongChen/Kaggle_CrowdFlower。">https://github.com/ChenglongChen/Kaggle_CrowdFlower。</a></p>
<p>其主要包含以下部分：</p>
<p>1.模块化特征工程<br>a) 接口统一，只需写少量的代码就能够生成新的特征；<br>b) 自动将单独的特征拼接成特征矩阵。</p>
<p>2.自动化模型调参和验证<br>a) 自定义训练集和验证集的划分方法；<br>b) 使用 Grid Search / Hyperopt 等方法，对特定的模型在指定的参数空间进行调优，并记录最佳的模型参数以及相应的性能。</p>
<p>3.自动化模型集成<br>a) 对于指定的基础模型，按照一定的方法（如Averaging_Stacking_Blending 等）生成集成模型。</p>

          
        
      
    </div>

    

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://github.com/zdkswd/2019/04/17/LightGBM Python quick start/"/>

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="ZDK"/>
      <meta itemprop="description" content=""/>
      <meta itemprop="image" content="/images/avatar.gif"/>
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ZDK's blog"/>
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/04/17/LightGBM Python quick start/" class="post-title-link" itemprop="url">LightGBM Python quick start</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-04-17 14:00:32 / 修改时间：14:00:16" itemprop="dateCreated datePublished" datetime="2019-04-17T14:00:32+08:00">2019-04-17</time>
            </span>
          

          
            

            
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/教程/" itemprop="url" rel="index"><span itemprop="name">教程</span></a></span>

                
                
              
            </span>
          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">评论数：</span>
                <a href="/2019/04/17/LightGBM Python quick start/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2019/04/17/LightGBM Python quick start/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
            <span id="/2019/04/17/LightGBM Python quick start/" class="post-meta-item leancloud_visitors" data-flag-title="LightGBM Python quick start">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              
                <span class="post-meta-item-text">阅读次数：</span>
              
                <span class="leancloud-visitors-count"></span>
            </span>
          

          

          

          <br/>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="LightGBM-Python-quick-start"><a href="#LightGBM-Python-quick-start" class="headerlink" title="LightGBM Python quick start"></a>LightGBM Python quick start</h1><p><img src="/img/media/LightGBM%20Python%20quick%20start/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-17%20%E4%B8%8B%E5%8D%881.00.51.png" alt=""></p>
<h1 id="Data-Interface"><a href="#Data-Interface" class="headerlink" title="Data Interface"></a>Data Interface</h1><p>The LightGBM Python module can load data from:</p>
<p>1.libsvm/ tsv / csv / txt format file<br>2.NumPy 2D array(s), pandas DataFrame, H2O DataTable’s Frame, SciPy sparse matrix<br>3.LightGBM binary file</p>
<p>The data is stored in a <strong>Dataset</strong> object.</p>
<p><strong>To load a libsvm text file or a LightGBM binary file into Dataset:</strong><br><img src="/img/media/LightGBM%20Python%20quick%20start/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-17%20%E4%B8%8B%E5%8D%881.02.58.png" alt=""></p>
<p><strong>To load a numpy array into Dataset:</strong><br><img src="/img/media/LightGBM%20Python%20quick%20start/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-17%20%E4%B8%8B%E5%8D%881.02.47.png" alt=""></p>
<p><strong>Saving Dataset into a LightGBM binary file will make loading faster:</strong><br><img src="/img/media/LightGBM%20Python%20quick%20start/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-17%20%E4%B8%8B%E5%8D%881.03.46.png" alt=""></p>
<p><strong>Create validation data:</strong><br><img src="/img/media/LightGBM%20Python%20quick%20start/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-17%20%E4%B8%8B%E5%8D%881.05.34.png" alt=""><br>or<br><img src="/img/media/LightGBM%20Python%20quick%20start/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-17%20%E4%B8%8B%E5%8D%881.06.14.png" alt=""><br>In LightGBM, the validation data should be aligned with training data.</p>
<p><strong>Specific feature names and categorical features:</strong><br><img src="/img/media/LightGBM%20Python%20quick%20start/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-17%20%E4%B8%8B%E5%8D%881.22.04.png" alt=""><br>LightGBM can use categorical features as input directly. It doesn’t need to convert to one-hot coding, and is much faster than one-hot coding (about 8x speed-up).</p>
<p><strong>Note</strong>: You should convert your categorical features to <strong>int</strong> type before you construct <strong>Dataset</strong>.</p>
<p><strong>Weights can be set when needed:</strong><br><img src="/img/media/LightGBM%20Python%20quick%20start/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-17%20%E4%B8%8B%E5%8D%881.22.53.png" alt=""><br>or<br><img src="/img/media/LightGBM%20Python%20quick%20start/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-17%20%E4%B8%8B%E5%8D%881.23.06.png" alt=""><br>And you can use <strong>Dataset.set_init_score()</strong> to set initial score, and <strong>Dataset.set_group()</strong> to set group/query data for ranking tasks.</p>
<p><strong>Memory efficient usage:</strong><br>The <strong>Dataset</strong> object in LightGBM is very memory-efficient, it only needs to save discrete bins. However, Numpy / Array / Pandas object is memory expensive. If you are concerned about your memory consumption, you can save memory by:</p>
<ol>
<li>Set <strong>free_raw_data=True</strong> (default is <strong>True</strong>) when constructing the <strong>Dataset</strong></li>
<li>Explicitly set <strong>raw_data=None</strong> after the <strong>Dataset</strong> has been constructed</li>
<li>Call <strong>gc</strong></li>
</ol>
<h1 id="Setting-Parameters"><a href="#Setting-Parameters" class="headerlink" title="Setting Parameters"></a>Setting Parameters</h1><p>LightGBM can use either a list of pairs or a dictionary to set <strong>Parameters</strong>. For instance:<br><strong>Booster parameters</strong>:<br><img src="/img/media/LightGBM%20Python%20quick%20start/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-17%20%E4%B8%8B%E5%8D%881.56.05.png" alt=""><br><img src="/img/media/LightGBM%20Python%20quick%20start/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-17%20%E4%B8%8B%E5%8D%881.27.59.png" alt=""></p>
<p><strong>You can also specify multiple eval metrics:</strong><br><img src="/img/media/LightGBM%20Python%20quick%20start/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-17%20%E4%B8%8B%E5%8D%881.28.16.png" alt=""></p>
<h1 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h1><p>Training a model requires a parameter list and data set:<br><img src="/img/media/LightGBM%20Python%20quick%20start/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-17%20%E4%B8%8B%E5%8D%881.29.43.png" alt=""></p>
<p>After training, the model can be saved:<br><img src="/img/media/LightGBM%20Python%20quick%20start/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-17%20%E4%B8%8B%E5%8D%881.30.14.png" alt=""></p>
<p>The trained model can also be dumped to JSON format:<br><img src="/img/media/LightGBM%20Python%20quick%20start/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-17%20%E4%B8%8B%E5%8D%881.31.17.png" alt=""></p>
<p>A saved model can be loaded:<br><img src="/img/media/LightGBM%20Python%20quick%20start/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-17%20%E4%B8%8B%E5%8D%881.31.36.png" alt=""></p>
<h1 id="CV"><a href="#CV" class="headerlink" title="CV"></a>CV</h1><p>Training with 5-fold CV:<br><img src="/img/media/LightGBM%20Python%20quick%20start/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-17%20%E4%B8%8B%E5%8D%881.33.58.png" alt=""></p>
<h1 id="Early-Stopping"><a href="#Early-Stopping" class="headerlink" title="Early Stopping"></a>Early Stopping</h1><p>If you have a validation set, you can use early stopping to find the optimal number of boosting rounds. Early stopping requires at least one set in <strong>valid_sets</strong>. If there is more than one, it will use all of them except the training data:<br><img src="/img/media/LightGBM%20Python%20quick%20start/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-17%20%E4%B8%8B%E5%8D%881.35.00.png" alt=""><br>The model will train until the validation score stops improving. Validation score needs to improve at least every <strong>early_stopping_rounds</strong> to continue training.</p>
<p>The index of iteration that has the best performance will be saved in the <strong>best_iteration</strong> field if early stopping logic is enabled by setting <strong>early_stopping_rounds</strong>. Note that <strong>train()</strong> will return a model from the best iteration.</p>
<p>This works with both metrics to minimize (L2, log loss, etc.) and to maximize (NDCG, AUC, etc.). Note that if you specify more than one evaluation metric, all of them will be used for early stopping. However, you can change this behavior and make LightGBM check only the first metric for early stopping by creating <strong>early_stopping</strong> callback with <strong>first_metric_only=True</strong>.</p>
<h1 id="Prediction"><a href="#Prediction" class="headerlink" title="Prediction"></a>Prediction</h1><p>A model that has been trained or loaded can perform predictions on datasets:<br><img src="/img/media/LightGBM%20Python%20quick%20start/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-17%20%E4%B8%8B%E5%8D%881.51.55.png" alt=""><br>If early stopping is enabled during training, you can get predictions from the best iteration with <strong>bst.best_iteration</strong>:<br><img src="/img/media/LightGBM%20Python%20quick%20start/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-17%20%E4%B8%8B%E5%8D%881.52.37.png" alt=""></p>

          
        
      
    </div>

    

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://github.com/zdkswd/2019/04/17/word2vec/"/>

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="ZDK"/>
      <meta itemprop="description" content=""/>
      <meta itemprop="image" content="/images/avatar.gif"/>
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ZDK's blog"/>
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/04/17/word2vec/" class="post-title-link" itemprop="url">word2vec</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-04-17 09:55:47 / 修改时间：09:56:26" itemprop="dateCreated datePublished" datetime="2019-04-17T09:55:47+08:00">2019-04-17</time>
            </span>
          

          
            

            
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/知识总结/" itemprop="url" rel="index"><span itemprop="name">知识总结</span></a></span>

                
                
              
            </span>
          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">评论数：</span>
                <a href="/2019/04/17/word2vec/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2019/04/17/word2vec/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
            <span id="/2019/04/17/word2vec/" class="post-meta-item leancloud_visitors" data-flag-title="word2vec">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              
                <span class="post-meta-item-text">阅读次数：</span>
              
                <span class="leancloud-visitors-count"></span>
            </span>
          

          

          

          <br/>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="word2vec"><a href="#word2vec" class="headerlink" title="word2vec"></a>word2vec</h1><p>谷歌2013年提出的word2vec是目前最常用的词嵌入模型之一。Word2ec实际是一种<strong>浅层的神经网络模型</strong>,它有两种网络结构,分别是CBOW( Continues Bag      of words)和 Skip-gram。</p>
<h1 id="百面-Word2Vec"><a href="#百面-Word2Vec" class="headerlink" title="百面  Word2Vec"></a>百面  Word2Vec</h1><h2 id="问-Word2Vec是如何工作的？"><a href="#问-Word2Vec是如何工作的？" class="headerlink" title="问  Word2Vec是如何工作的？"></a>问  Word2Vec是如何工作的？</h2><p>答：CBOW的目标是根据上下文出现的词语来预测当前词的生成概率，如图1.3 (a) 所示;而Skip-gram是根据当前词来预测上下文中各词的生成概率，如图1.3 (b)所示。<br><img src="/img/media/word2vec/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-26%20%E4%B8%8B%E5%8D%884.54.01.png" alt=""><br>其中w(t)是当前所关注的词，w(t-2)、 w(t-1)、 w(t+1)、 w(t+2)是 上下文中出现的词。这里前后滑动窗口大小均设为2。</p>
<p>CBOW和Skip-gram都可以表示成由输入层(Input)、映射层(Projection)和输出层(Output)组成的神经网络。</p>
<p>输入层中的每个词由独热编码方式表示,即所有词均表示成一个N维向量,其中N为词汇表中单词的总数。在向量中,每个词都将与之对应的维度置为1,其余维度的值均设为0。</p>
<p>在映射层(又称隐含层)中,K个隐含单元( Hidden units)的取值可以由N维输入向量以及连接输入和隐含单元之间的N乘K维权重矩阵计算得到。在CBOV中,还需要将各个输入词所计算出的隐含单元求和。补一个其他博客的图。<br><img src="/img/media/word2vec/20180113213325970.png" alt=""><br><img src="/img/media/word2vec/20180113212531373.png" alt=""><br>同理，输出层向量的值可以通过隐含层向量(K维)，以及连接隐含层和输出层之间的KxN维权重矩阵计算得到。输出层也是一个N维向量，每维与词汇表中的一个单词相对应。最后，对输出层向量应用Softmax激活函数，可以计算出每个单词的生成概率。Softmax激活函数的定义为：<br><img src="/img/media/word2vec/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-26%20%E4%B8%8B%E5%8D%886.21.23.png" alt=""><br>其中x代表N维的原始输出向量,xn为在原始输出向量中，与单词wn所对应维度的取值。</p>
<p>接下来的任务就是训练神经网络的权重,使得语料库中所有单词的整体生成概率最大化。从输入层到隐含层需要一个维度为NxK的权重矩阵，从隐含层到输出层又需要一个维度为KxN的权重矩阵，学习权重可以用反向传播算法实现，每次迭代时将权重沿梯度更优的方向进行一小步更新。但是由于Softmax激活函数中存在归一化项的缘故，推导出来的迭代公式需要对词汇表中的所有单词进行遍历，使得每次迭代过程非常缓慢，由此产生了Hierarchical Softmax和Negative Sampling两种改进方法。训练得到维度为NxK和KxN的两个权重矩阵之后，可以选择其中一个作为N个词的K维向量表示。</p>
<h1 id="王喆-知乎"><a href="#王喆-知乎" class="headerlink" title="王喆 知乎"></a>王喆 知乎</h1><p><a href="https://zhuanlan.zhihu.com/p/53194407" target="_blank" rel="noopener">万物皆Embedding，从经典的word2vec到深度学习基本操作item2vec - 知乎</a><br><strong>万物皆Embedding，从经典的word2vec到深度学习基本操作item2vec</strong></p>
<h2 id="什么是embedding？"><a href="#什么是embedding？" class="headerlink" title="什么是embedding？"></a>什么是embedding？</h2><p>简单来说，embedding就是用一个低维的向量表示一个物体，可以是一个词，或是一个商品，或是一个电影等等。这个<strong>embedding向量的性质是能使距离相近的向量对应的物体有相近的含义</strong>，比如 Embedding(复仇者联盟)和Embedding(钢铁侠)之间的距离就会很接近，但 Embedding(复仇者联盟)和Embedding(乱世佳人)的距离就会远一些。</p>
<p>除此之外Embedding<strong>甚至还具有数学运算</strong>的关系，比如Embedding（马德里）-Embedding（西班牙）+Embedding(法国)≈Embedding(巴黎)</p>
<p>从另外一个空间表达物体，甚至揭示了物体间的潜在关系，从某种意义上来说，Embedding方法甚至具备了一些本体论的哲学意义。</p>
<h2 id="为什么说embedding是深度学习的基本操作？"><a href="#为什么说embedding是深度学习的基本操作？" class="headerlink" title="为什么说embedding是深度学习的基本操作？"></a>为什么说embedding是深度学习的基本操作？</h2><p>Embedding能够用低维向量对物体进行编码还能保留其含义的特点非常适合深度学习。在传统机器学习模型构建过程中，经常使用one hot encoding对离散特征，特别是id类特征进行编码，但由于one hot encoding的维度等于物体的总数，比如阿里的商品one hot encoding的维度就至少是千万量级的。这样的编码方式对于商品来说是极端稀疏的，甚至用multi hot encoding对用户浏览历史的编码也会是一个非常稀疏的向量。</p>
<p>而深度学习的特点以及工程方面的原因使其不利于稀疏特征向量的处理。因此如果能把物体编码为一个低维稠密向量再喂给DNN，自然是一个高效的基本操作。因为从梯度下降的过程来说，如果特征过于稀疏会导致整个网络收敛过慢，因为每次更新只有极少数的权重会得到更新。这样在样本有限的情况下会导致模型不收敛。而且还会导致全连接层有过多的参数。</p>
<p>尽管有采用relu函数等各种手段减少梯度消失现象的发生，但nn还是会存在梯度消失问题，所以到输入层的时候梯度受输出层diff的影响已经很小了因此收敛慢再加上大量稀疏特征导致一次只有个别权重更新这个现象就更严重了。对于lr来说，梯度能够直接传导到权重，因为其只有一层。倒不是说lr更适合处理大规模离散特征 而是相比nn 需要更少的数据收敛 如果数据量和时间都无限的话nn也适合处理稀疏特征。</p>
<h1 id="McCormick-W2V"><a href="#McCormick-W2V" class="headerlink" title="McCormick W2V"></a>McCormick W2V</h1><p><a href="http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/" target="_blank" rel="noopener">http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/</a></p>
<h2 id="The-Model"><a href="#The-Model" class="headerlink" title="The Model"></a>The Model</h2><p>Word2Vec uses a trick you may have seen elsewhere in machine learning. We’re going to train a simple neural network with a single hidden layer to perform a certain task, but then we’re not actually going to use that neural network for the task we trained it on! Instead, the goal is actually just to learn the weights of the <strong>hidden layer</strong>。</p>
<p>We’ll train the neural network to do this by feeding it word pairs found in our training documents.The word highlighted in blue is the input word.</p>
<p>this is skip-gram.<br><img src="/img/media/word2vec/training_data.png" alt=""></p>
<h2 id="Model-Details"><a href="#Model-Details" class="headerlink" title="Model Details"></a>Model Details</h2><p>let’s say we have a vocabulary of 10,000 unique words.We’re going to represent an input word  as a one-hot vector. This vector will have 10,000 components.The output of the network is a single vector (also with 10,000 components) Here’s the architecture of our neural network.<br><img src="/img/media/word2vec/skip_gram_net_arch.png" alt=""><br>There is <strong>no activation function</strong> on the hidden layer neurons, but the output neurons use <strong>softmax</strong>.</p>
<p>When training this network on word pairs, the input is a one-hot vector representing the input word and the training output is also a one-hot vector representing the output word.But when you evaluate the trained network on an input word, the output vector will actually be a probability distribution (i.e., a bunch of floating point values, not a one-hot vector).</p>
<h2 id="The-Hidden-Layer"><a href="#The-Hidden-Layer" class="headerlink" title="The Hidden Layer"></a>The Hidden Layer</h2><p>For our example, we’re going to say that we’re learning word vectors with 300 features. So the hidden layer is going to be represented by a weight matrix with 10,000 rows (one for every word in our vocabulary) and 300 columns (one for every hidden neuron).The number of features is a “hyper parameter” that you would just have to tune to your application .</p>
<p>显然，每个单词对应一个300维的隐向量，也可以理解为300维的语义。</p>
<p>that is why we use one-hot .<strong>右边矩阵中绿色的值即为左边矩阵中1对应的词的隐向量。</strong> 对应的，竖着的第一列即为隐藏层第一个神经元连接的上一层的神经元权重。<br><img src="/img/media/word2vec/matrix_mult_w_one_hot.png" alt=""></p>
<p>just as this image.根据反向传播公式，当梯度传递到这一层时，只有非0的值才会对梯度进行更新。<br><img src="/img/media/word2vec/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-16%20%E4%B8%8B%E5%8D%889.42.26.png" alt=""><br>This means that the hidden layer of this model is really just operating as a <strong>lookup</strong> table. The output of the hidden layer is just the “word vector” for the input word.</p>
<h2 id="Negative-Sampling"><a href="#Negative-Sampling" class="headerlink" title="Negative Sampling"></a>Negative Sampling</h2><p> The skip-gram neural network contains a huge number of weights… For our example with 300 features and a vocab of 10,000 words, that’s 3M weights in the hidden layer and output layer each! Training this on a large dataset would be prohibitive, And to make matters worse, you need a huge amount of training data in order to tune that many weights and avoid over-fitting. so the word2vec authors introduced a number of tweaks to make training feasible.The first one is Negative Sampling</p>
<p>the innovations:</p>
<ol>
<li><strong>Subsampling</strong> frequent words to decrease the number of training examples.</li>
<li>Modifying the optimization objective with a technique they called “<strong>Negative Sampling</strong>”, which causes each training sample to update only a small percentage of the model’s weights.</li>
</ol>
<p>It’s worth noting that subsampling frequent words and applying Negative Sampling not only reduced the compute burden of the training process, but also improved the quality of their resulting word vectors as well.</p>
<h2 id="Subsampling-Frequent-Words"><a href="#Subsampling-Frequent-Words" class="headerlink" title="Subsampling Frequent Words"></a>Subsampling Frequent Words</h2><p>The word highlighted in blue is the input word.<br><img src="/img/media/word2vec/training_data%202.png" alt=""><br>There are two “problems” with common words like “the”:</p>
<ol>
<li>When looking at word pairs, (“fox”, “the”) doesn’t tell us much about the meaning of “fox”. “the” appears in the context of pretty much every word.</li>
<li>We will have many more samples of (“the”, …) than we need to learn a good vector for “the”.</li>
</ol>
<p>Word2Vec implements a “subsampling” scheme to address this. For each word we encounter in our training text, there is a chance that we will effectively delete it from the text. The probability that we cut the word is related to the word’s frequency.</p>
<p>If we have a window size of 10, and we remove a specific instance of “the” from our text:</p>
<ol>
<li>As we train on the remaining words, “the” will not appear in any of their context windows.</li>
<li>We’ll have 10 fewer training samples where “the” is the input word.</li>
</ol>
<h3 id="Sample-rate"><a href="#Sample-rate" class="headerlink" title="Sample rate"></a>Sample rate</h3><p>wi is the word, z(wi) is the fraction of the total words in the corpus that are that word. For example, if the word “peanut” occurs 1,000 times in a 1 billion word corpus, then z(‘peanut’) = 1E-6.<br><img src="/img/media/word2vec/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-17%20%E4%B8%8A%E5%8D%889.13.31.png" alt=""><br><img src="/img/media/word2vec/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-17%20%E4%B8%8A%E5%8D%889.13.53.png" alt=""><br>1.P(wi)=1.0 (100% chance of being kept) when z(wi)&lt;=0.0026.<br>This means that only words which represent more than 0.26% of the total words will be subsampled.<br>2.P(wi)=0.5 (50% chance of being kept) when z(wi)=0.00746.<br>3.P(wi)=0.033 (3.3% chance of being kept) when z(wi)=1.0.<br>That is, if the corpus consisted entirely of word wi, which of course is ridiculous.</p>
<h2 id="Negative-Sampling-1"><a href="#Negative-Sampling-1" class="headerlink" title="Negative Sampling"></a>Negative Sampling</h2><p>Negative sampling addresses this by having each training sample only modify a small percentage of the weights, rather than all of them. Here’s how it works.</p>
<p>When training the network on the word pair (“fox”, “quick”), recall that the “label” or “correct output” of the network is a one-hot vector. That is, for the output neuron corresponding to “quick” to output a 1, and for all of the other thousands of output neurons to output a 0.</p>
<p>With negative sampling, we are instead going to randomly select just a small number of “negative” words (let’s say 5) to update the weights for. (In this context, a “negative” word is one for which we want the network to output a 0 for). We will also still update the weights for our “positive” word (which is the word “quick” in our current example).</p>
<p>Recall that the output layer of our model has a weight matrix that’s 300 x 10,000. So we will just be updating the weights for our positive word (“quick”), plus the weights for 5 other words that we want to output 0. That’s a total of 6 output neurons, and 1,800 weight values total. That’s only 0.06% of the 3M weights in the output layer!</p>
<p>In the hidden layer, only the weights for the input word are updated (this is true whether you’re using Negative Sampling or not).</p>
<h3 id="Selecting-Negative-Samples"><a href="#Selecting-Negative-Samples" class="headerlink" title="Selecting Negative Samples"></a>Selecting Negative Samples</h3><p>The “negative samples” (that is, the 5 output words that we’ll train to output 0) are selected using a “unigram distribution”, where more frequent words are more likely to be selected as negative samples.</p>
<p>For instance, suppose you had your entire training corpus as a list of words, and you chose your 5 negative samples by picking randomly from the list. In this case, the probability for picking the word “couch” would be equal to the number of times “couch” appears in the corpus, divided the total number of word occus in the corpus. This is expressed by the following equation:<br><img src="/img/media/word2vec/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-17%20%E4%B8%8A%E5%8D%889.27.08.png" alt=""><br>The authors state in their paper that they tried a number of variations on this equation, and the one which performed best was to raise the word counts to the 3/4 power:<br><img src="/img/media/word2vec/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-17%20%E4%B8%8A%E5%8D%889.27.29.png" alt=""></p>
<h2 id="Applying-word2vec-to-Recommenders-and-Advertising"><a href="#Applying-word2vec-to-Recommenders-and-Advertising" class="headerlink" title="Applying word2vec to Recommenders and Advertising"></a>Applying word2vec to Recommenders and Advertising</h2><p>The key principle behind word2vec is the notion that the meaning of a word can be inferred from it’s context–what words tend to be around it. To abstract that a bit, text is really just a sequence of words, and the meaning of a word can be extracted from what words tend to be just before and just after it in the sequence.</p>
<p>What researchers and companies are finding is that the time series of online user activity offers the same opportunity for inferring meaning from context. That is, as a user browses around and interacts with different content, the abstract qualities of a piece of content can be inferred from what content the user interacts with before and after. This allows ML teams to apply word vector models to learn good vector representations for products, content, and ads.</p>
<p>The word2vec approach has proven successful in extracting these hidden insights, and being able to compare, search, and categorize items on these abstract dimensions opens up a lot of opportunities for smarter, better recommendations. </p>
<h2 id="Four-Production-Examples"><a href="#Four-Production-Examples" class="headerlink" title="Four Production Examples"></a>Four Production Examples</h2><h3 id="Music-Recommendations"><a href="#Music-Recommendations" class="headerlink" title="Music Recommendations"></a>Music Recommendations</h3><p><img src="/img/media/word2vec/Spotify_user_activity.png" alt=""><br>One use is to create a kind of “music taste” vector for a user by averaging together the vectors for songs that a user likes to listen to. This taste vector can then become the query for a similarity search to find songs which are similar to the user’s taste vector.<br>and Listing Recommendations at Airbnb,Product Recommendations in Yahoo Mail,Matching Ads to Search Queries</p>
<h1 id="几篇论文"><a href="#几篇论文" class="headerlink" title="几篇论文"></a>几篇论文</h1><p> <strong>Distributed Representations of Words and Phrases and their Compositionality</strong><br>Google的Tomas Mikolov提出word2vec的两篇文章之一，这篇文章更具有综述性质，列举了NNLM、RNNLM等诸多词向量模型，但最重要的还是提出了CBOW和Skip-gram两种word2vec的模型结构。虽然词向量的研究早已有之，但不得不说还是Google的word2vec的提出让词向量重归主流，拉开了整个embedding技术发展的序幕。</p>
<p><strong>Efficient Estimation of Word Representations in Vector Space</strong><br>Tomas Mikolov的另一篇word2vec奠基性的文章。相比上一篇的综述，本文更详细的阐述了Skip-gram模型的细节，包括模型的具体形式和 Hierarchical Softmax和 Negative Sampling两种可行的训练方法。</p>
<p> <strong>Word2vec Parameter Learning Explained</strong><br>虽然Mikolov的两篇代表作标志的word2vec的诞生，但其中忽略了大量技术细节，如果希望完全读懂word2vec的原理和实现方法，比如词向量具体如何抽取，具体的训练过程等，强烈建议大家阅读UMich Xin Rong博士的这篇针对word2vec的解释性文章。惋惜的是Xin Rong博士在完成这篇文章后的第二年就由于飞机事故逝世，在此也致敬并缅怀一下Xin Rong博士。</p>

          
        
      
    </div>

    

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://github.com/zdkswd/2019/04/16/About id/"/>

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="ZDK"/>
      <meta itemprop="description" content=""/>
      <meta itemprop="image" content="/images/avatar.gif"/>
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ZDK's blog"/>
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/04/16/About id/" class="post-title-link" itemprop="url">About id </a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-04-16 20:42:47" itemprop="dateCreated datePublished" datetime="2019-04-16T20:42:47+08:00">2019-04-16</time>
            </span>
          

          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-04-17 12:35:42" itemprop="dateModified" datetime="2019-04-17T12:35:42+08:00">2019-04-17</time>
              </span>
            
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/知识总结/" itemprop="url" rel="index"><span itemprop="name">知识总结</span></a></span>

                
                
              
            </span>
          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">评论数：</span>
                <a href="/2019/04/16/About id/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2019/04/16/About id/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
            <span id="/2019/04/16/About id/" class="post-meta-item leancloud_visitors" data-flag-title="About id ">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              
                <span class="post-meta-item-text">阅读次数：</span>
              
                <span class="leancloud-visitors-count"></span>
            </span>
          

          

          

          <br/>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="如何利用id类特征"><a href="#如何利用id类特征" class="headerlink" title="如何利用id类特征"></a>如何利用id类特征</h1><p><a href="https://www.zhihu.com/question/34819617" target="_blank" rel="noopener">机器学习中如何利用id类特征？ - 知乎</a></p>
<h2 id="为什么用"><a href="#为什么用" class="headerlink" title="为什么用"></a>为什么用</h2><p>会极大提高模型的个性化能力和实际效果。而且可以对抗热度穿透现象。</p>
<p>直接加入id类特征，尽管并不能实现完全的个性化，但是可以把每个用户的行为模式区分开，从而提高了其他特征的泛化能力。</p>
<p>例如有两个id和一项特征来进行一个回归预测。一个id是正常用户，一个id是刷子用户。式子可以表示为w1x1+w2x2+w3x3=rate<br>对于正常用户rate值低，刷子用户rate值高。即w1x1+w3x3值较小，w2x2+w3x3值较大。可得是w2x2项较大即模型学习到了提高w2，对于不同的id就进行了区别对待。</p>
<p>加入id类特征的价值：</p>
<ol>
<li>可以使得在学习过程中每个人的信号更合理地影响整体模型，使得模型泛化能力更好。</li>
<li>可以使得模型能够对每个id有更细粒度的排序能力，使得模型的个性化效果更好。</li>
</ol>
<h2 id="怎么用"><a href="#怎么用" class="headerlink" title="怎么用"></a>怎么用</h2><ol>
<li>id类特征上的信号是及其稀疏的，所以意味着我们需要更大量的数据，但是其实这并不困难，在计算广告，推荐系统的场景下，单个id上收集的数据其实是非常多的，但是一定要通过正则化的方法来限制以使id类特征不过拟合。</li>
<li>id类特征在预测中的命中率可能并不高，但这其实也不是问题。因为一个特征就是一个体系，一个体系化的特征是通过层次化的特征设计来达到命中率和个性化的综合。比如说 用户id-&gt;用户GPS坐标+用户喜好Tag+用户最近行为-&gt;用户年龄，用户性别。通过分层，由最细粒度到最粗粒度的特征搭配来保证特征命中率。</li>
<li>组合。单独的id类特征时意义并没有那么高，有意义的是不同层次的交叉组合。userid和itemid交互后，也就是用户对物品的评分矩阵，这时候就可以使用itemcf或svd等等。</li>
<li>模型和算法。实际上，LR是适合使用ID类特征One-hot编码的，原因在于LR适合接受超高维度的特征输入。但是这么做的前提是训练样本足够多。对于XGBoost，DNN，就要先对id特征one-hot进行embedding。</li>
</ol>
<h1 id="Entity-Embeddings-of-Categorical-Variables"><a href="#Entity-Embeddings-of-Categorical-Variables" class="headerlink" title="Entity Embeddings of Categorical Variables"></a>Entity Embeddings of Categorical Variables</h1><p><a href="https://arxiv.org/pdf/1604.06737.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1604.06737.pdf</a></p>
<h2 id="Intorduction"><a href="#Intorduction" class="headerlink" title="Intorduction"></a>Intorduction</h2><p>神经网路不适合去拟合非连续的函数，因为其假定一般形式具有连续性。在训练阶段，数据的连续性保证了优化的收敛性，在预测阶段，输入值的微小变化保证了输出的稳定。另一方面，决策树不假定特征变量有任何的连续性。</p>
<p>有意思的是，如果我们使用了正确的表示数据的形式，在现实世界中，我们面对的问题总是连续的。每当我们找到了一个更好的方式来表示连续数据，就提升了神经网络学习数据的能力。比如NLP是由于使用了w2v去将one-hot转为了连续的向量。</p>
<p>不同于自然中存在的非结构化数据，机器学习中使用的结构化数据可能很难发现连续性质。神经网络连续函数的特性限制了对于类别变量的应用。embedding解决了one-hot的两个问题，一是太过稀疏，而是类与类之间没有关联。</p>
<p>记号：（h,r,t），h和t是两个实体，r是关系。<br>在w2v中发现隐向量具有如下的关系：<br><img src="/img/media/About%20id/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-16%20%E4%B8%8B%E5%8D%881.49.04.png" alt=""><br>w2v除了使用常规的滑动窗口学习还可以使用标记进行监督学习。<br>以及说树是结构化数据中用的最多的模型。以及该方法针对的是表格数据中的离散目标值，也就是类别值。</p>
<h2 id="Entity-Embedding"><a href="#Entity-Embedding" class="headerlink" title="Entity Embedding"></a>Entity Embedding</h2><p>embedding层的维度D是个超参数。范围是[1,m-1]，m是类别数。在实际中，根据实验来确定最后的维度数。下面是一些经验之谈，一，大体的估计一下需要描述的维度，二如果不知道怎么下手，从m-1开始。</p>
<p>定义衡量两个隐向量相似度的方法。最简单的就是隐向量求距离。</p>
<p>a example for dimension<br><img src="/img/media/About%20id/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-16%20%E4%B8%8B%E5%8D%882.52.30.png" alt=""></p>
<p>原始的数据是两部分，第一部分是train.csv,包含了2.5年的每日销售数据，包括1115种不同的商店，共计有1017210条数据。第二部分是关于1115店的更详细的信息。除了主办方给出的数据，外部的数据同样重要，比如日期的天气信息，流行信息，甚至于重大比赛的日期信息。</p>
<h1 id="业务实践"><a href="#业务实践" class="headerlink" title="业务实践"></a>业务实践</h1><p>场景：网络购物场景中，运用W2V+BP进行个性化推荐。</p>
<ol>
<li><strong>对物品进行向量化</strong>，把每个用户看作一篇文章，用户购买物品按照时间序列排序，物品看作词，带入W2V模型得到物品的向量。</li>
<li><strong>样本收集</strong>，收集客户端中，对用户的物品曝光及购买记录，以用户历史购买的物品列表作为用户画像，以给用户曝光物品后用户是否购买为目标变量。</li>
<li><strong>构造W2V + BP的模型</strong>，模型的输入有两个，第一个为用户历史购买物品的向量均值，第二个为曝光物品的向量。模型的输出为用户是否购买曝光的物品，中间用BP网络进行连接。</li>
<li><strong>模型训练与使用</strong>，模型训练：目前业界一般使用TF进行实现，BP网络的节点数及层数需要根据训练情况确定。模型使用：给定一个用户u及一个物品i，把用户u购买物品向量均值及物品i的向量作为模型输入，计算物品i的模型得分。重复该操作，计算出用户u所有候选物品的模型得分，根据物品的模型得分降序推荐给用户。</li>
</ol>
<p><img src="/img/media/About%20id/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-16%20%E4%B8%8B%E5%8D%884.23.44.png" alt=""></p>

          
        
      
    </div>

    

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://github.com/zdkswd/2019/04/16/Keras_getting started/"/>

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="ZDK"/>
      <meta itemprop="description" content=""/>
      <meta itemprop="image" content="/images/avatar.gif"/>
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ZDK's blog"/>
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/04/16/Keras_getting started/" class="post-title-link" itemprop="url">Keras:getting started</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-04-16 20:31:32 / 修改时间：20:32:08" itemprop="dateCreated datePublished" datetime="2019-04-16T20:31:32+08:00">2019-04-16</time>
            </span>
          

          
            

            
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/教程/" itemprop="url" rel="index"><span itemprop="name">教程</span></a></span>

                
                
              
            </span>
          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">评论数：</span>
                <a href="/2019/04/16/Keras_getting started/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2019/04/16/Keras_getting started/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
            <span id="/2019/04/16/Keras_getting started/" class="post-meta-item leancloud_visitors" data-flag-title="Keras:getting started">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              
                <span class="post-meta-item-text">阅读次数：</span>
              
                <span class="leancloud-visitors-count"></span>
            </span>
          

          

          

          <br/>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="30s-to-Keras"><a href="#30s-to-Keras" class="headerlink" title="30s to Keras"></a>30s to Keras</h1><p>The core data structure of Keras is a <strong>model</strong>,The simplest type of model is the Sequential model, a linear stack of layers.<br><img src="/img/media/Keras:getting%20started/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-16%20%E4%B8%8B%E5%8D%887.07.41.png" alt=""><br>Stacking layers is as easy as .add():<br><img src="/img/media/Keras:getting%20started/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-16%20%E4%B8%8B%E5%8D%887.10.09.png" alt=""><br>the first layer should specify input_dim.</p>
<p>Once your model looks good, configure its learning process with .compile():<br><img src="/img/media/Keras:getting%20started/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-16%20%E4%B8%8B%E5%8D%887.11.58.png" alt=""><br>If you need to, you can further configure your optimizer. A core principle of Keras is to make things reasonably simple, while allowing the user to be fully in control when they need to<br><img src="/img/media/Keras:getting%20started/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-16%20%E4%B8%8B%E5%8D%887.13.57.png" alt=""><br>通过引入Momentum可以让那些因学习率太大而来回摆动的参数，梯度能前后抵消，从而阻止发散。</p>
<p>You can now iterate on your training data in batches:<br><img src="/img/media/Keras:getting%20started/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-16%20%E4%B8%8B%E5%8D%887.15.15.png" alt=""><br>Alternatively, you can feed batches to your model manually:<br><img src="/img/media/Keras:getting%20started/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-16%20%E4%B8%8B%E5%8D%887.15.36.png" alt=""><br>Evaluate your performance in one line:<br><img src="/img/media/Keras:getting%20started/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-16%20%E4%B8%8B%E5%8D%887.16.01.png" alt=""><br>Or generate predictions on new data:<br><img src="/img/media/Keras:getting%20started/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-16%20%E4%B8%8B%E5%8D%887.23.03.png" alt=""></p>
<h1 id="a-densely-connected-network"><a href="#a-densely-connected-network" class="headerlink" title="a densely-connected network"></a>a densely-connected network</h1><p><img src="/img/media/Keras:getting%20started/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-16%20%E4%B8%8B%E5%8D%887.53.43.png" alt=""><br><strong>notice that a layer instance is callable on a tensor,and returns a tensor.</strong></p>
<h1 id="All-models-are-callable-just-like-layers"><a href="#All-models-are-callable-just-like-layers" class="headerlink" title="All models are callable, just like layers"></a>All models are callable, just like layers</h1><p>With the functional API, it is easy to reuse trained models: you can treat any model as if it were a layer, by calling it on a tensor. Note that by calling a model you aren’t just reusing the architecture of the model, you are also reusing its weights.<br><img src="/img/media/Keras:getting%20started/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-16%20%E4%B8%8B%E5%8D%888.00.22.png" alt=""><br>This can allow, for instance, to quickly create models that can process sequences of inputs. You could turn an image classification model into a video classification model, in just one line.<br><img src="/img/media/Keras:getting%20started/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-16%20%E4%B8%8B%E5%8D%888.01.10.png" alt=""></p>
<h1 id="Multi-input-and-multi-output-models"><a href="#Multi-input-and-multi-output-models" class="headerlink" title="Multi-input and multi-output models"></a>Multi-input and multi-output models</h1><p>Here’s a good use case for the functional API: models with multiple inputs and outputs. The functional API makes it easy to manipulate a large number of intertwined datastreams.<br><img src="/img/media/Keras:getting%20started/multi-input-multi-output-graph.png" alt=""><br>The main input will receive the headline, as a sequence of integers (each integer encodes a word). The integers will be between 1 and 10,000 (a vocabulary of 10,000 words) and the sequences will be 100 words long.<br><img src="/img/media/Keras:getting%20started/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-16%20%E4%B8%8B%E5%8D%888.13.45.png" alt=""><br>Here we insert the auxiliary loss, allowing the LSTM and Embedding layer to be trained smoothly even though the main loss will be much higher in the model.<br><img src="/img/media/Keras:getting%20started/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-16%20%E4%B8%8B%E5%8D%888.14.57.png" alt=""><br>At this point, we feed into the model our auxiliary input data by concatenating it with the LSTM output:<br><img src="/img/media/Keras:getting%20started/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-16%20%E4%B8%8B%E5%8D%888.16.01.png" alt=""><br>This defines a model with two inputs and two outputs:<br><img src="/img/media/Keras:getting%20started/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-16%20%E4%B8%8B%E5%8D%888.28.04.png" alt=""><br>We compile the model and assign a weight of 0.2 to the auxiliary loss. To specify different loss_weights or loss for each different output, you can use a list or a dictionary. Here we pass a single loss as the loss argument, so the same loss will be used on all outputs.<br><img src="/img/media/Keras:getting%20started/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-16%20%E4%B8%8B%E5%8D%888.28.39.png" alt=""><br>We can train the model by passing it lists of input arrays and target arrays:<br><img src="/img/media/Keras:getting%20started/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-16%20%E4%B8%8B%E5%8D%888.28.59.png" alt=""><br>Since our inputs and outputs are named (we passed them a “name” argument), we could also have compiled the model via:<br><img src="/img/media/Keras:getting%20started/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-16%20%E4%B8%8B%E5%8D%888.29.23.png" alt=""></p>
<h1 id="Embedding"><a href="#Embedding" class="headerlink" title="Embedding"></a>Embedding</h1><p>keras.layers.Embedding(input_dim, output_dim, embeddings_initializer=‘uniform’, embeddings_regularizer=None, activity_regularizer=None, embeddings_constraint=None, mask_zero=False, input_length=None)</p>
<p>This layer can only be used as the first layer in a model.<br>Example<br><img src="/img/media/Keras:getting%20started/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-16%20%E4%B8%8B%E5%8D%887.27.54.png" alt=""></p>

          
        
      
    </div>

    

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://github.com/zdkswd/2019/04/16/转 逻辑回归LR的特征为什么要先离散化/"/>

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="ZDK"/>
      <meta itemprop="description" content=""/>
      <meta itemprop="image" content="/images/avatar.gif"/>
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ZDK's blog"/>
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/04/16/转 逻辑回归LR的特征为什么要先离散化/" class="post-title-link" itemprop="url">转 逻辑回归LR的特征为什么要先离散化</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-04-16 10:42:56 / 修改时间：10:43:14" itemprop="dateCreated datePublished" datetime="2019-04-16T10:42:56+08:00">2019-04-16</time>
            </span>
          

          
            

            
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/知识总结/" itemprop="url" rel="index"><span itemprop="name">知识总结</span></a></span>

                
                
              
            </span>
          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">评论数：</span>
                <a href="/2019/04/16/转 逻辑回归LR的特征为什么要先离散化/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2019/04/16/转 逻辑回归LR的特征为什么要先离散化/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
            <span id="/2019/04/16/转 逻辑回归LR的特征为什么要先离散化/" class="post-meta-item leancloud_visitors" data-flag-title="转 逻辑回归LR的特征为什么要先离散化">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              
                <span class="post-meta-item-text">阅读次数：</span>
              
                <span class="leancloud-visitors-count"></span>
            </span>
          

          

          

          <br/>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="转-逻辑回归LR的特征为什么要先离散化"><a href="#转-逻辑回归LR的特征为什么要先离散化" class="headerlink" title="转 逻辑回归LR的特征为什么要先离散化"></a>转 逻辑回归LR的特征为什么要先离散化</h1><p><a href="https://blog.csdn.net/yang090510118/article/details/39478033" target="_blank" rel="noopener">逻辑回归LR的特征为什么要先离散化 - yang090510118的专栏 - CSDN博客</a></p>
<p>在工业界，很少直接将连续值作为特征喂给逻辑回归模型，而是将连续特征离散化为一系列0、1特征交给逻辑回归模型，这样做的优势有以下几点：</p>
<ol>
<li>稀疏向量内积乘法运算速度快，计算结果方便存储，容易scalable（扩展）。</li>
<li>离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄&gt;30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰。</li>
<li>逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合。</li>
<li>离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力。</li>
<li>特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问。</li>
</ol>
<p>模型是使用离散特征还是连续特征，其实是一个“海量离散特征+简单模型” 同 “少量连续特征+复杂模型”的权衡。既可以离散化用线性模型，也可以用连续特征加深度学习。就看是喜欢折腾特征还是折腾模型了。通常来说，前者容易，而且可以n个人一起并行做，有成功经验；后者目前看很赞，能走多远还须拭目以待。</p>
<p>大概的理解：</p>
<p>1）计算简单<br>2）简化模型<br>3）增强模型的泛化能力，不易受噪声的影响</p>

          
        
      
    </div>

    

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/4/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/4/">4</a><span class="page-number current">5</span><a class="page-number" href="/page/6/">6</a><span class="space">&hellip;</span><a class="page-number" href="/page/28/">28</a><a class="extend next" rel="next" href="/page/6/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <div class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/avatar.gif"
                alt="ZDK"/>
            
              <p class="site-author-name" itemprop="name">ZDK</p>
              <div class="site-description motion-element" itemprop="description"></div>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">191</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">9</span>
                    <span class="site-state-item-name">分类</span>
                  
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">48</span>
                    <span class="site-state-item-name">标签</span>
                  
                </div>
              
            </nav>
          

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="https://github.com/zdkswd" title="GitHub &rarr; https://github.com/zdkswd"><i class="fa fa-fw fa-github"></i>GitHub</a>
                </span>
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="mailto:2822464407@qq.com" title="E-Mail &rarr; mailto:2822464407@qq.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                </span>
              
            </div>
          

          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-block">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                Links
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="https://github.com/zdkswd" title="https://github.com/zdkswd">Title</a>
                  </li>
                
              </ul>
            </div>
          

          
        </div>
      </div>

      

      
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          
        </div>
      

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">ZDK</span>

  

  
</div>


  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v3.7.1</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v7.2.0</div>




        








        
      </div>
    </footer>

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>










  
  













  
  <script src="/lib/jquery/index.js?v=3.4.1"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>




  <script src="/js/utils.js?v=7.2.0"></script>

  <script src="/js/motion.js?v=7.2.0"></script>



  
  


  <script src="/js/affix.js?v=7.2.0"></script>

  <script src="/js/schemes/pisces.js?v=7.2.0"></script>




  

  <script src="/js/next-boot.js?v=7.2.0"></script>

  

  

  

  

  
  

<script src="//unpkg.com/valine/dist/Valine.min.js"></script>

<script>
  var GUEST = ['nick', 'mail', 'link'];
  var guest = 'nick,mail,link';
  guest = guest.split(',').filter(function(item) {
    return GUEST.indexOf(item) > -1;
  });
  new Valine({
    el: '#comments',
    verify: false,
    notify: true,
    appId: 'QiU7UFIdgTTauFTk89N47mQS-gzGzoHsz',
    appKey: 'gkBx5soQkBREmER84PWbNJeM',
    placeholder: 'have fun',
    avatar: 'mm',
    meta: guest,
    pageSize: '10' || 10,
    visitor: true,
    lang: '' || 'zh-cn'
  });
</script>





  
  <script>
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url).replace(/\/{2,}/g, '/');
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('5');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x"></i></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x"></i></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  

  

  

  

  

  

  

  

  

  


  

</body>
</html>
