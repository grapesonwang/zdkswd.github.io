<!DOCTYPE html>












  


<html class="theme-next gemini use-motion" lang="zh-CN">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2"/>
<meta name="theme-color" content="#222"/>






















<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0"/>

<link rel="stylesheet" href="/css/main.css?v=7.2.0"/>


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.2.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.2.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.2.0">


  <link rel="mask-icon" href="/images/logo.svg?v=7.2.0" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '7.2.0',
    sidebar: {"position":"right","display":"post","offset":12,"onmobile":false},
    back2top: {"enable":true,"sidebar":true,"scrollpercent":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    translation: {
      copy_button: '复制',
      copy_success: '复制成功',
      copy_failure: '复制失败'
    }
  };
</script>



  <meta property="og:type" content="website">
<meta property="og:title" content="ZDK&#39;s blog">
<meta property="og:url" content="https://github.com/zdkswd/page/15/index.html">
<meta property="og:site_name" content="ZDK&#39;s blog">
<meta property="og:locale" content="zh-CN">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="ZDK&#39;s blog">



  <link rel="alternate" href="/atom.xml" title="ZDK's blog" type="application/atom+xml"/>



  
  
  <link rel="canonical" href="https://github.com/zdkswd/page/15/"/>



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>ZDK's blog</title>
  












  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <div class="container sidebar-position-right 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">ZDK's blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br/>首页</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
      
    

    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br/>标签</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">

    
    
      
    

    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br/>分类</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br/>归档</a>

  </li>

      
      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br/>搜索</a>
        </li>
      
    </ul>
  

  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
            

          
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://github.com/zdkswd/2018/11/20/精确率、召回率、真正类率（TPR）、假正类率（FPR）ROC曲线/"/>

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="ZDK"/>
      <meta itemprop="description" content=""/>
      <meta itemprop="image" content="/images/avatar.gif"/>
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ZDK's blog"/>
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2018/11/20/精确率、召回率、真正类率（TPR）、假正类率（FPR）ROC曲线/" class="post-title-link" itemprop="url">精确率、召回率、真正类率（TPR）、假正类率（FPR）ROC曲线</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2018-11-20 20:21:47" itemprop="dateCreated datePublished" datetime="2018-11-20T20:21:47+08:00">2018-11-20</time>
            </span>
          

          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2018-11-23 21:38:32" itemprop="dateModified" datetime="2018-11-23T21:38:32+08:00">2018-11-23</time>
              </span>
            
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/知识总结/" itemprop="url" rel="index"><span itemprop="name">知识总结</span></a></span>

                
                
              
            </span>
          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">评论数：</span>
                <a href="/2018/11/20/精确率、召回率、真正类率（TPR）、假正类率（FPR）ROC曲线/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2018/11/20/精确率、召回率、真正类率（TPR）、假正类率（FPR）ROC曲线/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
            <span id="/2018/11/20/精确率、召回率、真正类率（TPR）、假正类率（FPR）ROC曲线/" class="post-meta-item leancloud_visitors" data-flag-title="精确率、召回率、真正类率（TPR）、假正类率（FPR）ROC曲线">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              
                <span class="post-meta-item-text">阅读次数：</span>
              
                <span class="leancloud-visitors-count"></span>
            </span>
          

          

          

          <br/>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>已修改</p>
<h1 id="精确率、召回率、真正类率（TPR）、假正类率（FPR）ROC曲线"><a href="#精确率、召回率、真正类率（TPR）、假正类率（FPR）ROC曲线" class="headerlink" title="精确率、召回率、真正类率（TPR）、假正类率（FPR）ROC曲线"></a>精确率、召回率、真正类率（TPR）、假正类率（FPR）ROC曲线</h1><p>针对二分类问题。<br><img src="/img/media/%E7%B2%BE%E7%A1%AE%E7%8E%87%E3%80%81%E5%8F%AC%E5%9B%9E%E7%8E%87%E3%80%81%E7%9C%9F%E6%AD%A3%E7%B1%BB%E7%8E%87%EF%BC%88TPR%EF%BC%89%E3%80%81%E5%81%87%E6%AD%A3%E7%B1%BB%E7%8E%87%EF%BC%88FPR%EF%BC%89ROC%E6%9B%B2%E7%BA%BF/314331-2023b85ae2a8d04f.png" alt=""><br>精确率 = TP / (TP + FP)，表示返回的正例中真正例所占的比例；<br>召回率 = TP / (TP + FN)，表示返回的真正例占所有正例的比例。<br>真正类率（TPR），TPR = TP / (TP + FN)，返回的正类占所有正类的比例；（没错，跟召回率一个公式）TPR越大越好。<br>假正类率（FPR），FPR = FP / (FP + TN)，返回的负类占所有负类的比例。FPR越小越好<br>ROC curve:FPR越小越好。<br><img src="/img/media/%E7%B2%BE%E7%A1%AE%E7%8E%87%E3%80%81%E5%8F%AC%E5%9B%9E%E7%8E%87%E3%80%81%E7%9C%9F%E6%AD%A3%E7%B1%BB%E7%8E%87%EF%BC%88TPR%EF%BC%89%E3%80%81%E5%81%87%E6%AD%A3%E7%B1%BB%E7%8E%87%EF%BC%88FPR%EF%BC%89ROC%E6%9B%B2%E7%BA%BF/2394427-5f11fd1e6af07393.jpg" alt=""><br>对于ROC来说，横坐标就是FPR，而纵坐标就是TPR，因此可以想见，当 TPR越大，而FPR越小时，说明分类结果是较好的。</p>
<p>AUC:<br>AUC 即ROC曲线下的面积，计算方式即为ROC Curve的微积分值，其物理意义可以表示为：随机给定一正一负两个样本，将正样本排在负样本之前的概率，因此AUC越大，说明正样本越有可能被排在负样本之前，即分类的结果越好。</p>
<p>ROC的总结：</p>
<ol>
<li>ROC 可以反映二分类器的总体分类性能，但是无法直接从图中识别出分类最好的阈值，事实上最好的阈值也是视具体的场景所定；</li>
<li>ROC Curve 对应的AUC越大说明分类性能越好;</li>
<li>ROC曲线一定是需要在 y = x之上的，否则就是一个不理想的分类器；</li>
</ol>

          
        
      
    </div>

    

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://github.com/zdkswd/2018/11/20/GBDT+LR/"/>

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="ZDK"/>
      <meta itemprop="description" content=""/>
      <meta itemprop="image" content="/images/avatar.gif"/>
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ZDK's blog"/>
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2018/11/20/GBDT+LR/" class="post-title-link" itemprop="url">GBDT+LR</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2018-11-20 15:56:56 / 修改时间：20:03:32" itemprop="dateCreated datePublished" datetime="2018-11-20T15:56:56+08:00">2018-11-20</time>
            </span>
          

          
            

            
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/知识总结/" itemprop="url" rel="index"><span itemprop="name">知识总结</span></a></span>

                
                
              
            </span>
          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">评论数：</span>
                <a href="/2018/11/20/GBDT+LR/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2018/11/20/GBDT+LR/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
            <span id="/2018/11/20/GBDT+LR/" class="post-meta-item leancloud_visitors" data-flag-title="GBDT+LR">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              
                <span class="post-meta-item-text">阅读次数：</span>
              
                <span class="leancloud-visitors-count"></span>
            </span>
          

          

          

          <br/>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="GBDT"><a href="#GBDT" class="headerlink" title="GBDT"></a>GBDT</h1><p>gbdt在各种竞赛是大放异彩。原因大概有几个，一是效果确实挺不错。二是即可以用于分类也可以用于回归。三是可以筛选特征。</p>
<h2 id="GBDT如何选择特征"><a href="#GBDT如何选择特征" class="headerlink" title="GBDT如何选择特征"></a>GBDT如何选择特征</h2><p>gbdt选择特征的细节其实是想问你CART Tree生成的过程。这里有一个前提，gbdt的弱分类器默认选择的是CART TREE。其实也可以选择其他弱分类器的，选择的前提是低方差和高偏差。框架服从boosting 框架即可。</p>
<h2 id="GBDT如何构建特征"><a href="#GBDT如何构建特征" class="headerlink" title="GBDT如何构建特征"></a>GBDT如何构建特征</h2><p>其实说gbdt 能够构建特征并非很准确，gbdt 本身是不能产生特征的，但是我们可以利用gbdt去产生特征的组合。逻辑回归本身是适合处理线性可分的数据，如果我们想让逻辑回归处理非线性的数据，其中一种方式便是组合不同特征，增强逻辑回归对非线性分布的拟合能力。</p>
<h2 id="GBDT如何用于分类"><a href="#GBDT如何用于分类" class="headerlink" title="GBDT如何用于分类"></a>GBDT如何用于分类</h2><p>GBDT 无论用于分类还是回归一直都是使用的CART <strong>回归树</strong>。不会因为我们所选择的任务是分类任务就选用分类树。</p>
<h2 id="GB训练强学习器的思路"><a href="#GB训练强学习器的思路" class="headerlink" title="GB训练强学习器的思路"></a>GB训练强学习器的思路</h2><p><img src="/img/media/GBDT+LR/20180128125923199.png" alt=""><br><img src="/img/media/GBDT+LR/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-16%20%E4%B8%8B%E5%8D%887.49.46.png" alt=""><br><img src="/img/media/GBDT+LR/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-16%20%E4%B8%8B%E5%8D%887.50.37.png" alt=""><br><img src="/img/media/GBDT+LR/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-16%20%E4%B8%8B%E5%8D%887.51.41.png" alt=""></p>
<h2 id="GBDT原理"><a href="#GBDT原理" class="headerlink" title="GBDT原理"></a>GBDT原理</h2><p>对于任意的基分类器都可以利用GB的思想训练一个强分类器。而把基分类器选为决策树（DT)时，就是我们常用的GBDT。 </p>
<p>对于回归任务，当选择的loss function为Least-square。<br><img src="/img/media/GBDT+LR/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-16%20%E4%B8%8B%E5%8D%887.53.47.png" alt=""><br>伪代码为：<br><img src="/img/media/GBDT+LR/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-16%20%E4%B8%8B%E5%8D%887.54.12.png" alt=""><br><img src="/img/media/GBDT+LR/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-16%20%E4%B8%8B%E5%8D%887.54.38.png" alt=""></p>
<h2 id="GBDT例"><a href="#GBDT例" class="headerlink" title="GBDT例"></a>GBDT例</h2><p><img src="/img/media/GBDT+LR/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-16%20%E4%B8%8B%E5%8D%887.57.53.png" alt=""><br><img src="/img/media/GBDT+LR/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-16%20%E4%B8%8B%E5%8D%888.07.14.png" alt=""><br><img src="/img/media/GBDT+LR/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-16%20%E4%B8%8B%E5%8D%888.07.26.png" alt=""><br><img src="/img/media/GBDT+LR/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-16%20%E4%B8%8B%E5%8D%888.07.41.png" alt=""><br><img src="/img/media/GBDT+LR/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-16%20%E4%B8%8B%E5%8D%888.07.59.png" alt=""><br><img src="/img/media/GBDT+LR/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-16%20%E4%B8%8B%E5%8D%888.08.19.png" alt=""></p>
<h2 id="GDBT分类篇"><a href="#GDBT分类篇" class="headerlink" title="GDBT分类篇"></a>GDBT分类篇</h2><p>对于回归和分类，其实GBDT过程简直就是一模一样的。如果说最大的不同的话，那就是在于由于loss function不同而引起的初始化不同、叶子节点取值不同。<br><img src="/img/media/GBDT+LR/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-16%20%E4%B8%8B%E5%8D%888.12.08.png" alt=""></p>
<h2 id="分类例"><a href="#分类例" class="headerlink" title="分类例"></a>分类例</h2><p><img src="/img/media/GBDT+LR/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-16%20%E4%B8%8B%E5%8D%888.13.52.png" alt=""><br><img src="/img/media/GBDT+LR/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-16%20%E4%B8%8B%E5%8D%888.14.09.png" alt=""><br><img src="/img/media/GBDT+LR/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-16%20%E4%B8%8B%E5%8D%888.16.19.png" alt=""><br><img src="/img/media/GBDT+LR/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-16%20%E4%B8%8B%E5%8D%888.17.24.png" alt=""><br><img src="/img/media/GBDT+LR/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-16%20%E4%B8%8B%E5%8D%888.18.13.png" alt=""><br><img src="/img/media/GBDT+LR/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-16%20%E4%B8%8B%E5%8D%888.18.24.png" alt=""><br><img src="/img/media/GBDT+LR/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-16%20%E4%B8%8B%E5%8D%888.18.57.png" alt=""><br><img src="/img/media/GBDT+LR/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-16%20%E4%B8%8B%E5%8D%888.19.42.png" alt=""><br><img src="/img/media/GBDT+LR/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-16%20%E4%B8%8B%E5%8D%888.21.16.png" alt=""></p>
<p>多分类：<a href="https://blog.csdn.net/qq_22238533/article/details/79199605" target="_blank" rel="noopener">GBDT原理与实践-多分类篇 - SCUT_Sam - CSDN博客</a></p>
<h1 id="GBDT-LR"><a href="#GBDT-LR" class="headerlink" title="GBDT+LR"></a>GBDT+LR</h1><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>CTR预估（Click-Through Rate Prediction）是互联网计算广告中的关键环节，预估准确性直接影响公司广告收入。CTR预估中用的最多的模型是LR（Logistic Regression），LR是广义线性模型，与传统线性模型相比，LR使用了Logit变换将函数值映射到0~1区间，映射后的函数值就是CTR的预估值。LR这种线性模型很容易并行化，处理上亿条训练样本不是问题，但线性模型学习能力有限，需要大量特征工程预先分析出有效的特征、特征组合，从而去间接增强LR的非线性学习能力。</p>
<p>LR模型中的特征组合很关键， 但又无法直接通过特征笛卡尔积解决，只能依靠人工经验，耗时耗力同时并不一定会带来效果提升。如何自动发现有效的特征、特征组合，弥补人工经验不足，缩短LR特征实验周期，是亟需解决的问题。Facebook 2014年的文章介绍了通过GBDT（Gradient Boost Decision Tree）解决LR的特征组合问题，随后Kaggle竞赛也有实践此思路，GBDT与LR融合开始引起了业界关注。</p>
<p>GBDT（Gradient Boost Decision Tree）是一种常用的非线性模型，它基于集成学习中的boosting思想，每次迭代都在减少残差的梯度方向新建立一颗决策树，迭代多少次就会生成多少颗决策树。GBDT的思想使其具有天然优势可以发现多种有区分性的特征以及特征组合，决策树的路径可以直接作为LR输入特征使用，省去了人工寻找特征、特征组合的步骤。这种通过GBDT生成LR特征的方式（GBDT+LR），业界已有实践（Facebook，Kaggle-2014），且效果不错，是非常值得尝试的思路。</p>
<p>融合前人工寻找有区分性特征（raw feature）、特征组合、融合后直接通过黑盒子（Tree模型GBDT）进行特征、特种组合的自动发现。<br><img src="/img/media/GBDT+LR/2127249-a81329ce18881864.png" alt=""></p>
<p>在介绍这个模型之前，我们先来介绍两个问题：</p>
<ol>
<li>为什么要使用集成的决策树模型而不是单颗的决策树模型：<br>一棵树的表达能力很弱，不足以表达多个有区分性的特征组合，多棵树的表达能力更强一些。GBDT每棵树都在学习前面棵树尚存的不足，迭代多少次就会生成多少颗树。按paper以及Kaggle竞赛中的GBDT+LR融合方式，多棵树正好满足LR每条训练样本可以通过GBDT映射成多个特征的需求。</li>
<li>为什么建树采用GBDT而非RF：RF也是多棵树，但从效果上有实践证明不如GBDT。且GBDT前面的树，特征分裂主要体现对多数样本有区分度的特征；后面的树，主要体现的是经过前N颗树，残差仍然较大的少数样本。优先选用在整体上有区分度的特征，再选用针对少数样本有区分度的特征，思路更加合理，这应该也是用GBDT的原因。</li>
</ol>
<h2 id="GBDT-LR的结构"><a href="#GBDT-LR的结构" class="headerlink" title="GBDT+LR的结构"></a>GBDT+LR的结构</h2><p>GBDT用来对训练集提取特征作为新的训练输入数据，LR作为新训练输入数据的分类器。</p>
<p>具体步骤：</p>
<ol>
<li>GBDT首先对原始训练数据做训练，得到一个二分类器，当然这里也需要利用网格搜索寻找最佳参数组合。</li>
<li>与通常做法不同的是，当GBDT训练好做预测的时候，输出的并不是最终的二分类概率值，而是要把模型中的每棵树计算得到的预测概率值所属的叶子结点位置记为1，这样，就构造出了新的训练数据。在用GBDT构造新的训练数据时，采用的正是One-hot方法。并且由于每一弱分类器有且只有一个叶子节点输出预测结果，所以在一个具有n个弱分类器、共计m个叶子结点的GBDT中，每一条训练数据都会被转换为1*m维稀疏向量，且有n个元素为1，其余m-n 个元素全为0。</li>
<li>新的训练数据构造完成后，下一步就要与原始的训练数据中的label(输出)数据一并输入到Logistic Regression分类器中进行最终分类器的训练。思考一下，在对原始数据进行GBDT提取为新的数据这一操作之后，数据不仅变得稀疏，而且由于弱分类器个数，叶子结点个数的影响，可能会导致新的训练数据特征维度过大的问题，因此，在Logistic Regression这一层中，可使用正则化来减少过拟合的风险，在Facebook的论文中采用的是L1正则化。</li>
</ol>
<p>GBDT与LR的融合方式，Facebook的paper有个例子如下图2所示，图中Tree1、Tree2为通过GBDT模型学出来的两颗树，x为一条输入样本，遍历两棵树后，x样本分别落到两颗树的叶子节点上，每个叶子节点对应LR一维特征，那么通过遍历树，就得到了该样本对应的所有LR特征。由于树的每条路径，是通过最小化均方差等方法最终分割出来的有区分性路径，根据该路径得到的特征、特征组合都相对有区分性，效果理论上不会亚于人工经验的处理方式。<br><img src="/img/media/GBDT+LR/1473228-20180917183111311-2021770645.png" alt=""><br>论文中GBDT的参数，树的数量最多500颗（500以上就没有提升了），每棵树的节点不多于12。</p>
<p>GBDT模型的特点，非常适合用来挖掘有效的特征、特征组合。业界不仅GBDT+LR融合有实践，GBDT+FM也有实践，2014 Kaggle CTR竞赛冠军就是使用GBDT+FM（因子分解机），可见，使用GBDT融合其它模型是非常值得尝试的思路。</p>
<h2 id="RF-LR-Xgb-LR"><a href="#RF-LR-Xgb-LR" class="headerlink" title="RF + LR ? Xgb + LR?"></a>RF + LR ? Xgb + LR?</h2><p>例如Random Forest以及Xgboost等是并不是也可以按类似的方式来构造新的训练样本呢？没错，所有这些基于树的模型都可以和Logistic Regression分类器组合。</p>
<p>RF也是多棵树，但从效果上有实践证明不如GBDT。且GBDT前面的树，特征分裂主要体现对多数样本有区分度的特征；后面的树，主要体现的是经过前N颗树，残差仍然较大的少数样本。优先选用在整体上有区分度的特征，再选用针对少数样本有区分度的特征，思路更加合理，这应该也是用GBDT的原因。</p>
<h2 id="GBDT与LR融合具体方案"><a href="#GBDT与LR融合具体方案" class="headerlink" title="GBDT与LR融合具体方案"></a>GBDT与LR融合具体方案</h2><p>在CTR预估中，如何利用AD ID是一个问题。直接将AD ID作为特征建树不可行，而onehot编码过于稀疏，为每个AD ID建GBDT树，相当于发掘出区分每个广告的特征。而对于曝光不充分的样本即长尾部分，无法单独建树。</p>
<p>综合方案为：使用GBDT对非ID和ID分别建一类树。</p>
<ol>
<li>非ID类树：<br>不以细粒度的ID建树，此类树作为base，即这些ID一起构建GBDT。即便曝光少的广告、广告主，仍可以通过此类树得到有区分性的特征、特征组合。</li>
<li>ID类树：<br>以细粒度 的ID建一类树（每个ID构建GBDT），用于发现曝光充分的ID对应有区分性的特征、特征组合。</li>
</ol>
<p>如何根据GBDT建的两类树，对原始特征进行映射？当一条样本x进来之后，遍历两类树到叶子节点，得到的特征作为LR的输入。当AD曝光不充分不足以训练树时，其它树恰好作为补充。</p>
<p><img src="/img/media/GBDT+LR/GBDT%E5%8E%9F%E7%90%86%E5%8F%8A%E5%88%A9%E7%94%A8GBDT%E6%9E%84%E9%80%A0%E6%96%B0%E7%9A%84%E7%89%B9%E5%BE%814.jpg" alt=""><br>通过GBDT转换得到特征空间相比于原始ID低了很多。</p>
<h2 id="如何使用得到的特征？"><a href="#如何使用得到的特征？" class="headerlink" title="如何使用得到的特征？"></a>如何使用得到的特征？</h2><p>通过GBDT生成的特征，可直接作为LR的特征使用，省去人工处理分析特征的环节，LR的输入特征完全依赖于通过GBDT得到的特征。此思路已尝试，通过实验发现GBDT+LR在曝光充分的广告上确实有效果，但整体效果需要权衡优化各类树的使用。同时，也可考虑将GBDT生成特征与LR原有特征结合起来使用，待尝试。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>对于样本量大的数据，线性模型具有训练速度快的特点，但线性模型学习能力限于线性可分数据，所以就需要特征工程将数据尽可能地从输入空间转换到线性可分的特征空间。GBDT与LR的融合模型，其实使用GBDT来发掘有区分度的特征以及组合特征，来替代人工组合特征</p>
<h1 id="GBDT-LR-代码"><a href="#GBDT-LR-代码" class="headerlink" title="GBDT + LR 代码"></a>GBDT + LR 代码</h1><h2 id="sklearn多种模型ROC比较"><a href="#sklearn多种模型ROC比较" class="headerlink" title="sklearn多种模型ROC比较"></a>sklearn多种模型ROC比较</h2><p><a href="https://github.com/zdkswd/MLcode/blob/master/scikit-learn-code/sklearn-gbdt%2Blr.py">MLcode/sklearn-gbdt+lr.py at master · zdkswd/MLcode · GitHub</a></p>

          
        
      
    </div>

    

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://github.com/zdkswd/2018/11/16/提升 boosting/"/>

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="ZDK"/>
      <meta itemprop="description" content=""/>
      <meta itemprop="image" content="/images/avatar.gif"/>
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ZDK's blog"/>
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2018/11/16/提升 boosting/" class="post-title-link" itemprop="url">提升 boosting</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2018-11-16 16:08:47" itemprop="dateCreated datePublished" datetime="2018-11-16T16:08:47+08:00">2018-11-16</time>
            </span>
          

          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-03-26 13:26:22" itemprop="dateModified" datetime="2019-03-26T13:26:22+08:00">2019-03-26</time>
              </span>
            
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/知识总结/" itemprop="url" rel="index"><span itemprop="name">知识总结</span></a></span>

                
                
              
            </span>
          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">评论数：</span>
                <a href="/2018/11/16/提升 boosting/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2018/11/16/提升 boosting/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
            <span id="/2018/11/16/提升 boosting/" class="post-meta-item leancloud_visitors" data-flag-title="提升 boosting">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              
                <span class="post-meta-item-text">阅读次数：</span>
              
                <span class="leancloud-visitors-count"></span>
            </span>
          

          

          

          <br/>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="统计学习方法-提升方法"><a href="#统计学习方法-提升方法" class="headerlink" title="统计学习方法 提升方法"></a>统计学习方法 提升方法</h1><h2 id="AdaBoost"><a href="#AdaBoost" class="headerlink" title="AdaBoost"></a>AdaBoost</h2><p>在概率近似正确(probably approximately correct, <strong>PAC</strong>)学习的框架中，一个概念(一个类)，如果存在一个多项式的学习算法能够学习它，并且正确率很高，那么就称这个概念是<strong>强可学习</strong>的;一个概念，如果存在一个多项式的学习算法能够学习它,学习的正确率仅比随机猜测略好，那么就称这个概念是<strong>弱可学习</strong>的.</p>
<p>如果已经发现了“弱学习算法”，将它提升(boost) 为“强学习算法”.大家知道，发现弱学习算法通常要比发现强学习算法容易得多.那么如何具体实施提升，便成为开发提升方法时所要解决的问题.关于提升方法的研究很多，有很多算法被提出.最具代表性的是AdaBoost算法。</p>
<p>对于分类问题而言，给定一个训练样本集，求比较粗糙的分类规则(弱分类器)要比求精确的分类规则(强分类器)容易得多.提升方法就是从弱学习算法出发， 反复学习，得到一系列弱分类器(又称为基本分类器)，然后组合这些弱分类器，构成一个强分类器.大多数的提升方法都是改变训练数据的概率分布(训练数据的权值分布)，针对不同的训练数据分布调用弱学习算法学习一系列弱分类器.</p>
<p>对提升方法来说，有两个问题需要回答:一是在每一轮如何改变训练数据的权值或概率分布;二是如何将弱分类器组合成一个强分类器.关于第1个问题，AdaBoost的做法是，<strong>提高</strong>那些被前一轮弱分类器<strong>错误分类样本的权值</strong>，而<strong>降低</strong>那些被<strong>正确分类样本的权值</strong>.这样一来，那些没有得到正确分类的数据，由于其权值的加大而受到后一轮的弱分类器的更大关注.于是，分类问题被一系列的弱分类器“分而治之”.至于第2个问题，即弱分类器的组合，AdaBoost 采取加权多数表决的方法.具体地，<strong>加大</strong>分类<strong>误差率小</strong>的弱分类器的<strong>权值</strong>，使其在表决中起较大的作用，<strong>减小</strong>分类<strong>误差率大</strong>的弱分类器的<strong>权值</strong>，使其在表决中起较小的作用.</p>
<h3 id="AdaBoost算法"><a href="#AdaBoost算法" class="headerlink" title="AdaBoost算法"></a>AdaBoost算法</h3><p>给定一个二类分类的训练数据集。<br><img src="/img/media/%E6%8F%90%E5%8D%87%20boosting/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-12%20%E4%B8%8B%E5%8D%884.52.10.png" alt=""></p>
<h4 id="算法（AdaBoost）"><a href="#算法（AdaBoost）" class="headerlink" title="算法（AdaBoost）"></a>算法（AdaBoost）</h4><p><img src="/img/media/%E6%8F%90%E5%8D%87%20boosting/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-12%20%E4%B8%8B%E5%8D%884.53.06.png" alt=""><br><img src="/img/media/%E6%8F%90%E5%8D%87%20boosting/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-12%20%E4%B8%8B%E5%8D%884.55.56.png" alt=""></p>
<h3 id="AdaBoost的例子"><a href="#AdaBoost的例子" class="headerlink" title="AdaBoost的例子"></a>AdaBoost的例子</h3><p><img src="/img/media/%E6%8F%90%E5%8D%87%20boosting/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-12%20%E4%B8%8B%E5%8D%885.08.02.png" alt=""><br><img src="/img/media/%E6%8F%90%E5%8D%87%20boosting/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-12%20%E4%B8%8B%E5%8D%885.09.59.png" alt=""><br><img src="/img/media/%E6%8F%90%E5%8D%87%20boosting/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-12%20%E4%B8%8B%E5%8D%885.10.33.png" alt=""><br><img src="/img/media/%E6%8F%90%E5%8D%87%20boosting/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-12%20%E4%B8%8B%E5%8D%885.11.06.png" alt=""></p>
<h2 id="AdaBoost算法的训练误差分析"><a href="#AdaBoost算法的训练误差分析" class="headerlink" title="AdaBoost算法的训练误差分析"></a>AdaBoost算法的训练误差分析</h2><p>AdaBoost最基本的性质是它能在学习过程中不断减少训练误差，即在训练数据集上的分类误差率。</p>
<h3 id="定理（AdaBoost的训练误差界）"><a href="#定理（AdaBoost的训练误差界）" class="headerlink" title="定理（AdaBoost的训练误差界）"></a>定理（AdaBoost的训练误差界）</h3><p>AdaBoost算法最终分类器的训练误差界为：<br><img src="/img/media/%E6%8F%90%E5%8D%87%20boosting/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-12%20%E4%B8%8B%E5%8D%885.21.34.png" alt=""><br>这一定理说明，可以在每一轮选取适当的Gm使得Zm最小，从而使训练误差下降最快。对二类分类问题，有如下结果：</p>
<h3 id="定理（二类分类问题AdaBoost的训练误差界）"><a href="#定理（二类分类问题AdaBoost的训练误差界）" class="headerlink" title="定理（二类分类问题AdaBoost的训练误差界）"></a>定理（二类分类问题AdaBoost的训练误差界）</h3><p><img src="/img/media/%E6%8F%90%E5%8D%87%20boosting/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-12%20%E4%B8%8B%E5%8D%885.24.03.png" alt=""></p>
<h3 id="推论"><a href="#推论" class="headerlink" title="推论"></a>推论</h3><p><img src="/img/media/%E6%8F%90%E5%8D%87%20boosting/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-12%20%E4%B8%8B%E5%8D%885.24.50.png" alt=""><br>这表明在此条件下AdaBoost的训练误差是以指数速率下降的。这一性质当然很有吸引力。</p>
<p>AdaBoost算法不需要知道下界y.与早期的提升方法不同，AdaBoost具有适应性，即它能适应弱分类器各自的训练误差率。</p>
<h2 id="AdaBoost算法的解释"><a href="#AdaBoost算法的解释" class="headerlink" title="AdaBoost算法的解释"></a>AdaBoost算法的解释</h2><p>AdaBoost算法还有另一个解释，即可以认为AdaBoost算法是模型为加法模型、损失函数为指数函数、学习算法为前向分布算法时的二类分类学习方法。</p>
<h3 id="前向分布算法"><a href="#前向分布算法" class="headerlink" title="前向分布算法"></a>前向分布算法</h3><p>考虑加法模型<br><img src="/img/media/%E6%8F%90%E5%8D%87%20boosting/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-12%20%E4%B8%8B%E5%8D%886.01.16.png" alt=""><br>其中b（x;ym）为基函数，ym为基函数的参数，Bm为基函数的系数。</p>
<p>在给定训练数据及损失函数L（y，f（x））的条件下，学习加法模型f(x)成为损失函数极小化问题。<br><img src="/img/media/%E6%8F%90%E5%8D%87%20boosting/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-12%20%E4%B8%8B%E5%8D%886.06.13.png" alt=""><br>通常这是一个复杂得优化问题。前向分步算法求解这一优化问题的想法是，因为学习的是加法模型，如果能够从前向后，每一步只学习一个基函数及其系数，逐步逼近目标函数式，就可以简化优化的复杂度。具体的，每步只需优化损失函数：<br><img src="/img/media/%E6%8F%90%E5%8D%87%20boosting/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-12%20%E4%B8%8B%E5%8D%886.09.49.png" alt=""></p>
<h4 id="前向分步算法"><a href="#前向分步算法" class="headerlink" title="前向分步算法"></a>前向分步算法</h4><p><img src="/img/media/%E6%8F%90%E5%8D%87%20boosting/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-12%20%E4%B8%8B%E5%8D%886.12.17.png" alt=""><br><img src="/img/media/%E6%8F%90%E5%8D%87%20boosting/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-12%20%E4%B8%8B%E5%8D%886.14.37.png" alt=""></p>
<h3 id="前向分步算法与AdaBoost"><a href="#前向分步算法与AdaBoost" class="headerlink" title="前向分步算法与AdaBoost"></a>前向分步算法与AdaBoost</h3><h4 id="定理"><a href="#定理" class="headerlink" title="定理"></a>定理</h4><p>AdaBoost算法是前向分步算法加法算法的特例。这时模型是由基本分类器组成的加法模型，损失函数是指数函数。</p>
<h2 id="提升树"><a href="#提升树" class="headerlink" title="提升树"></a>提升树</h2><p>提升树是以分类树或回归树为基本分类器的提升方法.提升树被认为是统计学习中性能最好的方法之一。</p>
<h3 id="提升树模型"><a href="#提升树模型" class="headerlink" title="提升树模型"></a>提升树模型</h3><p>提升方法实际采用加法模型（即基函数的线性组合）与前向分步算法。以决策树为基函数的提升方法称为提升树（boosting tree）。对分类问题决策树是二叉分类树，对回归问题决策树是二叉回归树。基本分类器x&lt;v,或x&gt;v，可以看做是由一个根结点直接连接两个叶节点的简单决策树，即所谓的决策树桩。提升树模型可以表示为决策树的加法模型：<br><img src="/img/media/%E6%8F%90%E5%8D%87%20boosting/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-12%20%E4%B8%8B%E5%8D%8810.00.10.png" alt=""><br><img src="/img/media/%E6%8F%90%E5%8D%87%20boosting/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-12%20%E4%B8%8B%E5%8D%8810.00.39.png" alt=""></p>
<h3 id="提升树算法"><a href="#提升树算法" class="headerlink" title="提升树算法"></a>提升树算法</h3><p>提升树算法采用前向分步算法.首先确定初始提升树f0(x)=0,第m步的模型是<br><img src="/img/media/%E6%8F%90%E5%8D%87%20boosting/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-13%20%E4%B8%8A%E5%8D%8810.12.04.png" alt=""><br>其中，fm-1(x)为当前模型，通过最小化损失函数确定下一棵决策树的参数θm。<br><img src="/img/media/%E6%8F%90%E5%8D%87%20boosting/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-13%20%E4%B8%8A%E5%8D%8810.13.28.png" alt=""><br>由于树的线性组合可以很好地拟合训练数据，即使数据中的输入与输出之间的关系很复杂也是如此，所以提升树是一个高功能的学习算法.</p>
<p>针对不同问题的提升树学习算法，其主要区别在于使用的<strong>损失函数不同</strong>.包括用<strong>平方误差损失函数</strong>的<strong>回归</strong>问题，用<strong>指数损失</strong>函数的<strong>分类</strong>问题，以及用<strong>一般损失函数</strong>的<strong>一般决策问题</strong>.</p>
<h4 id="基函数是分类树（二叉分类树）"><a href="#基函数是分类树（二叉分类树）" class="headerlink" title="基函数是分类树（二叉分类树）"></a>基函数是分类树（二叉分类树）</h4><p>对于基函数是分类树时，我们使用指数损失函数，此时正是AdaBoost算法的特殊情况，即将AdaBoost算法中的基分类器使用分类树即可。</p>
<h4 id="回归问题的提升树算法（基函数是回归树）"><a href="#回归问题的提升树算法（基函数是回归树）" class="headerlink" title="回归问题的提升树算法（基函数是回归树）"></a>回归问题的提升树算法（基函数是回归树）</h4><p><img src="/img/media/%E6%8F%90%E5%8D%87%20boosting/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-13%20%E4%B8%8A%E5%8D%8810.18.48.png" alt=""><br><img src="/img/media/%E6%8F%90%E5%8D%87%20boosting/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-13%20%E4%B8%8A%E5%8D%8810.19.01.png" alt=""></p>
<h4 id="例"><a href="#例" class="headerlink" title="例"></a>例</h4><p><img src="/img/media/%E6%8F%90%E5%8D%87%20boosting/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-13%20%E4%B8%8A%E5%8D%8810.22.26.png" alt=""><br><img src="/img/media/%E6%8F%90%E5%8D%87%20boosting/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-13%20%E4%B8%8A%E5%8D%8810.22.47.png" alt=""><br><img src="/img/media/%E6%8F%90%E5%8D%87%20boosting/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-13%20%E4%B8%8A%E5%8D%8810.24.01.png" alt=""></p>
<h3 id="梯度提升"><a href="#梯度提升" class="headerlink" title="梯度提升"></a>梯度提升</h3><p>提升树利用加法模型与前向分步算法实现学习的优化过程.当损失函数是平方损失和指数损失函数时，每一步优化是很简单的.但对一一般损失函数而言，往往每一步优化并不那么容易.针对这一问题，Freidman 提出了梯度提升(gradient<br>boosting)算法，这是利用最速下降法的近似方法，其关键是利用损失函数负梯度在当前模型的值<br><img src="/img/media/%E6%8F%90%E5%8D%87%20boosting/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-13%20%E4%B8%8A%E5%8D%8810.27.46.png" alt=""><br>作为回归问题提升树算法中的<strong>残差的近似值</strong>，拟合一个<strong>回归树</strong>。</p>
<h4 id="梯度提升算法"><a href="#梯度提升算法" class="headerlink" title="梯度提升算法"></a>梯度提升算法</h4><p><img src="/img/media/%E6%8F%90%E5%8D%87%20boosting/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-13%20%E4%B8%8A%E5%8D%8810.31.09.png" alt=""><br><img src="/img/media/%E6%8F%90%E5%8D%87%20boosting/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-13%20%E4%B8%8A%E5%8D%8810.31.20.png" alt=""></p>
<h1 id="博客"><a href="#博客" class="headerlink" title="博客"></a>博客</h1><p>Bagging + 决策树 = 随机森林<br>AdaBoost + 决策树 = 提升树<br>Gradient Boosting + 决策树 = GBDT</p>
<h1 id="实践"><a href="#实践" class="headerlink" title="实践"></a>实践</h1><h2 id="AdaBoost单层决策树"><a href="#AdaBoost单层决策树" class="headerlink" title="AdaBoost单层决策树"></a>AdaBoost单层决策树</h2><p>分类问题。<br><img src="/img/media/%E6%8F%90%E5%8D%87%20boosting/mL_10_12.png" alt=""><br>可以看到，如果想要试着从某个坐标轴上选择一个值（即选择一条与坐标轴平行的直线）来将所有的蓝色圆点和橘色圆点分开，这显然是不可能的。这就是单层决策树难以处理的一个著名问题。通过使用多颗单层决策树，我们可以构建出一个能够对该数据集完全正确分类的分类器。<br><img src="/img/media/%E6%8F%90%E5%8D%87%20boosting/mL_10_13.png" alt=""><br>蓝横线上边的是一个类别，蓝横线下边是一个类别。显然，此时有一个蓝点分类错误，计算此时的分类误差，误差为1/5 = 0.2。这个横线与坐标轴的y轴的交点，就是我们设置的阈值，通过不断改变阈值的大小，找到使单层决策树的分类误差最小的阈值。同理，竖线也是如此，找到最佳分类的阈值，就找到了最佳单层决策树。</p>
<p>通过遍历，改变不同的阈值，计算最终的分类误差，找到分类误差最小的分类方式，即为我们要找的最佳单层决策树。这里lt表示less than，表示分类方式，对于小于阈值的样本点赋值为-1，gt表示greater than，也是表示分类方式，对于大于阈值的样本点赋值为-1。经过遍历，我们找到，训练好的最佳单层决策树的最小分类误差为0.2，就是对于该数据集，无论用什么样的单层决策树，分类误差最小就是0.2。这就是我们训练好的弱分类器。接下来，使用AdaBoost算法提升分类器性能，将分类误差缩短到0。此时使用AdaBoost提升分类器性能。<br><a href="https://github.com/zdkswd/MLcode/blob/master/%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0/AdaBoost%E5%8D%95%E5%B1%82%E5%86%B3%E7%AD%96%E6%A0%91.py">MLcode/AdaBoost单层决策树.py at master · zdkswd/MLcode · GitHub</a></p>
<p>通过改变样本的权值，会改变分类误差率，以选择分类误差率最小的弱选择器。</p>
<h2 id="scikit-learn-AdaBoost"><a href="#scikit-learn-AdaBoost" class="headerlink" title="scikit-learn AdaBoost"></a>scikit-learn AdaBoost</h2><p>sklearn.ensemble.AdaBoostClassifier共有五个参数，参数说明。</p>
<ol>
<li><strong>base_estimator</strong>:默认为<strong>DecisionTreeClassifier</strong>。理论上可以选择任何一个分类或者回归学习器，不过需要支持样本权重。AdaBoostClassifier默认使用CART分类树DecisionTreeClassifier，而AdaBoostRegressor默认使用CART回归树DecisionTreeRegressor。另外有一个要注意的点是，如果我们选择的AdaBoostClassifier算法是SAMME.R，则我们的弱分类学习器还需要支持概率预测，也就是在scikit-learn中弱分类学习器对应的预测方法除了predict还需要有predict_proba。</li>
<li><strong>algorithm</strong>：可选参数，默认为<strong>SAMME.R</strong>。scikit-learn实现了两种Adaboost分类算法，SAMME和SAMME.R。两者的主要区别是弱学习器权重的度量，SAMME使用对样本集分类效果作为弱学习器权重，而SAMME.R使用了对样本集分类的预测概率大小来作为弱学习器权重。由于SAMME.R使用了概率度量的连续值，迭代一般比SAMME快，因此AdaBoostClassifier的默认算法algorithm的值也是SAMME.R。我们一般使用默认的SAMME.R就够了，但是要注意的是使用了SAMME.R， 则弱分类学习器参数base_estimator必须限制使用支持概率预测的分类器。SAMME算法则没有这个限制。</li>
<li><strong>n_estimators</strong>：整数型，可选参数，默认为50。弱学习器的最大迭代次数，或者说最大的弱学习器的个数。一般来说n_estimators太小，容易欠拟合，n_estimators太大，又容易过拟合，一般选择一个适中的数值。默认是50。在实际调参的过程中，我们常常将n_estimators和下面介绍的参数learning_rate一起考虑。</li>
<li><strong>learning_rate</strong>：浮点型，可选参数，默认为1.0。每个弱学习器的权重缩减系数，取值范围为0到1，对于同样的训练集拟合效果，较小的v意味着我们需要更多的弱学习器的迭代次数。通常我们用步长和迭代最大次数一起来决定算法的拟合效果。所以这两个参数n_estimators和learning_rate要一起调参。一般来说，可以从一个小一点的v开始调参，默认是1。</li>
<li><strong>random_state</strong>：整数型，可选参数，默认为None。如果RandomState的实例，random_state是随机数生成器; 如果None，则随机数生成器是由np.random使用的RandomState实例。<br><a href="https://github.com/zdkswd/MLcode/tree/master/scikit-learn-code/scikit-learn-AdaBoost">https://github.com/zdkswd/MLcode/tree/master/scikit-learn-code/scikit-learn-AdaBoost</a></li>
</ol>

          
        
      
    </div>

    

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://github.com/zdkswd/2018/11/16/Practical Lessons from Predicting Clicks on Ads at Facebook/"/>

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="ZDK"/>
      <meta itemprop="description" content=""/>
      <meta itemprop="image" content="/images/avatar.gif"/>
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ZDK's blog"/>
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2018/11/16/Practical Lessons from Predicting Clicks on Ads at Facebook/" class="post-title-link" itemprop="url">Practical Lessons from Predicting Clicks on Ads at Facebook</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2018-11-16 15:48:56 / 修改时间：15:49:40" itemprop="dateCreated datePublished" datetime="2018-11-16T15:48:56+08:00">2018-11-16</time>
            </span>
          

          
            

            
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/论文/" itemprop="url" rel="index"><span itemprop="name">论文</span></a></span>

                
                
              
            </span>
          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">评论数：</span>
                <a href="/2018/11/16/Practical Lessons from Predicting Clicks on Ads at Facebook/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2018/11/16/Practical Lessons from Predicting Clicks on Ads at Facebook/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
            <span id="/2018/11/16/Practical Lessons from Predicting Clicks on Ads at Facebook/" class="post-meta-item leancloud_visitors" data-flag-title="Practical Lessons from Predicting Clicks on Ads at Facebook">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              
                <span class="post-meta-item-text">阅读次数：</span>
              
                <span class="leancloud-visitors-count"></span>
            </span>
          

          

          

          <br/>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><a href="https://dl.acm.org/citation.cfm?id=2648589" target="_blank" rel="noopener">https://dl.acm.org/citation.cfm?id=2648589</a></p>
<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>在线广告允许广告商只投标和支付可测量的用户响应，比如广告的点击。所以，点击预测系统时大多数广告系统的中心。有超过7.5亿的日活用户和超过一百万的活跃广告商，预测Facebook广告点击是一项有挑战的机器学习任务。这篇论文介绍一个决策树加逻辑回归的组合模型，使用组合模型的效果比单独使用模型的效果提升3%，这会对整体系统的表现的提升产生重大影响。作者探索了一些基础参数怎么影响系统的最终预测表现。不出意外的是，最重要的事情是拥有正确的特征：捕获到的用户和广告的历史信息支配着其他类型的特征。一旦我们有了正确的特征和正确的模型（决策树加逻辑回归），其他的因素就影响很小（虽然小的改进在规模上很重要）。选择对数据新鲜度，学习率模式，和数据采样的最佳处理能够轻微的改变模型，但远不如一开始就选择高价值的特征或选择正确的模型。</p>
<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>这篇论文的目的就是分享用真实世界的数据，并且具有鲁棒性和适应性的实验中得到的见解。</p>
<p>Facebook由于其特殊性，不能使用搜索记录进行广告推荐，而是基于对用户的定位，所以可展示广告的量也更多。Facebook为此建立的<strong>级联分类器</strong>。这篇文章专注于级联分类器的最后一个阶段点击预测模型，就是这个模型对最终的候选广告集的广告进行预测是否会被点击。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">*级联分类器*</span><br><span class="line">级联是基于几个分类器的串联的集合学习的特定情况，使用从给定分类器的输出收集的所有信息作为级联中的下一个分类器的附加信息。与投票或堆叠合奏（多专家系统）不同，级联是多阶段的。</span><br></pre></td></tr></table></figure></p>
<h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p>主要介绍实验计划。<br><strong>实验数据</strong>：2013年第4季度任意1周的离线训练数据，与线上数据相似。分为训练集和测试集，并且使用它们模拟在线训练和预测的流数据。论文中实验使用的都是相同的训练/测试数据。</p>
<p><strong>评估指标</strong>：<br><strong>Normalized Entropy</strong> （NE），NE定义为：<br><img src="/img/media/Practical%20Lessons%20from%20Predicting%20Clicks%20on%20Ads%20at%20Facebook/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-16%20%E4%B8%8B%E5%8D%881.26.45.png" alt=""><br>其中yi属于{-1，+1}，p为经验点击通过率CTR（即广告实际点击次数除以广告展示量）。NE在计算相关的信息增益时是至关重要的。上面是逻辑回归的损失函数，也就是交叉熵。下面是个常数，所以越小值，模型越好。</p>
<p><strong>Calibration</strong> （刻度标度）：评价估计CTR和经验CTR的比率。即预期点击次数与实际观察点击次数的比率。越接近1模型效果越好。</p>
<h1 id="预测模型结构"><a href="#预测模型结构" class="headerlink" title="预测模型结构"></a>预测模型结构</h1><p>评估不同的概率线性分类器和不同的在线学习算法。<br>混合模型结构：提升决策树和概率稀疏线性分类器的串联。<br>学习算法是用的是Stochastic Gradient Descent(SGD)，或者Bayesian online learning scheme for probit regression(BOPR)都可以。但是最终选择的是SGD，原因是资源消耗要小一些。<br>SGD和BOPR都可以针对单个样本进行训练，所以他们可以做成流式的学习器(stream learner)。</p>
<h2 id="决策树特征转换"><a href="#决策树特征转换" class="headerlink" title="决策树特征转换"></a>决策树特征转换</h2><p>对于一个样本，针对每一颗树得到一个类别型特征。该特征取值为样本在树中落入的叶节点的编号。<br><img src="/img/media/Practical%20Lessons%20from%20Predicting%20Clicks%20on%20Ads%20at%20Facebook/15288567782120.jpg" alt=""><br>上图中的提升决策树包含两棵子树，第一棵树包含3个叶节点，第二棵树包含2个叶节点。输入样本x，在两棵树种分别落入叶子节点2和叶子节点1。那么特征转换就得到特征向量[0 1 0 1 0]。也就是说，把叶节点编号进行one-hot编码。</p>
<p>直观的理解这种特征变化：</p>
<ol>
<li>看做是一种有监督的特征编码。把实值的vector转化为紧凑的二值的vector。</li>
<li>从根节点到叶节点的一条路径，表示的是在特征上的一个特定的规则。所以，叶节点的编号代表了这种规则。表征了样本中的信息，而且进行了非线性的组合变换。</li>
<li>最后再对叶节点编号组合，相当于学习这些规则的权重。</li>
</ol>
<p><img src="/img/media/Practical%20Lessons%20from%20Predicting%20Clicks%20on%20Ads%20at%20Facebook/15288712733387.jpg" alt=""></p>
<h2 id="Data-freshness"><a href="#Data-freshness" class="headerlink" title="Data freshness"></a>Data freshness</h2><p>一天的数据作为训练集，其后的一天活几天作为测试数据。<br><img src="/img/media/Practical%20Lessons%20from%20Predicting%20Clicks%20on%20Ads%20at%20Facebook/15288714646409.jpg" alt=""><br>可以发现随着天数的增加，data freshness也变得越来越差，模型的性能也越来越差。</p>
<p>一种做法是说每天都重新训练。即使是mini-batch来训练，也会非常耗时。提升树的训练时间受很多因素的影响，比如：样本数量、树深度、树数量、叶子节点个数等。为了加快速度，可以在多CPU上通过并行化来实现。</p>
<p>我们可以：</p>
<ol>
<li>提升树可以一天或者几天来训练一次。</li>
<li>LR可以实现在线学习，几乎是实时训练。</li>
</ol>
<h2 id="LR线性分类器"><a href="#LR线性分类器" class="headerlink" title="LR线性分类器"></a>LR线性分类器</h2><p>针对Logistic Regression进行在线增量训练。也就是说只要用户点击了广告，生成了新的样本，就进行增量训练。</p>
<p>Facebook针对SGD-based online learning研究了5中学习速率的设置方式。<br><img src="/img/media/Practical%20Lessons%20from%20Predicting%20Clicks%20on%20Ads%20at%20Facebook/15288719387888.jpg" alt=""></p>
<ol>
<li>前三种使得不同的参数有不同的学习速率</li>
<li>后两种对于所有的参数都是用相同的学习速率<br>实验结果显示Per-coordinate learning rate效果最好：<br><img src="/img/media/Practical%20Lessons%20from%20Predicting%20Clicks%20on%20Ads%20at%20Facebook/15288720145598.jpg" alt=""><h1 id="线上模型架构"><a href="#线上模型架构" class="headerlink" title="线上模型架构"></a>线上模型架构</h1>最关键的步骤就是把labels(click/no-click)和训练输入(ad impressions)以一种在线的方式连起(join)起来。所以系统被称为online data joiner。<h2 id="label标注"><a href="#label标注" class="headerlink" title="label标注"></a>label标注</h2>首先设定一个足够长的阈值。一个广告展示给用户之后，如果用户在阈值的时间内没有点击广告就标记为no-click，点击了的话就标记为click。这个等待的时间窗口需要非常小心的调整。<br>如果太长了，会增加缓存impression的内存消耗，而且影响实时数据的产生；如果太短了则会导致丢失一部分的点击样本，会影响click converage 点击覆盖。<br>click converage 点击覆盖表示有多少个点击行为被记录了下来生成了样本。online data joiner必须保证尽可能高的点击覆盖，也就是尽可能多的来记录下来所有的点击行为。但是如果等待太久就会增加缓存开销等影响。所以online data joiner必须在click converage和资源消耗之间做出平衡<h2 id="模型架构"><a href="#模型架构" class="headerlink" title="模型架构"></a>模型架构</h2><img src="/img/media/Practical%20Lessons%20from%20Predicting%20Clicks%20on%20Ads%20at%20Facebook/15288063310673.jpg" alt=""><h1 id="处理大量的训练数据"><a href="#处理大量的训练数据" class="headerlink" title="处理大量的训练数据"></a>处理大量的训练数据</h1>很多的计算广告领域的训练数据量都是非常巨大的，那么如何有效的控制训练带来的开销就非常重要。常用的办法是采样<h2 id="均匀采样"><a href="#均匀采样" class="headerlink" title="均匀采样"></a>均匀采样</h2>均匀采样非常的简单，易于实现。而且使用均匀采样没有改变训练数据的分布，所以模型不需要修改就可以直接应用于测试数据。</li>
</ol>
<p>不同采样率对模型性能的影响：<br><img src="/img/media/Practical%20Lessons%20from%20Predicting%20Clicks%20on%20Ads%20at%20Facebook/15288758580098.jpg" alt=""></p>
<h2 id="Negative-down-sampling"><a href="#Negative-down-sampling" class="headerlink" title="Negative down sampling"></a>Negative down sampling</h2><p>计算广告中大部分的训练样本都极度不平衡，这对模型会造成很大影响。一种解决办法就是对负样本进行欠采样。<br><img src="/img/media/Practical%20Lessons%20from%20Predicting%20Clicks%20on%20Ads%20at%20Facebook/15288761292102.jpg" alt=""><br>可以看到采样率不同，对模型性能影响很大。采样率为0.025的时候取得最好结果。</p>
<h2 id="Model-Re-Calibration"><a href="#Model-Re-Calibration" class="headerlink" title="Model Re-Calibration"></a>Model Re-Calibration</h2><p>负样本欠采样可以加快训练速度并提升模型性能。但是同样带来了问题：改变了训练数据分布。所以需要进行校准。<br>举例来说，采样之前CTR均值为0.1%，使用0.01采样之后，CTR均值变为10%。我们需要对模型进行Calibration(校准)使得模型在实际预测的时候恢复成0.1%。调整公式如下：<br><img src="/img/media/Practical%20Lessons%20from%20Predicting%20Clicks%20on%20Ads%20at%20Facebook/15288763715313.jpg" alt=""><br>其中w是采样率，p是在采样后空间中给出的CTR预估值，计算得到的q就是修正后的结果。</p>
<h1 id="其他实验结果"><a href="#其他实验结果" class="headerlink" title="其他实验结果"></a>其他实验结果</h1><p>所有的这些探索都是为了能够平衡模型性能(accuracy)和资源消耗(内存、CPU)。只有当你充分了解模型和数据每个部分后，才能根据实际情况做出最佳的取舍。</p>
<h2 id="Number-of-boosting-trees"><a href="#Number-of-boosting-trees" class="headerlink" title="Number of boosting trees"></a>Number of boosting trees</h2><p><img src="/img/media/Practical%20Lessons%20from%20Predicting%20Clicks%20on%20Ads%20at%20Facebook/15288739728495.jpg" alt=""><br>boosting tree数量从1到2000，叶节点个数被限制为最大12个。submodel之间的区别在于训练数据大小的不同，比如submodel 2的训练数据只有前面两个的1/4。<br>可以看到随着boosting tree数量的增加，模型的性能有所提升。但是几乎所有的提升都来自于前500个trees，而后面的1000个trees的提升甚至都不到0.1%。submodel 2在1000颗trees甚至模型效果在变差，原因是过拟合。</p>
<h2 id="Boosting-feature-importance"><a href="#Boosting-feature-importance" class="headerlink" title="Boosting feature importance"></a>Boosting feature importance</h2><p><img src="/img/media/Practical%20Lessons%20from%20Predicting%20Clicks%20on%20Ads%20at%20Facebook/15288742530967.jpg" alt=""><br>上图首先对特征按照重要程度来进行排序，编号后再画图。特征重要程度按照使用该特征进行分裂，所带来的loss减小的累积量。因为一个特征可以在多颗树上进行使用，所以累积要在所有的树上进行。</p>
<p>上图中，黄线表示对特征进行累加后的值，然后进行log变换。可以看到最终结果是1，表示所有特征的重要度总和是1. 最重要的是期初非常陡峭，上升的非常快，说明特征重要度主要集中在top10这些特征中。前10个特征，贡献了50%的重要度，后面300个特征，贡献了1%的重要度。<br><img src="/img/media/Practical%20Lessons%20from%20Predicting%20Clicks%20on%20Ads%20at%20Facebook/15288746104351.jpg" alt=""><br>显然，全部去掉是不行的。说明大量弱特征的累积也是很重要的，但是去掉部分不那么重要的特征，对模型的影响比较小，比如从400到200。</p>
<h2 id="Historical-features-VS-Context-features"><a href="#Historical-features-VS-Context-features" class="headerlink" title="Historical features VS Context features"></a>Historical features VS Context features</h2><p>针对两大类特征：历史信息特征（用户+广告）、上下文特征。论文还研究了这两类特征对模型性能的贡献程度。先给出结论：历史信息特征占主导地位。<br><img src="/img/media/Practical%20Lessons%20from%20Predicting%20Clicks%20on%20Ads%20at%20Facebook/15288750694203.jpg" alt=""><br>同样，先把特征按照重要程度排序，再画图。横轴是特征数量，纵轴是historical特征在top k个重要特征中所占的百分比。可以看到前10个特征中，全是历史信息特征；前20个特征中，只有2个上下文特征。所以：历史信息特征比上下文特征重要太多了。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>Facebook提出的LR + GBDT来提取非线性特征进行特征组合的方式非常经典，主要特性总结如下：</p>
<ol>
<li>Data Freshness很重要。模型至少一天需要重新训练一次</li>
<li>使用Boosted Decision Tree进行特征转换很大程度上提高了模型的性能</li>
<li>最好的在线学习方法：LR + per-coordinate learning rate</li>
</ol>
<p>关于平衡计算开销和模型性能所采用的技巧：</p>
<ol>
<li>调整Boosted decision trees数量</li>
<li>去掉部分重要性低的特征，对模型的影响比较小。但是不能全去掉</li>
<li>相比于上下文特征，用户/广告历史特征要重要的多</li>
<li>针对大量训练数据可以进行欠采样</li>
</ol>

          
        
      
    </div>

    

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://github.com/zdkswd/2018/11/16/回归/"/>

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="ZDK"/>
      <meta itemprop="description" content=""/>
      <meta itemprop="image" content="/images/avatar.gif"/>
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ZDK's blog"/>
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2018/11/16/回归/" class="post-title-link" itemprop="url">回归</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2018-11-16 12:34:47" itemprop="dateCreated datePublished" datetime="2018-11-16T12:34:47+08:00">2018-11-16</time>
            </span>
          

          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-03-27 19:50:34" itemprop="dateModified" datetime="2019-03-27T19:50:34+08:00">2019-03-27</time>
              </span>
            
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/知识总结/" itemprop="url" rel="index"><span itemprop="name">知识总结</span></a></span>

                
                
              
            </span>
          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">评论数：</span>
                <a href="/2018/11/16/回归/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2018/11/16/回归/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
            <span id="/2018/11/16/回归/" class="post-meta-item leancloud_visitors" data-flag-title="回归">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              
                <span class="post-meta-item-text">阅读次数：</span>
              
                <span class="leancloud-visitors-count"></span>
            </span>
          

          

          

          <br/>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="统计学习方法-逻辑斯蒂回归与最大熵模型"><a href="#统计学习方法-逻辑斯蒂回归与最大熵模型" class="headerlink" title="统计学习方法 逻辑斯蒂回归与最大熵模型"></a>统计学习方法 逻辑斯蒂回归与最大熵模型</h1><p>逻辑斯蒂回归与最大熵模型都属于对数线性模型。</p>
<h2 id="逻辑斯蒂回归模型"><a href="#逻辑斯蒂回归模型" class="headerlink" title="逻辑斯蒂回归模型"></a>逻辑斯蒂回归模型</h2><h3 id="逻辑斯蒂分布"><a href="#逻辑斯蒂分布" class="headerlink" title="逻辑斯蒂分布"></a>逻辑斯蒂分布</h3><h4 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h4><p>设X是连续随机变量，X服从逻辑斯蒂分布时指X具有下列分布函数和密度函数：<br><img src="/img/media/%E5%9B%9E%E5%BD%92/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-15%20%E4%B8%8B%E5%8D%8812.43.54.png" alt=""><br>u为位置参数，y&gt;0为形状参数。<br><img src="/img/media/%E5%9B%9E%E5%BD%92/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-15%20%E4%B8%8B%E5%8D%8812.45.02.png" alt=""></p>
<h3 id="二项逻辑斯蒂回归模型"><a href="#二项逻辑斯蒂回归模型" class="headerlink" title="二项逻辑斯蒂回归模型"></a>二项逻辑斯蒂回归模型</h3><p>二项逻辑斯蒂回归模型是一种分类模型，形式为参数化的逻辑斯蒂分布。随机变量X取值为实数，随机变量Y取值为1或0。我们通过监督学习方法来估计模型参数。</p>
<h4 id="定义（逻辑斯蒂回归模型）"><a href="#定义（逻辑斯蒂回归模型）" class="headerlink" title="定义（逻辑斯蒂回归模型）"></a>定义（逻辑斯蒂回归模型）</h4><p>二项逻辑斯蒂回归模型是如下的条件概率分布：<br><img src="/img/media/%E5%9B%9E%E5%BD%92/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-15%20%E4%B8%8B%E5%8D%8812.54.07.png" alt=""><br>x是输入，Y取{0，1}是输出，w，b是参数，w称为权值向量，b称为偏置。对于输入实例x，逻辑回归比较两个条件概率值的大小，将x分到概率值较大的那一类。<br><img src="/img/media/%E5%9B%9E%E5%BD%92/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-15%20%E4%B8%8B%E5%8D%881.04.39.png" alt=""><br>一个事件的几率（odds）指该事件发生的概率与该事件不发生的概率的比值，事件发生的概率为p，则事件的对数几率（log odds）或logit函数是<br><img src="/img/media/%E5%9B%9E%E5%BD%92/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-15%20%E4%B8%8B%E5%8D%881.11.42.png" alt=""><br>即输出Y=1的对数几率是由输入x的线性函数表示的模型，即逻辑斯蒂回归模型。<br>换一个角度看，考虑对输入x进行分类的线性函数wx,其值域为实数域。通过逻辑斯蒂回归模型可以将线性函数wx转换为概率：<br><img src="/img/media/%E5%9B%9E%E5%BD%92/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-15%20%E4%B8%8B%E5%8D%881.22.22.png" alt=""><br>这时，线性函数的值越接近正无穷，概率值就越接近1，线性函数的值越接近负无穷，概率值就越接近0.</p>
<h3 id="模型参数估计"><a href="#模型参数估计" class="headerlink" title="模型参数估计"></a>模型参数估计</h3><p>可以应用极大似然估计法估计模型参数，从而得到逻辑斯蒂回归模型。<br><img src="/img/media/%E5%9B%9E%E5%BD%92/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-15%20%E4%B8%8B%E5%8D%881.41.04.png" alt=""></p>
<h3 id="多项逻辑斯蒂回归"><a href="#多项逻辑斯蒂回归" class="headerlink" title="多项逻辑斯蒂回归"></a>多项逻辑斯蒂回归</h3><p>假设离散型随机变量Y的取值集合是{1,2,…,K}，那么多项逻辑斯蒂回归模型是：<br><img src="/img/media/%E5%9B%9E%E5%BD%92/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-15%20%E4%B8%8B%E5%8D%881.44.44.png" alt=""><br>二项逻辑斯蒂回归的参数估计法也可以推广到多项逻辑斯蒂回归。</p>
<h2 id="最大熵模型"><a href="#最大熵模型" class="headerlink" title="最大熵模型"></a>最大熵模型</h2><p>最大熵原理是概率模型学习的一个准则.最大熵原理认为，学习概率模型时，在所有可能的概率模型(分布)中，熵最大的模型是最好的模型.通常用约束条件来确定概率模型的集合，所以，最大熵原理也可以表述为在满足约束条件的模型集合中<strong>选取熵最大的模型</strong>.</p>
<p>假设离散随机变量X的概率分布是P（X），则其熵是<br><img src="/img/media/%E5%9B%9E%E5%BD%92/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-15%20%E4%B8%8B%E5%8D%881.49.21.png" alt=""><br>当X服从均匀分布时，熵最大。</p>
<h1 id="博客"><a href="#博客" class="headerlink" title="博客"></a>博客</h1><h2 id="梯度上升算法"><a href="#梯度上升算法" class="headerlink" title="梯度上升算法"></a>梯度上升算法</h2><p><img src="/img/media/%E5%9B%9E%E5%BD%92/ml_6_5.jpg" alt=""><br><img src="/img/media/%E5%9B%9E%E5%BD%92/ml_6_6.png" alt=""><br>z是一个矩阵，θ是参数列向量(要求解的)，x是样本列向量(给定的数据集)。θ^T表示θ的转置。g(z)函数实现了任意实数到[0,1]的映射，这样我们的数据集([x0,x1,…,xn])，不管是大于1或者小于0，都可以映射到[0,1]区间进行分类。hθ(x)给出了输出为1的概率。比如当hθ(x)=0.7，那么说明有70%的概率输出为1。输出为0的概率是输出为1的补集，也就是30%。如果这个概率大于0.5，我们就可以说样本是正样本，否则样本是负样本。</p>
<p>对于正样本，概率越接近一，分类效果越好，对于负样本，1-负样本的值越接近1越好。对于正样本和负样本两个公式合二为一：<br><img src="/img/media/%E5%9B%9E%E5%BD%92/ml_6_8.png" alt=""><br>为了简化问题，对表达式求对数。<br><img src="/img/media/%E5%9B%9E%E5%BD%92/ml_6_9.png" alt=""><br>至此，针对一个样本的代价函数已经出来了，假定样本与样本之间相互独立，整个样本集生成的概率即为所有样本生成概率的乘积，再将公式对数化，便得到公式：<br><img src="/img/media/%E5%9B%9E%E5%BD%92/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-15%20%E4%B8%8B%E5%8D%886.54.48.png" alt=""><br>满足J(θ)的最大的θ值即是我们需要求解的模型。<br>由于是求最大值，所以我们需要使用梯度上升算法，也就是求负值的最小也就是梯度下降。<br><img src="/img/media/%E5%9B%9E%E5%BD%92/ml_6_25.png" alt=""><br><img src="/img/media/%E5%9B%9E%E5%BD%92/ml_6_26.png" alt=""></p>
<h2 id="逻辑回归的优缺点"><a href="#逻辑回归的优缺点" class="headerlink" title="逻辑回归的优缺点"></a>逻辑回归的优缺点</h2><p>如何凸显你是一个对逻辑回归已经非常了解的人呢。那就是用一句话概括它！逻辑回归假设数据服从伯努利分布,通过极大化似然函数的方法，运用梯度下降来求解参数，来达到将数据二分类的目的。</p>
<p>逻辑回归应用到工业界中的一些优点：</p>
<ol>
<li>形式简单，模型的可解释性非常好。从特征的权重可以看到不同的特征对最后结果的影响，某个特征的权重值比较高，那么这个特征最后对结果的影响会比较大。</li>
<li>模型效果不错。在工程上是可以接受的（作为baseline)，如果特征工程做的好，效果不会太差，并且特征工程可以大家并行开发，大大加快开发的速度。</li>
<li>训练速度较快。分类的时候，计算量仅仅只和特征的数目相关。并且逻辑回归的分布式优化sgd发展比较成熟，训练的速度可以通过堆机器进一步提高，这样我们可以在短时间内迭代好几个版本的模型。</li>
<li>资源占用小,尤其是内存。因为只需要存储各个维度的特征值。</li>
<li>方便输出结果调整。逻辑回归可以很方便的得到最后的分类结果，因为输出的是每个样本的概率分数，我们可以很容易的对这些概率分数进行cutoff，也就是划分阈值(大于某个阈值的是一类，小于某个阈值的是一类)。</li>
</ol>
<p>缺点：</p>
<ol>
<li>准确率并不是很高。因为形式非常的简单(非常类似线性模型)，很难去拟合数据的真实分布。</li>
<li>很难处理数据不平衡的问题。</li>
<li>处理非线性数据较麻烦。逻辑回归在不引进其他方法的情况下，只能处理线性可分的数据，或者进一步说，处理二分类的问题。</li>
<li>逻辑回归本身无法筛选特征。有时候，我们会用gbdt来筛选特征，然后再上逻辑回归。</li>
</ol>
<h1 id="实践"><a href="#实践" class="headerlink" title="实践"></a>实践</h1><h2 id="AUC"><a href="#AUC" class="headerlink" title="AUC"></a>AUC</h2><p>评价分类的指标。<br><img src="/img/media/%E5%9B%9E%E5%BD%92/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-04%20%E4%B8%8B%E5%8D%882.40.33.png" alt=""></p>
<h2 id="差分与取对数"><a href="#差分与取对数" class="headerlink" title="差分与取对数"></a>差分与取对数</h2><p><img src="/img/media/%E5%9B%9E%E5%BD%92/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-04%20%E4%B8%8B%E5%8D%882.54.16.png" alt=""><br><img src="/img/media/%E5%9B%9E%E5%BD%92/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-04%20%E4%B8%8B%E5%8D%883.01.14.png" alt=""><br>滑动平均值是从一个有n项的时间序列中来计算多个连续m项序列的平均值。</p>
<h2 id="多项式回归"><a href="#多项式回归" class="headerlink" title="多项式回归"></a>多项式回归</h2><p>关键词：正则化，get_Variable,sess.run<br><a href="https://github.com/zdkswd/myTensorflowExamples/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%A4%9A%E9%A1%B9%E5%BC%8F%E5%9B%9E%E5%BD%92.py">https://github.com/zdkswd/myTensorflowExamples/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%A4%9A%E9%A1%B9%E5%BC%8F%E5%9B%9E%E5%BD%92.py</a></p>
<h2 id="逻辑回归与Softmax回归"><a href="#逻辑回归与Softmax回归" class="headerlink" title="逻辑回归与Softmax回归"></a>逻辑回归与Softmax回归</h2><p>softmax就是多分类的逻辑回归，使用的损失函数是交叉熵，交叉熵就是负的逻辑回归的极大似然估计。<br>softmax成为了激活函数。<br><a href="https://github.com/zdkswd/TensorFlow-Examples/blob/master/examples/2_BasicModels/logistic_regression.py">https://github.com/zdkswd/TensorFlow-Examples/blob/master/examples/2_BasicModels/logistic_regression.py</a></p>
<h2 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h2><p>从博客内容可知，为了使J(θ)最大，就尽可能的分对。<br><a href="https://github.com/zdkswd/MLcode/tree/master/%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92">https://github.com/zdkswd/MLcode/tree/master/%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92</a><br>这个分类结果相当不错，从上图可以看出，只分错了几个点而已。但是，尽管例子简单切数据集很小，但是这个方法却需要大量的计算(300次乘法)。因此将对改算法稍作改进，从而减少计算量，使其可以应用于大数据集上。</p>
<h2 id="逻辑回归算法的改进"><a href="#逻辑回归算法的改进" class="headerlink" title="逻辑回归算法的改进"></a>逻辑回归算法的改进</h2><p>梯度上升算法在每次更新回归系数(最优参数)时，都需要遍历整个数据集。<br>假设，我们使用的数据集一共有100个样本。那么，dataMatrix就是一个100<strong>3的矩阵。每次计算h的时候，都要计算dataMatrix</strong>weights这个矩阵乘法运算，要进行100<strong>3次乘法运算和100</strong>2次加法运算。同理，更新回归系数(最优参数)weights时，也需要用到整个数据集，要进行矩阵乘法运算。总而言之，该方法处理100个左右的数据集时尚可，但如果有数十亿样本和成千上万的特征，那么该方法的计算复杂度就太高了。因此，需要对算法进行改进，我们每次更新回归系数(最优参数)的时候，能不能不用所有样本呢？一次只用一个样本点去更新回归系数(最优参数)？这样就可以有效减少计算量了，这种方法就叫做随机梯度上升算法。</p>
<p>算法改进的第一点在于alpha在每次迭代都会调整，并且，虽然alpha会随着迭代次数不断减小，但永远不会减小到0，因为这里还存在一个常数项。必须这样做的原因是为了保证在多次迭代之后新数据仍然具有一定的影响。如果需要处理的问题是动态变化的，那么可以适当加大上述常数项，来确保新的值获得更大的回归系数。另一点值得注意的是，在降低alpha的函数中，alpha每次减少1/(j+i)，其中j是迭代次数，i是样本点的下标。第二个改进的地方在于更新回归系数(最优参数)时，只使用一个样本点，并且选择的样本点是随机的，每次迭代不使用已经用过的样本点。这样的方法，就有效地减少了计算量，并保证了回归效果。<br><a href="https://github.com/zdkswd/MLcode/blob/master/%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D.py">MLcode/逻辑回归随机梯度下降.py at master · zdkswd/MLcode · GitHub</a><br>当数据集较小时，我们使用梯度上升算法<br>当数据集较大时，我们使用改进的随机梯度上升算法<br>对应的，在Sklearn中，我们就可以根据数据情况选择优化算法，比如数据较小的时候，我们使用liblinear，数据较大时，我们使用sag和saga。</p>
<h2 id="使用Sklearn构建Logistic回归分类器"><a href="#使用Sklearn构建Logistic回归分类器" class="headerlink" title="使用Sklearn构建Logistic回归分类器"></a>使用Sklearn构建Logistic回归分类器</h2><p>LogisticRegression这个函数，一共有14个参数：<br><a href="https://cuijiahua.com/blog/2017/11/ml_7_logistic_2.html" target="_blank" rel="noopener">https://cuijiahua.com/blog/2017/11/ml_7_logistic_2.html</a><br>看文中的参数说明。<br>代码：<br><a href="https://github.com/zdkswd/MLcode/tree/master/scikit-learn-code/sklearn-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92">https://github.com/zdkswd/MLcode/tree/master/scikit-learn-code/sklearn-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92</a></p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>1、Logistic回归的优缺点<br>优点：<br>实现简单，易于理解和实现；计算代价不高，速度很快，存储资源低。<br>缺点：<br>容易欠拟合，分类精度可能不高。<br>2、其他<br>Logistic回归的目的是寻找一个非线性函数Sigmoid的最佳拟合参数，求解过程可以由最优化算法完成。<br>改进的一些最优化算法，比如sag。它可以在新数据到来时就完成参数更新，而不需要重新读取整个数据集来进行批量处理。<br>机器学习的一个重要问题就是如何处理缺失数据。这个问题没有标准答案，取决于实际应用中的需求。现有一些解决方案，每种方案都各有优缺点。<br>我们需要根据数据的情况，这是Sklearn的参数，以期达到更好的分类效果。</p>

          
        
      
    </div>

    

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://github.com/zdkswd/2018/11/10/决策树和随机森林/"/>

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="ZDK"/>
      <meta itemprop="description" content=""/>
      <meta itemprop="image" content="/images/avatar.gif"/>
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ZDK's blog"/>
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2018/11/10/决策树和随机森林/" class="post-title-link" itemprop="url">决策树和随机森林</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2018-11-10 20:01:47" itemprop="dateCreated datePublished" datetime="2018-11-10T20:01:47+08:00">2018-11-10</time>
            </span>
          

          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-02-22 20:35:20" itemprop="dateModified" datetime="2019-02-22T20:35:20+08:00">2019-02-22</time>
              </span>
            
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/知识总结/" itemprop="url" rel="index"><span itemprop="name">知识总结</span></a></span>

                
                
              
            </span>
          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">评论数：</span>
                <a href="/2018/11/10/决策树和随机森林/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2018/11/10/决策树和随机森林/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
            <span id="/2018/11/10/决策树和随机森林/" class="post-meta-item leancloud_visitors" data-flag-title="决策树和随机森林">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              
                <span class="post-meta-item-text">阅读次数：</span>
              
                <span class="leancloud-visitors-count"></span>
            </span>
          

          

          

          <br/>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="机器学习升级版-决策树和随机森林"><a href="#机器学习升级版-决策树和随机森林" class="headerlink" title="机器学习升级版 决策树和随机森林"></a>机器学习升级版 决策树和随机森林</h1><p><img src="/img/media/%E5%86%B3%E7%AD%96%E6%A0%91%E5%92%8C%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-06%20%E4%B8%8B%E5%8D%884.26.05.png" alt=""><br><img src="/img/media/%E5%86%B3%E7%AD%96%E6%A0%91%E5%92%8C%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-06%20%E4%B8%8B%E5%8D%884.29.38.png" alt=""><br><img src="/img/media/%E5%86%B3%E7%AD%96%E6%A0%91%E5%92%8C%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-06%20%E4%B8%8B%E5%8D%884.31.08.png" alt=""><br><img src="/img/media/%E5%86%B3%E7%AD%96%E6%A0%91%E5%92%8C%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-06%20%E4%B8%8B%E5%8D%884.32.58.png" alt=""><br><img src="/img/media/%E5%86%B3%E7%AD%96%E6%A0%91%E5%92%8C%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-06%20%E4%B8%8B%E5%8D%884.50.37.png" alt=""><br><img src="/img/media/%E5%86%B3%E7%AD%96%E6%A0%91%E5%92%8C%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-06%20%E4%B8%8B%E5%8D%884.55.26.png" alt=""><br><img src="/img/media/%E5%86%B3%E7%AD%96%E6%A0%91%E5%92%8C%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-06%20%E4%B8%8B%E5%8D%884.55.45.png" alt=""><br><img src="/img/media/%E5%86%B3%E7%AD%96%E6%A0%91%E5%92%8C%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-06%20%E4%B8%8B%E5%8D%886.21.21.png" alt=""><br><img src="/img/media/%E5%86%B3%E7%AD%96%E6%A0%91%E5%92%8C%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-06%20%E4%B8%8B%E5%8D%886.22.28.png" alt=""><br><img src="/img/media/%E5%86%B3%E7%AD%96%E6%A0%91%E5%92%8C%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-06%20%E4%B8%8B%E5%8D%886.24.26.png" alt=""><br><img src="/img/media/%E5%86%B3%E7%AD%96%E6%A0%91%E5%92%8C%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-06%20%E4%B8%8B%E5%8D%886.25.32.png" alt=""><br><img src="/img/media/%E5%86%B3%E7%AD%96%E6%A0%91%E5%92%8C%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-06%20%E4%B8%8B%E5%8D%886.28.16.png" alt=""><br><img src="/img/media/%E5%86%B3%E7%AD%96%E6%A0%91%E5%92%8C%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-06%20%E4%B8%8B%E5%8D%886.31.32.png" alt=""><br><img src="/img/media/%E5%86%B3%E7%AD%96%E6%A0%91%E5%92%8C%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-06%20%E4%B8%8B%E5%8D%886.32.52.png" alt=""><br><img src="/img/media/%E5%86%B3%E7%AD%96%E6%A0%91%E5%92%8C%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-06%20%E4%B8%8B%E5%8D%886.44.46.png" alt=""><br><img src="/img/media/%E5%86%B3%E7%AD%96%E6%A0%91%E5%92%8C%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-06%20%E4%B8%8B%E5%8D%886.50.31.png" alt=""><br>基于样本和特征的双重随机性。<br><img src="/img/media/%E5%86%B3%E7%AD%96%E6%A0%91%E5%92%8C%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-06%20%E4%B8%8B%E5%8D%886.59.37.png" alt=""><br>之所以有那么多种的处理办法，是因为没有某一种是十分有效的。</p>
<p>CART<br>classification and regression tree</p>
<h1 id="统计学习方法-决策树"><a href="#统计学习方法-决策树" class="headerlink" title="统计学习方法 决策树"></a>统计学习方法 决策树</h1><p>决策树( decision tree)是一种基本的分类与回归方法。这里主要是用于分类的决策树.决策树模型呈树形结构,在分类问题中,表示基于特征对实例进行分类的过程。它可以认为是 if-then规则的集合,也可以认为是定义在特征空间与类空间上的条件概率分布。</p>
<p>其主要优点是模型具有可读性,分类速度快。学习时,利用训练数据根据损失函数最小化的原则建立决策树模型。预测时,对新的数据,利用决策树模型进行分类。</p>
<p>决策树学习通常包括3个步骤:特征选择、决策树的生成和决策树的修剪。</p>
<h2 id="决策树模型与学习"><a href="#决策树模型与学习" class="headerlink" title="决策树模型与学习"></a>决策树模型与学习</h2><h3 id="决策树模型"><a href="#决策树模型" class="headerlink" title="决策树模型"></a>决策树模型</h3><p>分类决策树模型是一种描述对实例进行分类的树形结构。决策树由结点(node)和有向边( directed edge)组成。<br>结点有两种类型:内部结点( internal node)和叶结点( leaf node)。内部结点表示一个特征或属性,叶结点表示一个类。</p>
<p>用决策树分类,从根结点开始,对实例的某一特征进行测试,根据测试结果,将实例分配到其子结点;这时,每一个子结点对应着该特征的一个取值.如此递归地对实例进行测试并分配,直至达到叶结点.最后将实例分到叶结点的类中。<br><img src="/img/media/%E5%86%B3%E7%AD%96%E6%A0%91%E5%92%8C%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-02%20%E4%B8%8B%E5%8D%884.41.54.png" alt=""><br>圆代表内部节点，方框代表叶节点。</p>
<h3 id="决策树与if-then规则"><a href="#决策树与if-then规则" class="headerlink" title="决策树与if-then规则"></a>决策树与if-then规则</h3><p>可以将决策树看成一个 if-then规则的集合.将决策树转换成 if-then规则的过程是这样的:由决策树的根结点到叶结点的每一条路径构建一条规则;路径上内部结点的特征对应着规则的条件,而叶结点的类对应着规则的结论.决策树的路径或其对应的 if-then规则集合具有一个重要的性质:互斥并且完备.这就是说,每一个实例都被一条路径或一条规则所覆盖,而且只被一条路径或一条规则所覆盖.这里所谓覆盖是指实例的特征与路径上的特征一致或实例满足规则的条件。</p>
<h3 id="决策树与条件概率分布"><a href="#决策树与条件概率分布" class="headerlink" title="决策树与条件概率分布"></a>决策树与条件概率分布</h3><p>决策树还表示给定特征条件下类的条件概率分布。假设X为表示特征的随机变量,Y为表示类的随机变量,那么这个条件概率分布可以表示为：<br><img src="/img/media/%E5%86%B3%E7%AD%96%E6%A0%91%E5%92%8C%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-02%20%E4%B8%8B%E5%8D%884.49.50.png" alt=""><br>X取值与给定划分下单元的集合，Y取值于类的集合。各叶节点上的条件概率往往偏向某一类，即属于某一类的概率较大。决策树分类时将该节点的实例强行分到条件概率大的那一类去。</p>
<h3 id="决策树学习"><a href="#决策树学习" class="headerlink" title="决策树学习"></a>决策树学习</h3><p>决策树学习本质上是从训练数据集中归纳出一组分类规则。与训练数据集不<br>相矛盾的决策树(即能对训练数据进行正确分类的决策树)可能有多个,也可能<br>一个也没有。我们需要的是一个与训练数据矛盾较小的决策树,同时具有很好的<br>泛化能力。</p>
<p>决策树学习用损失函数表示这一目标,如下所述,决策树学习的损失函数通常是正则化的极大似然函数.决策树学习的策略是以损失函数为目标函数的最小化。</p>
<p>当损失函数确定以后,学习问题就变为在损失函数意义下选择最优决策树的问题,因为从所有可能的决策树中选取最优决策树是NP完全问题,所以现实中决策树学习算法通常采用<strong>启发式方法</strong>,近似求解这一最优化问题.这样得到的决策树是次最优(sub- optimal)的。</p>
<p>决策树学习的算法通常是一个递归地选择最优特征，并根据该特征对训练数据进行分割，使得对各个子数据集有一个最好的分类的过程。</p>
<p>生成的决策树可能对训练数据有很好的分类能力,但对未知的测试数据却未必有很好的分类能力,即可能发生过拟合现象.我们需要对已生成的树自下而上进行剪枝,将树变得更简单,从而使它具有更好的泛化能力。</p>
<p>如果特征数量很多,也可以在决策树学习开始的时候,对特征进行选择,只留下对训练数据有足够分类能力的特征。</p>
<p>决策树学习算法包含特征选择、决策树的生成与决策树的剪枝过程.由于决策树表示一个条件概率分布,所以深浅不同的决策树对应着不同复杂度的概率模型.决策树的生成对应于模型的局部选择,决策树的剪枝对应于模型的全局选择.决策树的生成只考虑局部最优,相对地,决策树的剪枝则考虑全局最优。</p>
<h2 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h2><h3 id="特征选择问题"><a href="#特征选择问题" class="headerlink" title="特征选择问题"></a>特征选择问题</h3><p>特征选择在于选取对训练数据具有分类能力的特征.这样可以提高决策树学习的效率.如果利用一个特征进行分类的结果与随机分类的结果没有很大差别,则称这个特征是没有分类能力的,经验上扔掉这样的特征对决策树学习的精度影响不大,通常特征选择的准则是<strong>信息增益</strong>或<strong>信息增益比</strong>。</p>
<h3 id="信息增益"><a href="#信息增益" class="headerlink" title="信息增益"></a>信息增益</h3><h4 id="信息增益的算法"><a href="#信息增益的算法" class="headerlink" title="信息增益的算法"></a>信息增益的算法</h4><p>输入训练集D和特征A：<br>输出：特征A对训练数据集D的信息增益g(D,A)。<br><img src="/img/media/%E5%86%B3%E7%AD%96%E6%A0%91%E5%92%8C%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-06%20%E4%B8%8B%E5%8D%888.38.31.png" alt=""><br><img src="/img/media/%E5%86%B3%E7%AD%96%E6%A0%91%E5%92%8C%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-06%20%E4%B8%8B%E5%8D%888.39.58.png" alt=""><br><img src="/img/media/%E5%86%B3%E7%AD%96%E6%A0%91%E5%92%8C%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-06%20%E4%B8%8B%E5%8D%888.44.53.png" alt=""><br>A3，A4类似，最后比较各特征的信息增益值。由于特征A3的信息增益值最大，所以选择特征A3作为最优特征。</p>
<h3 id="信息增益比"><a href="#信息增益比" class="headerlink" title="信息增益比"></a>信息增益比</h3><h4 id="信息增益比定义"><a href="#信息增益比定义" class="headerlink" title="信息增益比定义"></a>信息增益比定义</h4><p><img src="/img/media/%E5%86%B3%E7%AD%96%E6%A0%91%E5%92%8C%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-06%20%E4%B8%8B%E5%8D%888.47.21.png" alt=""></p>
<h2 id="决策树的生成"><a href="#决策树的生成" class="headerlink" title="决策树的生成"></a>决策树的生成</h2><h3 id="ID3算法"><a href="#ID3算法" class="headerlink" title="ID3算法"></a>ID3算法</h3><p>ID3算法的<strong>核心</strong>是在决策树各个结点上应用<strong>信息增益</strong>准则选择特征,递归地构建决策树.具体方法是:从根结点( root node)开始,对结点计算所有可能的特征的信息增益,选择信息增益最大的特征作为结点的特征,由该特征的不同取值建立子结点;再对子结点递归地调用以上方法,构建决策树;直到所有特征的信息增益均很小或没有特征可以选择为止,最后得到一个决策树.ID3相当于用极大似然法进行概率模型的选择。</p>
<h4 id="算法（ID3算法）"><a href="#算法（ID3算法）" class="headerlink" title="算法（ID3算法）"></a>算法（ID3算法）</h4><p><img src="/img/media/%E5%86%B3%E7%AD%96%E6%A0%91%E5%92%8C%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-06%20%E4%B8%8B%E5%8D%888.51.51.png" alt=""><br><img src="/img/media/%E5%86%B3%E7%AD%96%E6%A0%91%E5%92%8C%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-06%20%E4%B8%8B%E5%8D%888.52.00.png" alt=""><br><img src="/img/media/%E5%86%B3%E7%AD%96%E6%A0%91%E5%92%8C%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-06%20%E4%B8%8B%E5%8D%888.54.11.png" alt=""><br>ID3算法只有树的生成，所以该算法生成的树容易产生过拟合。而且ID3还存在着不能直接处理连续型特征的问题。只有事先将连续型特征离散化，才能在ID3算法中使用，但这种转换过程会破坏连续型变量的内在特性。</p>
<h3 id="C4-5的生成算法"><a href="#C4-5的生成算法" class="headerlink" title="C4.5的生成算法"></a>C4.5的生成算法</h3><p>C4.5算法与ID3算法相似，C4.5算法对ID3算法进行了改进。C4.5在生成的过程中，用<strong>信息增益比</strong>来选择特征。</p>
<h4 id="算法（C4-5的生成算法）"><a href="#算法（C4-5的生成算法）" class="headerlink" title="算法（C4.5的生成算法）"></a>算法（C4.5的生成算法）</h4><p><img src="/img/media/%E5%86%B3%E7%AD%96%E6%A0%91%E5%92%8C%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-06%20%E4%B8%8B%E5%8D%888.57.13.png" alt=""></p>
<h2 id="决策树的剪枝"><a href="#决策树的剪枝" class="headerlink" title="决策树的剪枝"></a>决策树的剪枝</h2><p>决策树生成算法递归地产生决策树,直到不能继续下去为止,这样产生的树往往对训练数据的分类很准确,但对未知的测试数据的分类却没有那么准确,即出现过拟合现象.过拟合的原因在于学习时过多地考虑如何提高对训练数据的正确分类,从而构建出过于复杂的决策树,解决这个问题的办法是考虑决策树的复杂度,对已生成的决策树进行简化。</p>
<p>在决策树学习中将已生成的树进行简化的过程称为剪枝( pruning).具体地,剪枝从已生成的树上裁掉一些子树或叶结点,并将其根结点或父结点作为新的叶结点,从而简化分类树模型。</p>
<p>决策树的剪枝往往通过极小化决策树整体的损失函数或代价函数来实现。设树T的叶节点个数为|T|，t是树T的叶节点，该<br><img src="/img/media/%E5%86%B3%E7%AD%96%E6%A0%91%E5%92%8C%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-06%20%E4%B8%8B%E5%8D%889.18.35.png" alt=""><br>C（T）表示模型对训练数据的预测误差，即模型与训练数据的拟合程度，|T|表示模型复杂度，参数α≥0控制两者之间的影响。较大的α促使选择较简单的模型（树），较小的α促使选择较复杂的模型（树）。α=0意味着只考虑模型与训练数据的拟合程度，不考虑模型的复杂度。</p>
<p>剪枝,就是当α确定时,选择损失函数最小的模型,即损失函数最小的子树.当α值确定时,子树越大,往往与训练数据的拟合越好,但是模型的复杂度就越髙;相反,子树越小,模型的复杂度就越低,但是往往与训练数据的拟合不好,损失函数正好表示了对两者的平衡。</p>
<h3 id="算法（树的剪枝算法）"><a href="#算法（树的剪枝算法）" class="headerlink" title="算法（树的剪枝算法）"></a>算法（树的剪枝算法）</h3><p>输入：生成算法产生的整个树T，参数α；<br>输出：修剪后的子树Tα。</p>
<ol>
<li>计算每个结点的经验熵。</li>
<li>递归地从树的叶节点向上回缩。</li>
</ol>
<p><img src="/img/media/%E5%86%B3%E7%AD%96%E6%A0%91%E5%92%8C%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-06%20%E4%B8%8B%E5%8D%889.30.27.png" alt=""></p>
<ol start="3">
<li>返回2，直至不能继续为止，得到损失函数最小的子树Tα。</li>
</ol>
<p>式(515)只需考虑两个树的损失函数的差,其计算可以在局部进行,所以,决策树的剪枝算法可以由一种动态规划的算法实现。</p>
<h2 id="CART算法"><a href="#CART算法" class="headerlink" title="CART算法"></a>CART算法</h2><p>CART：classification and regression tree<br>CART算法有两步：</p>
<ol>
<li>决策树生成，基于<strong>训练数据集</strong>生成决策树，生成的决策树要<strong>尽量大</strong></li>
<li>决策树剪枝，用<strong>验证数据集</strong>对已生成的树进行剪枝并选择最优子树，这时用<strong>损失函数最小</strong>作为剪枝的标准。</li>
</ol>
<h3 id="CART生成"><a href="#CART生成" class="headerlink" title="CART生成"></a>CART生成</h3><p>决策树的生成就是递归地构建二叉决策树的过程。对<strong>回归</strong>树用<strong>平方误差</strong>最小化准则，对<strong>分类</strong>树用<strong>基尼指数</strong>最小化准则，进行特征选择，生成二叉树。</p>
<h4 id="回归树的生成"><a href="#回归树的生成" class="headerlink" title="回归树的生成"></a>回归树的生成</h4><p><img src="/img/media/%E5%86%B3%E7%AD%96%E6%A0%91%E5%92%8C%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-09%20%E4%B8%8B%E5%8D%889.10.36.png" alt=""><br>用人话解释：回归树的原理及Python实现 - 李小文的文章 - 知乎<br><a href="https://zhuanlan.zhihu.com/p/43939904" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/43939904</a><br>就是把连续的数据分为几个区间，问题的关键就是分割点的选取。</p>
<h4 id="分类树的生成"><a href="#分类树的生成" class="headerlink" title="分类树的生成"></a>分类树的生成</h4><p>分类树用基尼指数选择最优特征，同时决定该特征的最优二值切分点。<br><img src="/img/media/%E5%86%B3%E7%AD%96%E6%A0%91%E5%92%8C%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-09%20%E4%B8%8B%E5%8D%889.16.44.png" alt=""><br>生成方法与ID3决策树类似。</p>
<h3 id="CART剪枝"><a href="#CART剪枝" class="headerlink" title="CART剪枝"></a>CART剪枝</h3><p>CART剪枝算法从“完全生长”的决策树的底端剪去一些子树，使决策树变小(模型变简单)，从而能够对未知数据有更准确的预测。</p>
<p> CART剪枝算法由两步组成:首先从生成算法产生的决策树To底端开始不断剪枝,直到To的根结点，形成一个子树序列{T1,…,Tn};然后通过交叉验证法在独立的验证数据集上对子树序列进行测试，从中选择最优子树。</p>
<h4 id="剪枝，形成一个子树序列"><a href="#剪枝，形成一个子树序列" class="headerlink" title="剪枝，形成一个子树序列"></a>剪枝，形成一个子树序列</h4><p>在剪枝过程中，计算子树的损失函数:<br><img src="/img/media/%E5%86%B3%E7%AD%96%E6%A0%91%E5%92%8C%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-07%20%E4%B8%8B%E5%8D%885.24.46.png" alt=""><br>T为任意子树，C（T）为训练数据的预测误差（如基尼指数），|T|为子树的叶节点个数，α≥0为参数，Cα（T）为参数是α时的子树T的整体损失。参数α权衡训练数据的拟合程度与模型的复杂度。</p>
<p>对固定的α,一定存在使损失函数C(T)最小的子树，将其表示为Tα. Tα在损失函数Cα(T)最小的意义下是最优的.容易验证这样的最优子树是唯一的.当α大的时候，最优子树Tα偏小;当α小的时候，最优子树Tα偏大. 极端情况，当α=0时，整体树是最优的，当α趋近正无穷时，根结点组成的单结点树是最优的.</p>
<p>从整体树T0开始剪枝，对T0的任意内部节点t，以t为单结点树的损失函数是<br><img src="/img/media/%E5%86%B3%E7%AD%96%E6%A0%91%E5%92%8C%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-07%20%E4%B8%8B%E5%8D%885.55.53.png" alt=""><br>以t为根结点的子树Tt的损失函数是<br><img src="/img/media/%E5%86%B3%E7%AD%96%E6%A0%91%E5%92%8C%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-07%20%E4%B8%8B%E5%8D%885.56.28.png" alt=""><br>当α=0及α充分小时，有不等式<br><img src="/img/media/%E5%86%B3%E7%AD%96%E6%A0%91%E5%92%8C%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-07%20%E4%B8%8B%E5%8D%886.02.05.png" alt=""><br>当α增大时，在某一α有<br><img src="/img/media/%E5%86%B3%E7%AD%96%E6%A0%91%E5%92%8C%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-07%20%E4%B8%8B%E5%8D%886.02.38.png" alt=""><br>当α再增大时，不等式反向。只要<br><img src="/img/media/%E5%86%B3%E7%AD%96%E6%A0%91%E5%92%8C%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-07%20%E4%B8%8B%E5%8D%886.03.34.png" alt=""><br>Tt与t有相同的损失函数值，而t的节点少，因此t比Tt更可取，对Tt进行剪枝。</p>
<p>为此，对T0中每一内部结点t，计算<br><img src="/img/media/%E5%86%B3%E7%AD%96%E6%A0%91%E5%92%8C%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-07%20%E4%B8%8B%E5%8D%886.05.18.png" alt=""><br>它表示剪枝后整体损失函数减少的程度.在T0中剪去g(t)最小的Tt,将得到的子<br>树作为T1，同时将最小的g(t)设为α1. T1为区间[α1,α2)的最优子树.</p>
<p>如此剪枝下去，直到得到根节点。在这一过程中，不断增加α的值，产生新的区间。</p>
<h4 id="在剪枝得到的子树序列T0-T1-…-Tn中通过交叉验证选取最优子树Tα"><a href="#在剪枝得到的子树序列T0-T1-…-Tn中通过交叉验证选取最优子树Tα" class="headerlink" title="在剪枝得到的子树序列T0,T1,…,Tn中通过交叉验证选取最优子树Tα"></a>在剪枝得到的子树序列T0,T1,…,Tn中通过交叉验证选取最优子树Tα</h4><p>具体地，利用独立的验证数据集，测试子树序列T0,T1,…,Tn中各棵子树的平方误差或基尼指数.平方误差或基尼指数最小的决策树被认为是最优的决策树.在子树序列中，每棵子树T1,T2,.,Tn都对应于一个参数α1,α2,…,αn,.所以，当最优子树Tk确定时，对应的ak也确定了，即得到最优决策树Tα。</p>
<h3 id="CART剪枝算法"><a href="#CART剪枝算法" class="headerlink" title="CART剪枝算法"></a>CART剪枝算法</h3><p><img src="/img/media/%E5%86%B3%E7%AD%96%E6%A0%91%E5%92%8C%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-07%20%E4%B8%8B%E5%8D%886.11.57.png" alt=""></p>
<h1 id="西瓜书-树剪枝"><a href="#西瓜书-树剪枝" class="headerlink" title="西瓜书 树剪枝"></a>西瓜书 树剪枝</h1><p>可以采用留出法，即预留一部分数据用作“验证集”以进行性能评估。</p>
<h2 id="预剪枝"><a href="#预剪枝" class="headerlink" title="预剪枝"></a>预剪枝</h2><p><img src="/img/media/%E5%86%B3%E7%AD%96%E6%A0%91%E5%92%8C%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-09%20%E4%B8%8B%E5%8D%889.49.18.png" alt=""><br>预剪枝使得决策树的很多分支都没有“展开”，这不仅降低了过拟合的风险，还显著减少了决策树的训练时间开销和测试时间开销。但在另一方面，有些分支的当前划分虽不能提高泛化性能、甚至可能导致泛化性能暂时下降，但在其基础上进行的后续划分却有可能导致性能显著提高；预剪枝基于“贪心”本质禁止这些分支展开，给预剪枝决策树带来了欠拟合的风险。</p>
<h2 id="后剪枝"><a href="#后剪枝" class="headerlink" title="后剪枝"></a>后剪枝</h2><p>后剪枝先从训练集生成一颗完整决策树。<br><img src="/img/media/%E5%86%B3%E7%AD%96%E6%A0%91%E5%92%8C%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-10%20%E4%B8%8A%E5%8D%8810.03.13.png" alt=""><br><img src="/img/media/%E5%86%B3%E7%AD%96%E6%A0%91%E5%92%8C%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-10%20%E4%B8%8A%E5%8D%8810.03.36.png" alt=""><br>对比可得，后剪枝决策树通常比预剪枝决策树保留了更多的分支。一般来说，后剪枝决策树的欠拟合风险很小，泛化性能往往优于预剪枝决策树。但后剪枝过程是在生成完全决策树之后进行的，并且要自底向上地对树中的所有非叶节点进行逐一考察，因此其训练时间开销比未剪枝决策树和预剪枝决策树都要大。</p>
<h1 id="博客"><a href="#博客" class="headerlink" title="博客"></a>博客</h1><p>回归树：<br><img src="/img/media/%E5%86%B3%E7%AD%96%E6%A0%91%E5%92%8C%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/1%20dt%20divide.png" alt=""></p>
<p>回归树与线性回归的对比：<br><img src="/img/media/%E5%86%B3%E7%AD%96%E6%A0%91%E5%92%8C%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/1%20dt%20sklearn.png" alt=""></p>
<h1 id="西瓜书-Bagging与随机森林"><a href="#西瓜书-Bagging与随机森林" class="headerlink" title="西瓜书 Bagging与随机森林"></a>西瓜书 Bagging与随机森林</h1><p>欲得到泛化性能强的集成,集成中的个体学习器应<strong>尽可能相互独立</strong>;虽然“独立”在现实任务中无法做到，但可以设法使基学习器尽可能具有较大的差异.给定一个训练数据集,一种可能的做法是对训练样本进行采样，产生出若干个不同的子集，再从每个数据子集中训练出一个基学习器.这样，由于训练数据不同,我们获得的基学习器可望具有比较大的差异.然而，为获得好的集成，我们同时还希望个体学习器不能太差.如果采样出的每个子集都<strong>完全不同</strong>，则每个基学习器只用到了<strong>一小部分训练数据</strong>，甚至不足以进行有效学习,这显然无法确保产生出比较好的基学习器.为解决这个问题，我们可考虑使用相互有交叠的采样子集.</p>
<h2 id="Bagging"><a href="#Bagging" class="headerlink" title="Bagging"></a>Bagging</h2><p>Bagging是并行式集成学习最著名的代表，直接基于自助采样法（bootstrap sampling）。给定包含m个样本的数据集,我们先随机取出一个样本放入采样集中，再把该样本放回初始数据集，使得下次采样时该样本仍有可能被选中,这样,经过m次随机采样操作,我们得到含m个样本的采样集，初始训练集中有的样本在采样集里多次出现，有的则从未出现，由式(2.1)可知, 初始训练集中约有63.2%的样本出现在采样集中.</p>
<p>我们可采样出T个含m个训练样本的采样集，然后基于每个采样集训练出一个基学习器，再将这些基学习器进行结合.这就是Bagging的基本流程.在对预测输出进行结合时，Bagging通常对分类任务使用简单投票法，对回归任务使用简单平均法.若分类预测时出现两个类收到同样票数的情形,则最简单的做法是随机选择一个,也可进一步考察学习器投票的置信度来确定最终胜者.<br><img src="/img/media/%E5%86%B3%E7%AD%96%E6%A0%91%E5%92%8C%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-10%20%E4%B8%8B%E5%8D%883.16.19.png" alt=""><br>自助采样过程还给Bagging带来了另一个优点:由于每个基学习器只使用了初始训练集中约63.2%的样本,剩下约36.8%的样本可用作验证集来对泛化性能进行“包外估计”。</p>
<p>包外样本还有许多其他用途.例如当基学习器是决策树时，可使用包外样本来辅助剪枝,或用于估计决策树中各结点的后验概率以辅助对零训练样本结点的处理;当基学习器是神经网络时，可使用包外样本来辅助早期停止以减小过拟合风险.</p>
<p>从偏差方差分解的角度看，Bagging主要关注降低方差，因此它在不剪枝决策树、神经网络等易受样本扰动的学习器上效用更为明显。</p>
<h2 id="随机森林"><a href="#随机森林" class="headerlink" title="随机森林"></a>随机森林</h2><p>随机森林(Random Forest,简称RF) 是Bagging的一个扩展变体. RF在以决策树为基学习器构建Bagging集成的基础上,进一步在决策树的训练过程中引入了随机属性选择.具体来说，传统决策树在选择划分属性时是在当前结点的属性集合(假定有d个属性)中选择一个最优属性;而在RF中，对基决策树的每个结点，先从该结点的属性集合中随机选择一个包含k个属性的子集，然后再从这个子集中选择一个最优属性用于划分。这里的参数k控制了随机性的引入程度。若k=d则基决策树的构建与传统决策树相同，一般，推荐k=log2d。</p>
<p>随机森林简单、容易实现、计算开销小，令人惊奇的是，它在很多现实任务中展现出强大的性能,被誉为“代表集成学习技术水平的方法”.可以看出，随机森林对Bagging只做了小改动,但是与Bagging中基学习器的“多样性”仅通过样本扰动(通过对初始训练集采样)而来不同，随机森林中基学习器的多样性<strong>不仅来自样本扰动</strong>,还来自<strong>属性扰动</strong>，这就使得最终集成的泛化性能可通过个体学习器之间差异度的增加而进一步提升.</p>
<p>随机森林的收敛性与Bagging相似，起始性能往往相对较差，特别是在集成中只包含一个基学习器时。这不难理解，因为通过引入属性扰动，随机森林中个体学习器的性能往往有所降低，但是随着个体学习器数目的增加，随机森林通常会收敛到更低的泛化误差值得一提的是,随机森林的训练效率常优于Bagging,因为在个体决策树的构建过程中, Bagging使用的是“确定型”决策树,在选择划分属性时要对结点的所有属性进行考察，而随机森林使用的“随机型”央策树则只需考察一个属性子集.<br><img src="/img/media/%E5%86%B3%E7%AD%96%E6%A0%91%E5%92%8C%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-10%20%E4%B8%8B%E5%8D%883.41.16.png" alt=""></p>
<h2 id="结合策略"><a href="#结合策略" class="headerlink" title="结合策略"></a>结合策略</h2><h3 id="学习器结合的好处"><a href="#学习器结合的好处" class="headerlink" title="学习器结合的好处"></a>学习器结合的好处</h3><p>学习器结合会带来三个方面的好处：</p>
<ol>
<li>统计方面，可能有多个假设在训练集上达到同等性能，此时若使用单学习器可能会因为误选而导致泛化性能不佳，结合多个学习器则会减小这一风险。</li>
<li>计算方面，学习算法往往会陷入局部极小,有的局部极小点所对应的泛化性能可能很糟糕，而通过多次运行之后进行结合，可降低陷入糟糕局部极小点的风险。</li>
<li>表示方面，某些学习任务的真实假设可能不在当前学习算法所考虑的假设空间中，此时若使用单学习器则肯定无效,而通过结合多个学习器，由于相应的假设空间有所扩大，有可能学得更好的近似。<h3 id="平均法"><a href="#平均法" class="headerlink" title="平均法"></a>平均法</h3>对于数值型输出，最常见的就是平均法。<h4 id="简单平均法"><a href="#简单平均法" class="headerlink" title="简单平均法"></a>简单平均法</h4><img src="/img/media/%E5%86%B3%E7%AD%96%E6%A0%91%E5%92%8C%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-10%20%E4%B8%8B%E5%8D%884.35.03.png" alt=""><h4 id="加权平均法"><a href="#加权平均法" class="headerlink" title="加权平均法"></a>加权平均法</h4><img src="/img/media/%E5%86%B3%E7%AD%96%E6%A0%91%E5%92%8C%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-10%20%E4%B8%8B%E5%8D%884.35.13.png" alt=""><br><img src="/img/media/%E5%86%B3%E7%AD%96%E6%A0%91%E5%92%8C%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-10%20%E4%B8%8B%E5%8D%884.35.29.png" alt=""><br>加权平均法的权重一般是从训练数据中学习而得，现实任务中的训练样本通常不充分或存在噪声，这将使得学出的权重不完全可靠.尤其是对规模比较大的集成来说，要学习的权重比较多,较容易导致过拟合.因此,实验和应用均显示出，加权平均法未必一定优于简单平均法.一般而言,在个体学习器性能相差较大时宜使用加权平均法，而在个体学习器性能相近时宜使用简单平均法.<h3 id="投票法"><a href="#投票法" class="headerlink" title="投票法"></a>投票法</h3><h4 id="绝对多数投票法"><a href="#绝对多数投票法" class="headerlink" title="绝对多数投票法"></a>绝对多数投票法</h4>即若某标记得票过半数，则预测为该标记，否则拒绝预测。<h4 id="相对多数投票法"><a href="#相对多数投票法" class="headerlink" title="相对多数投票法"></a>相对多数投票法</h4>即预测为得票最多的标记，若同时又多个标记获最高票，则从中随机选取一个。<h4 id="加权投票法"><a href="#加权投票法" class="headerlink" title="加权投票法"></a>加权投票法</h4>与加权平均法类似。</li>
</ol>
<p>标准的绝对多数投票法提供了“拒绝预测”选项,这在可靠性要求较高的学习任务中是一个很好的机制.但若学习任务要求必须提供预测结果，则绝对多数投票法将退化为相对多数投票法.因此,在不允许拒绝预测的任务中,绝对多数、相对多数投票法统称为“多数投票法”。</p>
<p>硬投票：输出预测的类标记，软投票，输出一个概率估计。<br><img src="/img/media/%E5%86%B3%E7%AD%96%E6%A0%91%E5%92%8C%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-10%20%E4%B8%8B%E5%8D%884.54.09.png" alt=""><br><img src="/img/media/%E5%86%B3%E7%AD%96%E6%A0%91%E5%92%8C%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-10%20%E4%B8%8B%E5%8D%884.55.31.png" alt=""></p>
<h1 id="实践"><a href="#实践" class="headerlink" title="实践"></a>实践</h1><h2 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h2><p>算法实现。<br>背景：贷款放贷与否<br><a href="https://github.com/zdkswd/MLcode/blob/master/%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0/%E5%86%B3%E7%AD%96%E6%A0%91.py">https://github.com/zdkswd/MLcode/blob/master/%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0/%E5%86%B3%E7%AD%96%E6%A0%91.py</a></p>
<h2 id="scikit-learn决策树"><a href="#scikit-learn决策树" class="headerlink" title="scikit-learn决策树"></a>scikit-learn决策树</h2><p>使用scikit-learn实现。<br>背景：配隐形眼镜<br><a href="https://github.com/zdkswd/MLcode/tree/master/scikit-learn-code/scikit-learn%E5%86%B3%E7%AD%96%E6%A0%91">https://github.com/zdkswd/MLcode/tree/master/scikit-learn-code/scikit-learn%E5%86%B3%E7%AD%96%E6%A0%91</a></p>
<h2 id="回归树"><a href="#回归树" class="headerlink" title="回归树"></a>回归树</h2><p>关键词：<strong>CART，预剪枝，后剪枝</strong></p>
<p>回归树与分类树比较类似，不同的是分类树最后的决策的结果是离散型的值，回归树决策的结果是输出一个实数。实例中的实数输出（就是叶子节点）的是四个样本的平均值（四个样本是在进行预剪枝时设置的值）。</p>
<p>CART回归树这里使用最小总方差法选取划分特征。示例中采用的是REP（错误率降低剪枝）。还有一种方法叫做PEP（悲观剪枝）把一颗子树（具有多个叶子节点）用一个叶子节点来替代的话，比起REP剪枝法，它不需要一个单独的测试数据集。</p>
<p>本例中既有预剪枝又有后剪枝。一般来说都是结合着使用。</p>
<p>背景：连续数据的离散的点<br><a href="https://github.com/zdkswd/MLcode/tree/master/%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0/%E5%9B%9E%E5%BD%92%E6%A0%91">https://github.com/zdkswd/MLcode/tree/master/%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0/%E5%9B%9E%E5%BD%92%E6%A0%91</a></p>
<h2 id="scikit-learn随机森林"><a href="#scikit-learn随机森林" class="headerlink" title="scikit-learn随机森林"></a>scikit-learn随机森林</h2><p>待解决问题，在scikit-learn的随机森林中如何决定k值。<br><a href="https://github.com/zdkswd/MLcode/blob/master/scikit-learn-code/scikit-learn%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97.py">https://github.com/zdkswd/MLcode/blob/master/scikit-learn-code/scikit-learn%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97.py</a></p>
<h1 id="百面机器学习"><a href="#百面机器学习" class="headerlink" title="百面机器学习"></a>百面机器学习</h1><p>树形结构与销售诊断等场景下的决策过程十分相似。</p>
<h2 id="问题1-决策树有哪些常用的启发函数？"><a href="#问题1-决策树有哪些常用的启发函数？" class="headerlink" title="问题1 决策树有哪些常用的启发函数？"></a>问题1 决策树有哪些常用的启发函数？</h2><p>问：常用的决策树算法有ID3，C4.5，CART，它们构件树所使用的启发式函数各是什么，除了构建准则外，它们之间的区别与联系是什么？</p>
<p>答： ID3-最大信息增益 C4.5-最大信息增益比 CART-最大基尼指数</p>
<p>ID3采用信息增益作为评价标准，除了一些逆天特征（如与分类结果保持一致的特征）会倾向于取值较多的特征，因为信息增益反映的是给定条件以后不确定性减小的程度，特征取值越多就意味着确定性越高，也就是信息增益越大。这在实际应用中是一个缺陷。这种分类的泛化能力是非常弱的。因此C4.5实际上是对ID3进行优化，通过引入信息增益比，一定程度上对取值比较多的特征进行惩罚，避免ID3出现过拟合的特性，提升决策树的泛化能力。</p>
<p>从样本角度，ID3只能处理离散型变量，而C4.5和CART都可以处理连续型变量。C4.5处理连续型变量时，通过对数据排序后找到类别不同的分割点作为切分点，根据切分点把连续属性转换为布尔型，从而将连续型变量转换为布尔型，从而将连续型变量转换为多个取值区间的离散型变量。而对于CART，由于其构建时每次都会对特征进行二值划分，因此可以很好地适用于连续性变量。</p>
<p>从应用角度，ID3和C4.5只能用于分类任务，而CART可以用于分类也可以用于回归。（回归使用最小平方误差准则）</p>
<p>从实现细节，优化过程等角度，三种决策树有所不同。ID3对样本特征缺失值比较敏感，而C4.5和CART可以对缺失值进行不同方式的处理。ID3和C4.5可以在每个结点上产生出多叉分支，且每个特征在层级之间不会复用，而CART每个结点只会产生两个分支，因此最后形成一颗二叉树，且每个特征可以被重复使用。ID3和C4.5通过剪枝来权衡树的准确性与泛化能力，而CART直接利用全部数据发现所有可能的数结构进行对比。</p>
<h2 id="如何对决策树进行剪枝"><a href="#如何对决策树进行剪枝" class="headerlink" title="如何对决策树进行剪枝"></a>如何对决策树进行剪枝</h2><p>问：决策树的剪枝通常有两种方法，预剪枝与后剪枝，各自是如何进行的？又各有什么优缺点？</p>
<p>答：预剪枝具有思想简单，算法简单，效率高等特点，适合解决大规模问题。但如何准确地估计何时停止树的生长（例如确定深度或阈值），针对不同问题会有很大的差别，需要一定经验判断。且预剪枝存在一定局限性，有欠拟合的风险。</p>
<p>后剪枝的核心思想是让算法生成一颗完全生长的决策树，然后从最底层向上计算是否剪枝。剪枝过程将子树删除，用一个叶子结点代替，该结点的类别同样按照多数投票的原则进行判断。同样地，后剪枝也可以通过在测试集上的准确率进行判断，如果剪枝过后准确率有所提升，则进行剪枝。相比于预剪枝，后剪枝方法通常可以得到泛化能力更强的决策树，但时间开销会更大。</p>
<p>剪枝过程在决策树模型中占据着及其重要的位置。有很多研究表明，剪枝比树的生成过程更为关键。对于不同划分标准生成的过拟合决策树，在经过剪枝后都能保留最重要的属性划分，因此最终的性能差距并不大。理解剪枝方法的理论，在实际应用中根据不同的数据类型，规模，决定使用何种决策树以及对应的剪枝策略，灵活变通，找到最优选择。</p>
<p>常见的后剪枝方法包括REP，PEP，MEP，CVP，OPP等方法，这些剪枝方法各有利弊，关注不同的优化角度。</p>

          
        
      
    </div>

    

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://github.com/zdkswd/2018/11/08/scikit-learn介绍/"/>

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="ZDK"/>
      <meta itemprop="description" content=""/>
      <meta itemprop="image" content="/images/avatar.gif"/>
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ZDK's blog"/>
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2018/11/08/scikit-learn介绍/" class="post-title-link" itemprop="url">scikit-learn介绍</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2018-11-08 15:56:56" itemprop="dateCreated datePublished" datetime="2018-11-08T15:56:56+08:00">2018-11-08</time>
            </span>
          

          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2018-11-12 17:30:12" itemprop="dateModified" datetime="2018-11-12T17:30:12+08:00">2018-11-12</time>
              </span>
            
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/教程/" itemprop="url" rel="index"><span itemprop="name">教程</span></a></span>

                
                
              
            </span>
          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">评论数：</span>
                <a href="/2018/11/08/scikit-learn介绍/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2018/11/08/scikit-learn介绍/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
            <span id="/2018/11/08/scikit-learn介绍/" class="post-meta-item leancloud_visitors" data-flag-title="scikit-learn介绍">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              
                <span class="post-meta-item-text">阅读次数：</span>
              
                <span class="leancloud-visitors-count"></span>
            </span>
          

          

          

          <br/>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="机器学习：问题背景"><a href="#机器学习：问题背景" class="headerlink" title="机器学习：问题背景"></a>机器学习：问题背景</h1><p>学习问题分以下几类：</p>
<h2 id="有监督学习"><a href="#有监督学习" class="headerlink" title="有监督学习"></a>有监督学习</h2><h3 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h3><h3 id="回归"><a href="#回归" class="headerlink" title="回归"></a>回归</h3><h2 id="无监督学习"><a href="#无监督学习" class="headerlink" title="无监督学习"></a>无监督学习</h2><p>聚类，密度估计等</p>
<h1 id="加载示例数据集"><a href="#加载示例数据集" class="headerlink" title="加载示例数据集"></a>加载示例数据集</h1><p>有几个标准数据集，鸢尾花和手写字用于分类，以及波士顿房价数据集用于回归。<br>鸢尾花与手写字数据集的导入。<br><img src="/img/media/scikit-learn%E4%BB%8B%E7%BB%8D/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-08%20%E4%B8%8A%E5%8D%8810.30.43.png" alt=""><br>数据集就是一个字典对象，包含了所有的数据以及一些关于数据的元数据。数据存储在.data成员中，它是一个n个样本，n个特征的数组。在监督学习的问题中，因变量存在.target成员中。<br><img src="/img/media/scikit-learn%E4%BB%8B%E7%BB%8D/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-08%20%E4%B8%8A%E5%8D%8810.37.10.png" alt=""><br><img src="/img/media/scikit-learn%E4%BB%8B%E7%BB%8D/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-08%20%E4%B8%8A%E5%8D%8810.41.19.png" alt=""></p>
<h2 id="数据数组的形状"><a href="#数据数组的形状" class="headerlink" title="数据数组的形状"></a>数据数组的形状</h2><p>数据总是二维数组，（n_sample,n_features）的形状，即使原始数据可能有着不同的形状。在手写字识别中，每个原始例子是（8  *  8）。<br><img src="/img/media/scikit-learn%E4%BB%8B%E7%BB%8D/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-08%20%E4%B8%8A%E5%8D%8810.48.28.png" alt=""><br>但是在data中变成了（1  * 64）。</p>
<h1 id="学习以及预测"><a href="#学习以及预测" class="headerlink" title="学习以及预测"></a>学习以及预测</h1><p>在scikit-learn中，分类器是一个实现了方法fit(x,y)和predict(T)的Python对象。</p>
<p>先把分类器当做是一个黑盒。<br><img src="/img/media/scikit-learn%E4%BB%8B%E7%BB%8D/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-08%20%E4%B8%8A%E5%8D%8810.57.41.png" alt=""><br>选择模型的参数<br>在这个例子中我们是人工选择的参数，为了寻找这些参数的更好的值，我们能使用例如网格搜索和交叉验证的工具。</p>
<p>分类器实例拟合模型是通过传递训练集给fit方法。对于训练集，我们使用除了最后一张图片的所有图片，最后一张图片用来做预测。<br><img src="/img/media/scikit-learn%E4%BB%8B%E7%BB%8D/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-08%20%E4%B8%8B%E5%8D%881.48.35.png" alt=""><br>来预测：<br><img src="/img/media/scikit-learn%E4%BB%8B%E7%BB%8D/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-08%20%E4%B8%8B%E5%8D%881.49.15.png" alt=""></p>
<h1 id="模型持久化"><a href="#模型持久化" class="headerlink" title="模型持久化"></a>模型持久化</h1><p>使用pickle。<br><img src="/img/media/scikit-learn%E4%BB%8B%E7%BB%8D/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-08%20%E4%B8%8B%E5%8D%881.50.45.png" alt=""></p>
<p>使用joblib，joblib在大数据方面更加高效，但是遗憾的是它只能把数据持久化到硬盘而不是字符串（搬到字符串意味着数据在内存中）。<br><img src="/img/media/scikit-learn%E4%BB%8B%E7%BB%8D/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-08%20%E4%B8%8B%E5%8D%881.57.57.png" alt=""></p>
<p>之后可以重新加载模型（也可以在其他的Python进程中）<br><img src="/img/media/scikit-learn%E4%BB%8B%E7%BB%8D/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-08%20%E4%B8%8B%E5%8D%881.59.34.png" alt=""><br><strong>注意</strong>：joblib.dump和joblib.load也接收像文件一样的对象而不是文件名。</p>
<h1 id="惯例"><a href="#惯例" class="headerlink" title="惯例"></a>惯例</h1><h2 id="类型转换"><a href="#类型转换" class="headerlink" title="类型转换"></a>类型转换</h2><p>除非特别指明，输入将被转换为float64:<br><img src="/img/media/scikit-learn%E4%BB%8B%E7%BB%8D/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-08%20%E4%B8%8B%E5%8D%882.09.50.png" alt=""><br>x是float32型，可以通过fit_transform(x)转换为float64。</p>
<p>回归的目标类型转化为float64，分类的目标类型保留下来。</p>
<h2 id="改装和更新参数"><a href="#改装和更新参数" class="headerlink" title="改装和更新参数"></a>改装和更新参数</h2><p>超参数在通过set_params()方法创建后能够更新。调用fit()函数超过一次将会重写之前fit()所学的内容。</p>
<h2 id="多类别vs多标签拟合"><a href="#多类别vs多标签拟合" class="headerlink" title="多类别vs多标签拟合"></a>多类别vs多标签拟合</h2><p>当使用多类别分类器，所执行的学习和预测任务取决于适合于目标数据的格式。</p>

          
        
      
    </div>

    

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/14/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/14/">14</a><span class="page-number current">15</span><a class="page-number" href="/page/16/">16</a><span class="space">&hellip;</span><a class="page-number" href="/page/28/">28</a><a class="extend next" rel="next" href="/page/16/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <div class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/avatar.gif"
                alt="ZDK"/>
            
              <p class="site-author-name" itemprop="name">ZDK</p>
              <div class="site-description motion-element" itemprop="description"></div>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">191</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">9</span>
                    <span class="site-state-item-name">分类</span>
                  
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">48</span>
                    <span class="site-state-item-name">标签</span>
                  
                </div>
              
            </nav>
          

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="https://github.com/zdkswd" title="GitHub &rarr; https://github.com/zdkswd"><i class="fa fa-fw fa-github"></i>GitHub</a>
                </span>
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="mailto:2822464407@qq.com" title="E-Mail &rarr; mailto:2822464407@qq.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                </span>
              
            </div>
          

          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-block">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                Links
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="https://github.com/zdkswd" title="https://github.com/zdkswd">Title</a>
                  </li>
                
              </ul>
            </div>
          

          
        </div>
      </div>

      

      
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          
        </div>
      

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">ZDK</span>

  

  
</div>


  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v3.7.1</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v7.2.0</div>




        








        
      </div>
    </footer>

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>










  
  













  
  <script src="/lib/jquery/index.js?v=3.4.1"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>




  <script src="/js/utils.js?v=7.2.0"></script>

  <script src="/js/motion.js?v=7.2.0"></script>



  
  


  <script src="/js/affix.js?v=7.2.0"></script>

  <script src="/js/schemes/pisces.js?v=7.2.0"></script>




  

  <script src="/js/next-boot.js?v=7.2.0"></script>

  

  

  

  

  
  

<script src="//unpkg.com/valine/dist/Valine.min.js"></script>

<script>
  var GUEST = ['nick', 'mail', 'link'];
  var guest = 'nick,mail,link';
  guest = guest.split(',').filter(function(item) {
    return GUEST.indexOf(item) > -1;
  });
  new Valine({
    el: '#comments',
    verify: false,
    notify: true,
    appId: 'QiU7UFIdgTTauFTk89N47mQS-gzGzoHsz',
    appKey: 'gkBx5soQkBREmER84PWbNJeM',
    placeholder: 'have fun',
    avatar: 'mm',
    meta: guest,
    pageSize: '10' || 10,
    visitor: true,
    lang: '' || 'zh-cn'
  });
</script>





  
  <script>
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url).replace(/\/{2,}/g, '/');
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('5');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x"></i></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x"></i></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  

  

  

  

  

  

  

  

  

  


  

</body>
</html>
