<!DOCTYPE html>












  


<html class="theme-next gemini use-motion" lang="zh-CN">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2"/>
<meta name="theme-color" content="#222"/>






















<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0"/>

<link rel="stylesheet" href="/css/main.css?v=7.2.0"/>


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.2.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.2.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.2.0">


  <link rel="mask-icon" href="/images/logo.svg?v=7.2.0" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '7.2.0',
    sidebar: {"position":"right","display":"post","offset":12,"onmobile":false},
    back2top: {"enable":true,"sidebar":true,"scrollpercent":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    translation: {
      copy_button: '复制',
      copy_success: '复制成功',
      copy_failure: '复制失败'
    }
  };
</script>



  <meta property="og:type" content="website">
<meta property="og:title" content="ZDK&#39;s blog">
<meta property="og:url" content="https://github.com/zdkswd/page/9/index.html">
<meta property="og:site_name" content="ZDK&#39;s blog">
<meta property="og:locale" content="zh-CN">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="ZDK&#39;s blog">



  <link rel="alternate" href="/atom.xml" title="ZDK's blog" type="application/atom+xml"/>



  
  
  <link rel="canonical" href="https://github.com/zdkswd/page/9/"/>



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>ZDK's blog</title>
  












  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <div class="container sidebar-position-right 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">ZDK's blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br/>首页</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
      
    

    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br/>标签</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">

    
    
      
    

    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br/>分类</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br/>归档</a>

  </li>

      
      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br/>搜索</a>
        </li>
      
    </ul>
  

  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
            

          
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://github.com/zdkswd/2019/03/04/XGBoost 个人总结/"/>

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="ZDK"/>
      <meta itemprop="description" content=""/>
      <meta itemprop="image" content="/images/avatar.gif"/>
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ZDK's blog"/>
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/03/04/XGBoost 个人总结/" class="post-title-link" itemprop="url">XGBoost 陈天奇论文 ppt 个人总结</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-03-04 16:49:32" itemprop="dateCreated datePublished" datetime="2019-03-04T16:49:32+08:00">2019-03-04</time>
            </span>
          

          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-04-29 12:53:06" itemprop="dateModified" datetime="2019-04-29T12:53:06+08:00">2019-04-29</time>
              </span>
            
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/论文/" itemprop="url" rel="index"><span itemprop="name">论文</span></a></span>

                
                
              
            </span>
          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">评论数：</span>
                <a href="/2019/03/04/XGBoost 个人总结/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2019/03/04/XGBoost 个人总结/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
            <span id="/2019/03/04/XGBoost 个人总结/" class="post-meta-item leancloud_visitors" data-flag-title="XGBoost 陈天奇论文 ppt 个人总结">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              
                <span class="post-meta-item-text">阅读次数：</span>
              
                <span class="leancloud-visitors-count"></span>
            </span>
          

          

          

          <br/>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><a href="https://arxiv.org/pdf/1603.02754.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1603.02754.pdf</a></p>
<h1 id="阅读目标"><a href="#阅读目标" class="headerlink" title="阅读目标"></a>阅读目标</h1><p>了解作者群<br>1 论文解决了什么问题<br>2 论文核心贡献是什么<br>3 详细研究论文的具体方法</p>
<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>文中提到，XGBoost，这个系统的效果已经被大量的机器学习和数据挖掘竞赛所验证。以Kaggle为例，2015年29组优胜方案中17组使用了XGBoost。这其中，8组只是使用了XGBoost去训练模型，其他大部分都是将XGBoost和神经网络做集成。作为对比，第二受欢迎的工具，深度神经网络，有11组采用。在KDDCup 2015中，top-10 每一组都使用了XGBoost。</p>
<p>在以下问题上提出了优胜的解决方案，包括：商店销售预测，高能物理事件分类，网站文本分类，顾客行为预测，动作检测，广告点击通过率预测，风险预测，大规模的在线课程辍学率预测。</p>
<p>XGBoost在所有场景中的可扩展性也十分的重要，XGBoost的可扩展性归功于一些重要的系统和算法优化。这些创新包括：一种处理稀疏数据的新奇的树的学习算法；并行和分布式计算让学习进行的更快能够进行更快的模型探索。提出了具有合理理论支撑的分布分位调整框架。</p>
<p>论文的主要贡献有：</p>
<ol>
<li>构建了高可扩展的端到端的boosting系统。</li>
<li>提出了具有合理理论支撑的分布分位调整框架。</li>
<li>介绍了一个新奇的并行适应稀疏处理树学习算法。</li>
<li>提出了基于缓存快的结构便于外存树的学习。</li>
</ol>
<p>已经有人做了并行树、基于外存的计算、缓存的计算、稀疏特征的学习等一些列工作，这篇文章最重要的是能够把很多特征结合到一个系统中。</p>
<p>文章的组织结构：boosting树的正则化（防止过拟合）、树的split方法（decision Tree 使用Gini划分）、系统设计、实验。</p>
<p>除了正则项外，文中还提到了两种防止过拟合的技术，Shrinkage 和 Subsampling<br>Subsampling 就按照随机森林去理解。<br>Shrinkage仍然以残差作为学习目标，但对于残差学习出来的结果，只累加一小部分（step*残差）逐步逼近目标，step一般都比较小，如0.01~0.001（注意该step非gradient的step），导致各个树的残差是渐变的而不是陡变的。直觉上这也很好理解，不像直接用残差一步修复误差，而是只修复一点点，其实就是把大步切成了很多小步。本质上，Shrinkage为每棵树设置了一个weight，累加时要乘以这个weight。Shrinkage能减少过拟合发生也是经验证明的，目前还没有看到从理论的证明。</p>
<p>陈天奇XGBoost ppt（内容非常详细）链接:<a href="https://pan.baidu.com/s/10NWfRM9qimswGxPsF9VlDw" target="_blank" rel="noopener">https://pan.baidu.com/s/10NWfRM9qimswGxPsF9VlDw</a>  密码:v3y6</p>
<p><img src="/img/add/24.png" alt=""></p>
<p><img src="/img/add/25.png" alt=""></p>
<p><img src="/img/add/26.png" alt=""></p>

          
        
      
    </div>

    

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://github.com/zdkswd/2019/03/01/卷积神经网络/"/>

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="ZDK"/>
      <meta itemprop="description" content=""/>
      <meta itemprop="image" content="/images/avatar.gif"/>
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ZDK's blog"/>
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/03/01/卷积神经网络/" class="post-title-link" itemprop="url">卷积神经网络</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-03-01 15:01:47" itemprop="dateCreated datePublished" datetime="2019-03-01T15:01:47+08:00">2019-03-01</time>
            </span>
          

          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-03-03 20:44:18" itemprop="dateModified" datetime="2019-03-03T20:44:18+08:00">2019-03-03</time>
              </span>
            
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/知识总结/" itemprop="url" rel="index"><span itemprop="name">知识总结</span></a></span>

                
                
              
            </span>
          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">评论数：</span>
                <a href="/2019/03/01/卷积神经网络/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2019/03/01/卷积神经网络/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
            <span id="/2019/03/01/卷积神经网络/" class="post-meta-item leancloud_visitors" data-flag-title="卷积神经网络">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              
                <span class="post-meta-item-text">阅读次数：</span>
              
                <span class="leancloud-visitors-count"></span>
            </span>
          

          

          

          <br/>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="博客"><a href="#博客" class="headerlink" title="博客"></a>博客</h1><p>参考<a href="https://cuijiahua.com/blog/2018/12/dl-10.html" target="_blank" rel="noopener">深度学习实战教程(四)：卷积神经网络</a><br>卷积神经网络常见架构，也就是N个卷积层叠加，然后(可选)叠加一个Pooling层，重复这个结构M次，最后叠加K个全连接层。<br><img src="/img/media/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/dl-10-3.png" alt=""></p>
<p>卷积神经网络的层结构和全连接神经网络的层结构有很大不同。全连接神经网络每层的神经元是按照一维排列的，也就是排成一条线的样子；而卷积神经网络每层的神经元是按照三维排列的，也就是排成一个长方体的样子，有宽度、高度和深度。</p>
<p>对于上图展示的神经网络，输入层的宽度和高度对应于输入图像的宽度和高度，而它的深度为1。接着，第一个卷积层对这幅图像进行了卷积操作，得到了三个Feature Map。因为图中看出这个卷积层包含三个Filter，也就是三套参数，每个Filter都可以把原始输入图像卷积得到一个Feature Map，三个Filter就可以得到三个Feature Map。至于一个卷积层可以有多少个Filter，那是可以自由设定的。也就是说，卷积层的Filter个数也是一个<strong>超参数</strong>。我们可以把Feature Map可以看做是通过卷积变换提取到的图像特征，三个Filter就对原始图像提取出三组不同的特征，也就是得到了三个Feature Map，也称做<strong>三个通道</strong>(channel)。</p>
<p>在第一个卷积层之后，Pooling层对三个Feature Map做了下采样，得到了三个更小的Feature Map。接着，是第二个卷积层，它有5个Filter。每个Fitler都把前面下采样之后的3个Feature Map卷积在一起，得到一个新的Feature Map。这样，5个Filter就得到了5个Feature Map。接着，是第二个Pooling，继续对5个Feature Map进行下采样，得到了5个更小的Feature Map。</p>
<p>上图最后两层是全连接层，第一个全连接层的每个神经元，和上一层5个Feature Map中的每个神经元相连，第二个全连接层(也就是输出层)的每个神经元，则和第一个全连接层的每个神经元相连，这样得到了整个网络的输出。</p>
<p>对于卷积操作<br><img src="/img/media/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/dl-10-4.png" alt=""><br>用X𝑖,𝑗表示图像的第i行第j列元素；对filter的每个权重进行编号，用W𝑚,𝑛表示第m行第n列权重，用𝑤𝑏表示filter的偏置项；对Feature Map的每个元素进行编号，用𝑎𝑖,𝑗表示Feature Map的第i行第j列元素；用f表示激活函数(这个例子选择relu函数作为激活函数)。然后，使用下列公式计算卷积：<br><img src="/img/media/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/dl-10-1-2.png" alt=""><br>在卷积过程中，可以设置步幅strid。</p>
<p>图像大小、步幅和卷积后的Feature Map大小有关。<br><img src="/img/media/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/dl-10-1-5.png" alt=""><br>在上面两个公式中，𝑊2是卷积后Feature Map的宽度；𝑊1是卷积前图像的宽度；𝐹是filter的宽度；𝑃是Zero Padding数量，Zero Padding是指在原始图像周围补几圈0，如果𝑃的值是1，那么就补1圈0；𝑆是步幅；𝐻2是卷积后Feature Map的高度；𝐻1是卷积前图像的宽度。式2和式3本质上是一样的。</p>
<p>如果卷积前的图像深度为D，那么相应的filter的深度也必须为D。<br><img src="/img/media/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/dl-10-1-7.png" alt=""></p>
<p><img src="/img/media/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/20171209171829551.png" alt=""><br>我们前面还曾提到，每个卷积层可以有多个filter。每个filter和原始图像进行卷积后，都可以得到一个Feature Map。因此，卷积后Feature Map的个数和卷积层的filter个数是相同的。</p>
<p>这里面体现了局部连接和权值共享：每层神经元只和上一层部分神经元相连(卷积计算规则)，且filter的权值对于上一层所有神经元都是一样的。对于包含两个3* 3 *3的fitler的卷积层来说，其参数数量仅有(3 * 3 * 3+1) * 2=56个，且参数数量与上一层神经元个数无关。与全连接神经网络相比，其参数数量大大减少了。</p>
<p>池化层的操作<br>Pooling层主要的作用是下采样，通过去掉Feature Map中不重要的样本，进一步减少参数数量。Max Pooing之外，常用的还有Mean Pooling——取各样本的平均值。对于深度为D的Feature Map，各层独立做Pooling，因此Pooling后的深度仍然为D。</p>
<h2 id="卷积神经网络的训练"><a href="#卷积神经网络的训练" class="headerlink" title="卷积神经网络的训练"></a>卷积神经网络的训练</h2><p>和全连接神经网络相比，卷积神经网络的训练要复杂一些。但训练的原理是一样的：利用链式求导计算损失函数对每个权重的偏导数（梯度），然后根据梯度下降公式更新权重。训练算法依然是反向传播算法。</p>
<p>对于卷积神经网络，由于涉及到局部连接、下采样的等操作，影响到了第二步误差项𝛿的具体计算方法，而权值共享影响了第三步权重𝑤的梯度的计算方法。</p>
<h3 id="卷积层的训练"><a href="#卷积层的训练" class="headerlink" title="卷积层的训练"></a>卷积层的训练</h3><p>获得了所有的梯度之后，就是根据梯度下降算法来更新每个权重。</p>
<h3 id="Pooling层的训练"><a href="#Pooling层的训练" class="headerlink" title="Pooling层的训练"></a>Pooling层的训练</h3><p>无论max pooling还是mean pooling，都没有需要学习的参数。因此，在卷积神经网络的训练中，Pooling层需要做的仅仅是将误差项传递到上一层，而没有梯度的计算。<br>详见<a href="https://cuijiahua.com/blog/2018/12/dl-10.html" target="_blank" rel="noopener">https://cuijiahua.com/blog/2018/12/dl-10.html</a></p>
<h1 id="美团-卷积神经网络"><a href="#美团-卷积神经网络" class="headerlink" title="美团 卷积神经网络"></a>美团 卷积神经网络</h1><p>卷积神经网络中最重要的两个概念是卷积和池化。</p>
<p>卷积操作在信号处理中可以看作滤波过程，过滤或提取出需要的频段信息。相应的，图像处理中也可实现不同功能的卷积操作，如边缘轮廓，锐化，模糊化等。简言之，卷积核沿着输入矩阵从左到右，从上到下遍历，每到一个网格，其输出是输入矩阵对应位置元素相乘并求和。如下图所示的卷积操作过程可以看出，如果卷积核设计得当（可以通过网络学习出来），不同层次的局部重要信息就能提取出来，另一方面卷积核共享参数能显著降低参数量。<br><img src="/img/media/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-28%20%E4%B8%8B%E5%8D%888.53.11.png" alt=""></p>
<p>另外一个重要的概念是池化，下图为最大池化的操作。池化有两个作用：一个是降维，另一个就是保持局部不变性，提取抽象信息。深灰色区域在池化后的输出值为6，而且即使该区域的像素发生扰动（像素值不超过6），输出值也不会变，因此具备一定健壮性。<br><img src="/img/media/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-28%20%E4%B8%8B%E5%8D%888.53.31.png" alt=""><br>卷积神经网络在计算机视觉应用非常广，包括图像分类，检测，识别等，从2012年的AlexNet，到2014年的VGG，2015年的GoogleNet，2016年的ResNet，都是卷积神经网络不断演化并在ImageNet比赛上不断刷新成绩。卷积神经网络在自然语言处理方向也有很好的应用，比如情感分类，文本匹配等。</p>
<h1 id="百面-深度卷积神经网络"><a href="#百面-深度卷积神经网络" class="headerlink" title="百面 深度卷积神经网络"></a>百面 深度卷积神经网络</h1><p>卷积神经网络是一种前馈神经网络，其特点是每层的神经元节点只响应前一层局部区域范围内的神经元（全连接网络中每个神经元节点响应前一层的全部节点）。一个深度卷积神经网络模型通常由若干卷积层叠加若干全连接层组成，中间也包括各种非线性操作以及池化操作。卷积神经网络同样可以使用反向传播算法进行训练，相较于其他网络模型，卷积神经网络的<strong>参数共享</strong>特性使得需要优化的参数数目大大缩减，提高了模型的训练效率以及可扩展性。由于卷积运算主要用于处理类网格结构的数据，因此对于时间序列以及图像数据的分析与识别具有显著优势。</p>
<h2 id="问题1"><a href="#问题1" class="headerlink" title="问题1"></a>问题1</h2><p>问：卷积操作的本质特性包括系数交互和参数共享，具体解释这两种特性及其作用。</p>
<p>答：稀疏交互（sparse interaction）<br>在传统神经网络中，网络层之间输入与输出的连接关系可以由一个权值参数矩阵来表示，其中每个单独的参数值都表示了前后层某两个神经元节点之间的交互。对于全连接网络，任意一对输入与输出神经元之间都产生交互，形成稠密的连接结构。</p>
<p>而在卷积神经网络中，卷积核尺度远小于输入的维度，这样每个输出神经元仅与前一层特定局部区域内的神经元存在连接权重（即产生交互），我们称这种特性为稀疏交互。具体来说，假设网络中相邻两层分别有m个输入和n个输出，全连接网络中权值参数矩阵将包含m * n个参数。对于稀疏交互的卷积神经网络，如果限定每个输出与前一层神经元的连接数为k，那么该层的参数总量为k * n。在实际应用中，一般k值远小于m就可以取得较为可观的效果；而此时优化过程的时间复杂度将会减小几个数量级，过拟合的情况也得到较好的改善。<br><img src="/img/media/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-28%20%E4%B8%8B%E5%8D%889.57.16.png" alt=""><br>稀疏交互的物理意义是，通常图像，文本，语音等现实世界中的数据都具有局部的特征结构，我们可以先学习局部的特征，再将局部的特征组合起来形成更复杂和抽象的特征。</p>
<p>参数共享（Parameter Sharing）<br>参数共享是指在同一个模型的不同模块中使用相同的参数，它是卷积运算的固有属性。全连接网络中，计算每层的输出时，权值参数矩阵中的每个元素只作用于某个输入元素一次；而在卷积神经网络中，卷积核中的每一个元素将作用于每一次局部输入的特定位置上。根据参数共享的思想，我们只需要学习一组参数集合，而不需要针对每个位置的每个参数都进行优化，从而大大降低了模型的存储需求。</p>
<p>参数共享的物理意义是使得卷积层具有平移等特性。假如图像中有一只猫，那么无论它出现在图像中的任何位置，我们都应该将它识别为猫，也就是说神经网络的输出对于平移变换来说应当是等变的。特别地，当函数f（x）与g（x）满足f（g（x））=g（f（x））时我们称f（x）关于变换g具有等变性。也就是说，在猫的图片上先进行卷积，再向右平移l像素的输出，与先将图片向右平移l像素再进行卷积操作的输出结果是相等的。</p>
<h2 id="问题3"><a href="#问题3" class="headerlink" title="问题3"></a>问题3</h2><p>问：卷积神经网络如何用于文本分类任务？</p>
<p>答：卷积神经网络的核心思想是捕捉局部特征，最初在图像领域取得了巨大的成功，后来在文本领域也得到了广泛的应用。对于文本来说，局部特征就是由若干单词组成的滑动窗口，类似于N-gram特征进行组合和筛选，获得不同抽象层次的语义信息。由于在每次卷积中采用了共享权重的机制，因此它的训练速度相对较快，在实际的文本分类任务中取得了非常不错的效果。</p>
<p>下图是一个用卷积神经网络模型进行文本表示，并最终用于文本分类的网络结构。<br><img src="/img/media/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-01%20%E4%B8%8B%E5%8D%881.35.52.png" alt=""><br> 输入层是一个N * K的矩阵，其中N为文章所对应的单词总数，K是每个词对应的表示向量的维度。每个词的K维向量可以是预先在其他语料库中训练好的，也可以作为未知的参数有网络训练得到。这两种方法各有优势，一方面，预先训练的词嵌入可以利用其他语料库得到更多的先验知识；另一方面，由当前网络训练的词向量能够更好地抓住与当前任务相关联的特征。因此，图中的输入层实际采用了两个通道的形式，即有两个N * K的输入矩阵，其中一个用预先训练好的词嵌入表达，并且在训练过程中不再发生变化；另外一个也由同样的方式初始化，但是会作为参数，随着网络的训练发生改变。</p>

          
        
      
    </div>

    

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://github.com/zdkswd/2019/02/28/百面 多层感知机的反向传播算法/"/>

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="ZDK"/>
      <meta itemprop="description" content=""/>
      <meta itemprop="image" content="/images/avatar.gif"/>
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ZDK's blog"/>
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/02/28/百面 多层感知机的反向传播算法/" class="post-title-link" itemprop="url">百面机器学习 多层感知机的反向传播算法</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-02-28 20:01:47" itemprop="dateCreated datePublished" datetime="2019-02-28T20:01:47+08:00">2019-02-28</time>
            </span>
          

          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-03-29 21:42:10" itemprop="dateModified" datetime="2019-03-29T21:42:10+08:00">2019-03-29</time>
              </span>
            
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/知识总结/" itemprop="url" rel="index"><span itemprop="name">知识总结</span></a></span>

                
                
              
            </span>
          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">评论数：</span>
                <a href="/2019/02/28/百面 多层感知机的反向传播算法/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2019/02/28/百面 多层感知机的反向传播算法/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
            <span id="/2019/02/28/百面 多层感知机的反向传播算法/" class="post-meta-item leancloud_visitors" data-flag-title="百面机器学习 多层感知机的反向传播算法">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              
                <span class="post-meta-item-text">阅读次数：</span>
              
                <span class="leancloud-visitors-count"></span>
            </span>
          

          

          

          <br/>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="百面-多层感知机的反向传播算法"><a href="#百面-多层感知机的反向传播算法" class="headerlink" title="百面 多层感知机的反向传播算法"></a>百面 多层感知机的反向传播算法</h1><p><img src="/img/media/%E7%99%BE%E9%9D%A2%20%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%9A%84%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-28%20%E4%B8%8B%E5%8D%884.35.49.png" alt=""><br>在网络训练中，前向传播最终产生一个标量损失函数，反向传播算法则将损失函数的信息沿网络层向后传播用以计算梯度，达到优化网络参数的目的。</p>
<h2 id="问题1"><a href="#问题1" class="headerlink" title="问题1"></a>问题1</h2><p>问：写出多层感知机的平方误差和交叉熵损失函数。</p>
<p>答：图片内容说明，图中有平方误差函数以及交叉熵损失函数在二分类和多分类的情况下的表达式。<br><img src="/img/media/%E7%99%BE%E9%9D%A2%20%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%9A%84%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95/321551343834_.pic_hd.jpg" alt=""></p>
<h2 id="问题2"><a href="#问题2" class="headerlink" title="问题2"></a>问题2</h2><p>问：根据问题1中定义的损失函数，推导各层参数更新的梯度计算公式。</p>
<p><img src="/img/media/%E7%99%BE%E9%9D%A2%20%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%9A%84%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95/341551353140_.pic.jpg" alt=""><br><img src="/img/media/%E7%99%BE%E9%9D%A2%20%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%9A%84%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95/351551353141_.pic.jpg" alt=""></p>
<p><img src="/img/media/%E7%99%BE%E9%9D%A2%20%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%9A%84%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95/331551353138_.pic.jpg" alt=""></p>
<h2 id="问题3"><a href="#问题3" class="headerlink" title="问题3"></a>问题3</h2><p>问：平方误差损失函数和交叉熵损失函数分别适合什么场景？</p>
<p>答：一般来说，平方损失函数更适合输出为连续，并且最后一层不含Sigmoid或Softmax激活函数的神经网络；交叉熵损失则更适合二分类或多分类的场景。</p>
<p>平方损失函数不适合最后一层含有Sigmoid或Softmax激活函数的神经网路的原因是。由（9.23）得，右式最后一项是激活函数的导数，当激活函数为Sigmoid函数时，如果z的绝对值较大，函数的梯度会趋于饱和，即右式最后一项导数的绝对值非常小，导致等式的左边取值非常小，使得基于梯度的学习速度非常缓慢。</p>
<p>当使用交叉熵损失时，激活函数为Softmax，由（9.27），导数是线性的，因此不会存在学习速度过慢的问题。</p>
<h2 id="神经元"><a href="#神经元" class="headerlink" title="神经元"></a>神经元</h2><p><img src="/img/add/27.png" alt=""></p>

          
        
      
    </div>

    

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://github.com/zdkswd/2019/02/28/激活函数 训练技巧/"/>

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="ZDK"/>
      <meta itemprop="description" content=""/>
      <meta itemprop="image" content="/images/avatar.gif"/>
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ZDK's blog"/>
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/02/28/激活函数 训练技巧/" class="post-title-link" itemprop="url">激活函数 训练技巧</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-02-28 16:00:47" itemprop="dateCreated datePublished" datetime="2019-02-28T16:00:47+08:00">2019-02-28</time>
            </span>
          

          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-03-10 15:47:38" itemprop="dateModified" datetime="2019-03-10T15:47:38+08:00">2019-03-10</time>
              </span>
            
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/知识总结/" itemprop="url" rel="index"><span itemprop="name">知识总结</span></a></span>

                
                
              
            </span>
          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">评论数：</span>
                <a href="/2019/02/28/激活函数 训练技巧/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2019/02/28/激活函数 训练技巧/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
            <span id="/2019/02/28/激活函数 训练技巧/" class="post-meta-item leancloud_visitors" data-flag-title="激活函数 训练技巧">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              
                <span class="post-meta-item-text">阅读次数：</span>
              
                <span class="leancloud-visitors-count"></span>
            </span>
          

          

          

          <br/>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="百面-深度神经网络中的激活函数"><a href="#百面-深度神经网络中的激活函数" class="headerlink" title="百面 深度神经网络中的激活函数"></a>百面 深度神经网络中的激活函数</h1><p>线性模型是机器学习领域中最基本也是最重要的工具，以逻辑回归和线性回归为例，无论是通过闭解形式还是使用凸优化，它们都能高效且可靠地拟合数据。然而真实情况中，我们往往会遇到线性不可分的问题（如XOR异或函数），需要非线性变换对数据的分布进行重新映射。对于深度神经网络，我们在每一层线性变换后叠加一个非线性激活函数，以避免多层网络等效于单层线性函数，从而获得更强大的学习与拟合能力。</p>
<h2 id="问题1"><a href="#问题1" class="headerlink" title="问题1"></a>问题1</h2><p>问：写出常用的激活函数及其导数。</p>
<p>答：<br><strong>Sigmoid</strong>激活函数的形式为<br><img src="/img/media/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%20%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/WechatIMG121551316508_.pic_hd.jpg" alt=""><br>对应的导函数为<br><img src="/img/media/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%20%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/WechatIMG161551316509_.pic_hd.jpg" alt=""></p>
<p><strong>Tanh</strong>激活函数的形式为<br><img src="/img/media/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%20%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/WechatIMG141551316509_.pic_hd.jpg" alt=""><br>对应的导函数为<br><img src="/img/media/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%20%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/WechatIMG131551316509_.pic_hd.jpg" alt=""></p>
<p><strong>ReLU</strong>激活函数的形式为<br><img src="/img/media/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%20%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/WechatIMG111551316508_.pic_hd.jpg" alt=""><br>对应的导函数为<br><img src="/img/media/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%20%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/WechatIMG151551316509_.pic_hd.jpg" alt=""></p>
<h2 id="问题2"><a href="#问题2" class="headerlink" title="问题2"></a>问题2</h2><p>问：为什么Sigmoid和Tanh激活函数会导致梯度消失的现象？</p>
<p>答：<strong>Sigmoid</strong>激活函数的曲线如下，它将输入z映射到区间（0，1），当z很大时，f（z）趋近于1；当z很小时，f（z）趋近于0。其导数在z很大或很小时都会趋近于0，造成梯度消失的现象。<br><img src="/img/media/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%20%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/191551317388_.pic_hd.jpg" alt=""></p>
<p><strong>Tanh</strong>激活函数的曲线如下。当z很大时，f（z）趋近于1；当z很小时，f（z）趋近于-1.。其导数在z很大或很小时都会趋近于0，同样会出现“梯度消失”。实际上，<strong>Tanh</strong>激活函数相当于<strong>Sigmoid</strong>的平移：<br><img src="/img/media/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%20%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/171551317387_.pic_hd.jpg" alt=""><br><img src="/img/media/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%20%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/181551317388_.pic_hd.jpg" alt=""></p>
<h2 id="问题3"><a href="#问题3" class="headerlink" title="问题3"></a>问题3</h2><p>问：ReLU系列的激活函数相对于Sigmoid和Tanh激活函数的优点是什么？它们有什么局限性以及如何改进？</p>
<p>答：优点：</p>
<ol>
<li>从计算的角度，Sigmoid和Tanh激活函数均需要计算指数，复杂度高，而ReLU只需要一个阈值即可得到激活值。</li>
<li>ReLU的非饱和性可以有效地解决梯度消失的问题，提供相对宽的激活边界。</li>
<li>ReLU的单侧抑制提供了网络的稀疏表达能力。</li>
</ol>
<p>关于稀疏性的解释：通过对大脑的研究发现，大脑在工作的时候只有大约5%的神经元是激活的，而采用sigmoid激活函数的人工神经网络，其激活率大约是50%。有论文声称人工神经网络在15%-30%的激活率时是比较理想的。因为relu函数在输入小于0时是完全不激活的，因此可以获得一个更低的激活率。</p>
<p>局限性：<br>ReLU的局限性在于其训练过程中会导致神经元的死亡问题。这是由于函数f（z）=max（0，z）导致负梯度在经过该ReLU单元时被置为0，且在之后也不被任何数据激活，即流经该神经元的梯度永远为0，不对任何数据产生响应。在实际训练中，如果学习率设置较大，会导致超过一定比例的神经元不可逆死亡，进而参数梯度无法更新，整个训练过程失败。</p>
<p>为解决这一问题，人们设计了ReLU的变种Leaky ReLU（LReLU），其形式表示为<br><img src="/img/media/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%20%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/201551318136_.pic_hd.jpg" alt=""><br>ReLU和LReLU的函数曲线对比如下图，一般a是一个很小的正常数，这样即实现了单侧抑制，又保留了部分负梯度信息以致不完全丢失，另一方面，a值的选择增加了问题难度，需要较强的人工先验或多次重复训练以确定合适的参数值。<br><img src="/img/media/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%20%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/211551318136_.pic_hd.jpg" alt=""><br>基于比，参数化的PReLU应运而生。它与LReLU的主要区别是将负轴部分斜率a作为网络中一个可学习的参数，进行反向传播训练，与其他含参数网络层联合优化。而另一个LReLU的变种增加了随机化机制，具体的，在训练过程中，斜率a作为一个满足某种分布的随机采样，测试时再固定下来。Random ReLU在一定程度上能起到正则化的作用。</p>
<h1 id="百面-神经网络训练技巧"><a href="#百面-神经网络训练技巧" class="headerlink" title="百面 神经网络训练技巧"></a>百面 神经网络训练技巧</h1><p>在大规模神经网络的训练过程中，我们经常会面临过拟合问题，即当参数数目过于庞大而相应的训练数据短缺时，模型在训练集上损失值很小，但在测试集上损失较大，泛化能力很差。解决过拟合的方法有很多，包括数据集增强，参数范数惩罚，正则化，模型集成等。其中Dropout是模型集成方法中最高效与常用的技巧。同时，深度神经网络的训练中涉及诸多手调参数，如学习率，权重衰减系数，Dropout比例等，这些参数的选择会显著影响模型最终的训练效果，批量归一化（Batch Normalization，BN）方法有效规避了这些复杂参数对网络训练产生的影响，在加速训练收敛的同时也提升了网络的泛化能力。</p>
<h2 id="问题1-1"><a href="#问题1-1" class="headerlink" title="问题1"></a>问题1</h2><p>问：神经网络训练时是否可以将全部参数初始化为0？</p>
<p>答：考虑全连接的深度神经网络，同一层中的任意神经元都是同构的，它们拥有相同的输入和输出，如果再将参数全部初始化为同样的值，那么无论是前向传播还是反向传播的取值都是完全相同的。学习过程将永远无法打破这种对称性，最终同一网络层中各个参数任然是相同的。</p>
<p>因此，我们需要随机地初始化神经网络参数的值，以打破这种对称性。简单来说，我们可以初始化参数为取值范围<br><img src="/img/media/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%20%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/221551322121_.pic.jpg" alt=""><br>的均匀分布，其中d是一个神经元接受的输入维度。偏置可以被简单地设为0，并不会导致参数对称的问题。</p>
<h2 id="问题2-1"><a href="#问题2-1" class="headerlink" title="问题2"></a>问题2</h2><p>问：为什么Dropout可以抑制过拟合？它的工作原理和实现？</p>
<p>答：Dropout是指在深度网络的训练中，以一定的概率随机地“临时丢弃”一部分神经元节点，具体来讲，Dropout作用于每份小批量训练数据，由于其随机丢弃部分神经元的机制，相当于每次迭代都在训练不同结构的神经网络。类比于Bagging方法，Dropout可被认为是一种使用的大规模深度神经网络的模型集成算法。这是由于传统意义上的Bagging涉及多个模型的同时训练与测试评估，当网络与参数规模庞大时，这种集成方式需要消耗大量的运算时间与空间。Dropout在小批量级别上的操作，提供了一种轻量级的Bagging集成近似，能够实现指数级数量神经网络的训练与评测。</p>
<p>Dropout的具体实现中，要求某个神经元节点激活值以一定的概率p被丢弃，即该神经元暂时停止工作，如下图所示<br><img src="/img/media/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%20%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/241551332898_.pic_hd.jpg" alt=""><br>因此，对于包含N个神经元节点的网络，在Dropout的作用下可看作为2^N 个模型的集成。这2^N 个模型可认为是原始网络的子网络，它们共享部分权值，并且具有相同的网络层数，而模型整体的参数数目不变，这就大大简化了运算。对于任意神经元，每次训练中都与一组随机挑选的不同神经元集合共同进行优化，这个过程会减弱全体神经元之间的联合适应性，减小过拟合风险，增强泛化能力。</p>
<p>在神经网络中应用Dropout包括训练和预测两个阶段。在训练阶段中，每个神经节点需要增加一个概率系数，如下图所示。<br><img src="/img/media/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%20%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/251551333501_.pic_hd.jpg" alt=""><br>训练阶段又分为前向传播和反向传播两个步骤，原始网络对应的前向传播公式为：<br><img src="/img/media/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%20%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/231551332897_.pic_hd.jpg" alt=""><br>应用Dropout之后，前向传播公式变为：<br><img src="/img/media/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%20%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/271551334463_.pic_hd.jpg" alt=""><br><img src="/img/media/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%20%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/261551334462_.pic_hd.jpg" alt=""><br>上面的Bernoulli函数的作用是以概率系数p随机生成一个取值为0或1的向量，代表每个神经元是否需要被丢弃。如果取值为0，则该神经元将不会计算梯度或参与后面的误差传播。</p>
<p>测试阶段是前向传播过程。在前向传播的计算时，每个神经元的参数要预先乘以概率系数p，以恢复在训练该神经元只有p的概率被用于整个神经网络的前向传播计算。</p>
<h2 id="问题3-1"><a href="#问题3-1" class="headerlink" title="问题3"></a>问题3</h2><p>问：批量归一化的基本动机与原理是什么？在卷积神经网络中如何使用？</p>
<p>答：神经网络训练的本质是学习数据分布，如果训练数据与测试数据的分布不同将大大降低网络的泛化能力，因此我们需要在训练开始前对所有输入数据进行归一化处理。</p>
<p>然而随着网络训练的进行，每个隐层的参数变化使得后一层的输入发生变化，从而每一批训练数据的分布也随之改变，致使网络在每次迭代中都需要拟合不同的数据分布，增大训练的复杂度以及过拟合的风险。</p>
<p>批量归一化方法是针对每一批数据，在网络的每一层输入之前增加归一化处理（均值Wie0，标准差为1），将所有批数据强制在统一地数据分布下，即对该层的任意一个神经元（假设为第k维）采用如下公式：<br><img src="/img/media/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%20%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/281551335543_.pic.jpg" alt=""><br>x^k 是该层第k个神经元的原始输入数据，E是这一批输入数据在第k个神经元的均值，分母是这一批数据在第k个神经元的标准差。</p>
<p>批量归一化可以看作在每一层输入和上一层输出之间加入了一个新的计算层，对数据的分布进行额外的约束，从而增强模型的泛化能力。但是批量归一化同时也降低了模型的拟合能力，归一化之后的输入分布被强制为0均值和1标准差。以Sigmoid激活函数为例，批量归一化之后数据整体处于函数的非饱和区域，只包含线性变换，破坏了之前学习到的特征分布。为了恢复原始数据分布，具体实现中引入了变换重构以及可学习参数γ和β：<br><img src="/img/media/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%20%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/291551336805_.pic.jpg" alt=""><br>其中γ和β分别是输入数据分布的方差和偏差。对于一般的网络，不采用批量归一化操作时，这两个参数高度依赖前面网络学习到的连接权重（对应复杂的非线性）。而在批量归一化操作中，γ和β变成了该层的学习参数，仅用两个参数就可以恢复最优的输入数据分布，与之前网络层的参数解耦，从而更有利于优化的过程，提高模型的泛化能力。</p>
<p>完整的批量归一化网络层的前向传导过程公式如下：<br><img src="/img/media/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%20%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/301551338705_.pic.jpg" alt=""><br>批量归一化在卷积神经网络中应用时，需要注意卷积神经网络的参数共享机制。每一个卷积核的参数在不同位置的神经元当中是共享的，因此也应该被一起归一化。具体实现中，假设网络训练中每一批包含b个样本，由一个卷积核生成的特征图的宽高分别是w和h，则每个特征图所对应的全部神经元个数为b <em> w </em> h；利用这些神经元对应的所有输入数据，我们根据一组待学习的参数γ和β对每个输入数据进行批量归一化操作。如果有f个卷积核，就对应f个特征图和f组不同的β和γ参数。</p>
<p>批标准化的使用方法：<br>批标准化一般用在非线性映射（激活函数）之前，对y= Wx + b进行规范化，是结果(输出信号的各个维度)的均值都为0,方差为1,让每一层的输入有一个稳定的分布会有利于网络的训练。<br>在神经网络收敛过慢或者梯度爆炸时的那个无法训练的情况下都可以尝试。<br>批准标化指的是批数据, 把数据分成小批小批进行随机梯度下降. 而且在每批数据进行前向传递的时候, 对每一层都进行标化的处理。</p>
<h1 id="美团-激活函数"><a href="#美团-激活函数" class="headerlink" title="美团 激活函数"></a>美团 激活函数</h1><p>在深度前馈神经网络中，有个关键的问题需要考虑：非线性变换有什么用，如何进行选择？</p>
<p>理论上，包含非线性变换的深度前馈神经网络能拟合任何连续函数。非线性变换一方面可以解决线性感知机无法解决的问题（比如XOR问题），真正发挥深层网络的优势。</p>
<p>Sigmoid有双边饱和效应，当信号超过一定门限后会被抑制，反向梯度容易消失，回传不会多远。而且取值范围（0，1），取值不是以0为中心对称，这样容易产生梯度同正同负的线性，降低收敛速度，适合在最外层用来做概率预测。</p>
<p>Tanh取值虽然是以0为中心对称，但依然存在饱和问题。</p>
<p>在深层网络中，ReLU使用最广泛，它以0为中心，右边是线性函数可以保证梯度回传很远，左边直接对信号进行抑制。</p>

          
        
      
    </div>

    

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://github.com/zdkswd/2019/02/27/感知机/"/>

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="ZDK"/>
      <meta itemprop="description" content=""/>
      <meta itemprop="image" content="/images/avatar.gif"/>
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ZDK's blog"/>
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/02/27/感知机/" class="post-title-link" itemprop="url">感知机</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-02-27 19:28:47" itemprop="dateCreated datePublished" datetime="2019-02-27T19:28:47+08:00">2019-02-27</time>
            </span>
          

          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-02-28 17:00:10" itemprop="dateModified" datetime="2019-02-28T17:00:10+08:00">2019-02-28</time>
              </span>
            
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/知识总结/" itemprop="url" rel="index"><span itemprop="name">知识总结</span></a></span>

                
                
              
            </span>
          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">评论数：</span>
                <a href="/2019/02/27/感知机/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2019/02/27/感知机/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
            <span id="/2019/02/27/感知机/" class="post-meta-item leancloud_visitors" data-flag-title="感知机">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              
                <span class="post-meta-item-text">阅读次数：</span>
              
                <span class="leancloud-visitors-count"></span>
            </span>
          

          

          

          <br/>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="百面机器学习"><a href="#百面机器学习" class="headerlink" title="百面机器学习"></a>百面机器学习</h1><p>深度前馈网络是一类网络模型的统称，常见的多层感知机，自编码器，限制玻尔兹曼机，以及卷积神经网络等，都是其中的成员。<br>在人工神经网络领域中，感知机也被认为是单层的人工神经网络。</p>
<h2 id="多层感知机与布尔函数"><a href="#多层感知机与布尔函数" class="headerlink" title="多层感知机与布尔函数"></a>多层感知机与布尔函数</h2><p>神经网络概念的诞生很大程度上受到了神经科学的启发。生物学研究表明，大脑皮层的感知与计算功能是分层实现的，例如视觉图像，首先光信号进入大脑皮层的V1区，即初级视皮层，之后依次通过V2层和V4层，即纹外皮层，进入下叶参与物体识别。深度神经网络，除了模拟人脑功能的多层结构，最大的优势在于能够以紧凑简洁的方式来表达比浅层网络更复杂的函数集合（这里的简洁定义为隐层单元的数目与输入单元的数目呈多项式关系）。</p>
<h3 id="问题1"><a href="#问题1" class="headerlink" title="问题1"></a>问题1</h3><p>问：多层感知机表示异或逻辑时最少需要几个隐含层（仅考虑二元输入）？</p>
<p>答：回顾逻辑回归的公式Z=sigmoid(AX+BY+C)，发现如果用表示XY的异或关系，采用逻辑回归（即不带隐藏层的感知机）无法精确学习出一个输出为异或的模型表示。</p>
<p>考虑有一个隐藏层的情况，通用近似定理表示，一个前馈神经网络如果具有线性输出层和至少一层具有任何一种“挤压”性质的激活函数的隐藏层，当给予网络足够数量的隐藏单元时，可以以任意精度近似任何从一个有限维空间到另一个有限维空间的波莱尔可测函数。可以简单的认为我们常用的激活函数和目标函数是通用近似定理适用的一个子集，因此多层感知机的表达能力是非常强的，关键在于我们是否能够学习到对应此表达的模型参数。</p>
<p>第一个隐藏单元在X和Y均为1时激活，第二个隐藏单元在X和Y均为0时激活，最后再将两个隐藏单元的输出做一个线性变换即可事先异或操作。<br><img src="/img/media/%E6%84%9F%E7%9F%A5%E6%9C%BA/WechatIMG5.jpeg" alt=""></p>
<h3 id="问题2"><a href="#问题2" class="headerlink" title="问题2"></a>问题2</h3><p>问：如果只使用一个隐层，需要多少隐节点能够实现包含n元输入的任意布尔函数？</p>
<p>答：包含n元输入的任意布尔函数可以唯一表示为析取范式，当n=5时的简单范例：<br><img src="/img/media/%E6%84%9F%E7%9F%A5%E6%9C%BA/WechatIMG6.jpeg" alt=""><br>最终的输出Y可以表示成由6个合取范式所组成的析取范式。该函数可由包含6个隐节点的3层感知机实现。<br><img src="/img/media/%E6%84%9F%E7%9F%A5%E6%9C%BA/WechatIMG7.jpeg" alt=""><br>首先证明单个隐节点可以表示任意合取范式。考虑任意布尔变量假设Xi，若它在合取范式中出现的形式为正（Xi），则设权重为1；若出现的形式为非，则设置权重为-1；若没有在合取范式中出现。设置权重为0；并且偏置设为合取范式中变量的总数取负加1。可以看出，当采用ReLU激活函数之后，当且仅当所有出现的布尔变量均满足条件时，该隐藏单元才会被激活（输出1），否则输出0，这与合取范式的定义的相符。然后，令所有隐藏单元到输出层的参数为1，并设输出单元的偏置为0.这样，当且仅当所有的隐藏单元到输出层的参数为1，并设输出单元的偏置为0.这样，当且仅当所有的隐藏单元都未被激活时才会输出0，否则都将输出一个正数，起到了析取的作用。</p>
<p>可以使用卡诺图表示析取式，即用网格表示真值表，当输入的合取式值为1时，则填充相应的网格。卡诺图中相邻的填色区域可以进行规约，以达到化简布尔函数的目的。该函数可由包含3个隐节点的3层感知机实现：<br><img src="/img/media/%E6%84%9F%E7%9F%A5%E6%9C%BA/WechatIMG9.jpeg" alt=""><br>回顾初始的问题：在最差的情况下，需要多少个隐藏结点来表示包含n元输入的布尔函数呢？现在问题可以转化为：寻找“最大不可规约的”n元析取范式，也等价于最大不可规约的卡诺图。直观上，我们只需间隔填充网格即可实现，其表示的布尔函数恰为n元输入的异或操作，如下图，容易看出，在间隔填充的网格上反转任意网格的取值都会引起一次规约，因此，n元布尔函数的析取范式最多包含2^(n-1)个不可规约的合取范式，对于单隐层的感知机，需要2^(n-1)个隐节点来实现。<br><img src="/img/media/%E6%84%9F%E7%9F%A5%E6%9C%BA/WechatIMG8.jpeg" alt=""></p>
<h3 id="问题3"><a href="#问题3" class="headerlink" title="问题3"></a>问题3</h3><p>问：考虑多隐层的情况，实现包含n元输入的任意布尔函数最少需要多少个网络节点和网络层？</p>
<p>答：参考问题1的解答，考虑二元输入的情况，需要3个节点可以完成一次异或操作，其中隐藏层由两个节点构成，输出层需要一个结点，用来输出异或的结果并作为下一个结点的输入。对于四元输入，包含三次异或操作，需要3 * 3=9个节点即可完成。如下图<br><img src="/img/media/%E6%84%9F%E7%9F%A5%E6%9C%BA/WechatIMG10.jpeg" alt=""><br>输入W，X，Y，Z四个布尔变量，首先用3个结点计算W异或X；然后再加入3个结点，将W，X的输出与Y进行异或，得到W异或X异或Y，最后与Z进行异或，整个网络总共需要9个结点。以此类推，n元异或函数需要包括3（n-1）个节点（包括最终输出节点）。多隐层结构可以将隐节点的数目从指数级O（2^(n-1)）直接减少至线性级O（3（n-1））。</p>
<p>层数还可以进一步减小，如果在同一层中计算W异或X和Y异或Z，再将二者的输出进行异或就能将层数从6变成4，根据二分思想，每层节点两两分组进行异或运算，需要的最少网络层数为2log2N（向上取整）。</p>
<h1 id="美团-深度学习技术发展历程"><a href="#美团-深度学习技术发展历程" class="headerlink" title="美团 深度学习技术发展历程"></a>美团 深度学习技术发展历程</h1><h2 id="第一阶段-1943到1986年"><a href="#第一阶段-1943到1986年" class="headerlink" title="第一阶段 1943到1986年"></a>第一阶段 1943到1986年</h2><p>1943年，人工神经元模型MCP（作者名字的缩写）的发明可以看做是人工神经网络的起点。MCP模型由多个输入加权求和，二值激活函数组成，通过网络来模拟神经元的过程。</p>
<p>1957年，感知机算法Perceptron的发明，使用MCP模型对多维输入做二分类，梯度下降法学习权重。感知机算法的思想很简单，即通过样本正确与否调整分类面，使得分类面对样本的分类误差最小。该算法是神经网络和支持向量机的基础，随后被证明能够收敛，其理论和实践效果引发了第一次神经网络浪潮。</p>
<p>1969年，美国数学家和人工智能先驱Minsky证明了感知机是一种线性模型，它只能处理线性分类问题，比如简单的异或问题（XOR）就解决不了。从而神经网络陷入了第一次近20年的停止期。</p>
<h2 id="第二阶段-1986年到2006年"><a href="#第二阶段-1986年到2006年" class="headerlink" title="第二阶段 1986年到2006年"></a>第二阶段 1986年到2006年</h2><p>1986年，Hinton发明了优化多层感知机（MLP）的反向传播算法（BP）算法，从而解决了神经网络只能解决线性分类的问题，引发了第二轮研究热潮。三年后，就证明了MLP的万能逼近原理，也就是包含非线性隐层的MLP能逼近任意的联系函数，极大鼓舞了神经网络研究热情。</p>
<p>1990年，20世纪90年代，支持向量机如火如荼发展起来，理论上有很好的解释，且效果非常好。而神经网络开始走下坡路，一方面BP在深层网络中存在梯度消失和爆炸问题，另一方面其理论解释没那么完善，就这样神经网络一直低迷到2006年。</p>
<h2 id="第三阶段-2006年至今"><a href="#第三阶段-2006年至今" class="headerlink" title="第三阶段 2006年至今"></a>第三阶段 2006年至今</h2><p>2006年。Hinton有了重大发明，提出的无监督分层初始化方法结合深度玻尔兹曼机解决了深层神经网络梯度消失等难题。这项发明发表在science期刊上，从而开启了深度学习第三次研究热潮。</p>
<p>2012年，ImageNet比赛夺冠AlexNet，引爆深度学习研究热潮。</p>
<p>2015年至今，各大公司纷纷开源其深度学习框架和模型，极大推进了各领域应用深度学习的进程。</p>
<h1 id="统计学习方法-感知机"><a href="#统计学习方法-感知机" class="headerlink" title="统计学习方法  感知机"></a>统计学习方法  感知机</h1><p>感知机(perceptron)是二类分类的线性分类模型,其输入为实例的特征向量, 输出为实例的类别,取+1和-1二值.感知机对应于输入空间(特征空间)中将实例划分为正负两类的分离超平面,属于判别模型.感知机学习旨在求出将训练数据进行线性划分的分离超平面,为此,导入基于误分类的损失函数,利用梯度下降法对损失函数进行极小化,求得感知机模型.感知机学习算法具有简单而易于实现的优点,分为原始形式和对偶形式.感知机预测是用学习得到的感知机模型对新的输入实例进行分类.感知机1957年由 Rosenblat提出,是神经网络与支持向量机的基础。</p>
<h2 id="感知机模型"><a href="#感知机模型" class="headerlink" title="感知机模型"></a>感知机模型</h2><p><img src="/img/media/%E6%84%9F%E7%9F%A5%E6%9C%BA/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-27%20%E4%B8%8B%E5%8D%887.14.25.png" alt=""><br>感知机是一种线性分类模型，属于判别模型。感知机模型的假设空间是定义在特征空间中的所有线性分类模型或线性分类器。</p>
<p>感知机的几何解释，线性方程wx+b=0，对于特征空间R中的一个超平面S，其中w是超平面的法向量，b是超平面的截距。这个超平面将特征空间划分为两个部分。位于两部分的点（特征向量）分别为正负两类，因此，超平面S被称为分离超平面。<br><img src="/img/media/%E6%84%9F%E7%9F%A5%E6%9C%BA/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-27%20%E4%B8%8B%E5%8D%887.19.26.png" alt=""></p>
<p>感知机学习的策略是极小化损失函数：<br><img src="/img/media/%E6%84%9F%E7%9F%A5%E6%9C%BA/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-27%20%E4%B8%8B%E5%8D%887.22.22.png" alt=""><br>损失函数对应于误分类点到分离超平面的总距离。</p>
<p>感知机学习算法是基于随机梯度下降法的对损失函数的最优化算法，有原始形式和对偶形式.算法简单且易于实现.原始形式中，首先任意选取一个超平面，然后用梯度下降法不断极小化目标函数.在这个过程中一次随机选取一个误分类点使其梯度下降。</p>

          
        
      
    </div>

    

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://github.com/zdkswd/2019/02/26/FM FFM/"/>

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="ZDK"/>
      <meta itemprop="description" content=""/>
      <meta itemprop="image" content="/images/avatar.gif"/>
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ZDK's blog"/>
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/02/26/FM FFM/" class="post-title-link" itemprop="url">FM FFM</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-02-26 18:27:56 / 修改时间：18:28:44" itemprop="dateCreated datePublished" datetime="2019-02-26T18:27:56+08:00">2019-02-26</time>
            </span>
          

          
            

            
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/知识总结/" itemprop="url" rel="index"><span itemprop="name">知识总结</span></a></span>

                
                
              
            </span>
          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">评论数：</span>
                <a href="/2019/02/26/FM FFM/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2019/02/26/FM FFM/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
            <span id="/2019/02/26/FM FFM/" class="post-meta-item leancloud_visitors" data-flag-title="FM FFM">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              
                <span class="post-meta-item-text">阅读次数：</span>
              
                <span class="leancloud-visitors-count"></span>
            </span>
          

          

          

          <br/>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="FM-FFM"><a href="#FM-FFM" class="headerlink" title="FM FFM"></a>FM FFM</h1><h1 id="深入浅出Factorization-Machines系列"><a href="#深入浅出Factorization-Machines系列" class="headerlink" title="深入浅出Factorization Machines系列"></a>深入浅出Factorization Machines系列</h1><p><a href="http://kubicode.me/2018/02/23/Deep%20Learning/Deep-in-out-Factorization-Machines-Series/" target="_blank" rel="noopener">深入浅出Factorization Machines系列 | Kubi Code’Blog</a></p>
<h1 id="FM"><a href="#FM" class="headerlink" title="FM"></a>FM</h1><p>Factorization Machine(FM)由Steffen Rendle在2010年提出，旨在解决系数数据下的特征组合的问题，目前该系列模型在搜索推荐领域被广泛使用。</p>
<h2 id="一个例子"><a href="#一个例子" class="headerlink" title="一个例子"></a>一个例子</h2><p><img src="/img/media/FM%20FFM/fm_case.png" alt=""><br>问题就是需要对电影进行评分(y项)，而x都是特征,其中:</p>
<ol>
<li>第一部分蓝色的为当前评分的用户</li>
<li>第二部分红色的为被评分的电影</li>
<li>第三部分黄色的为该用户曾经对其他电影的评分情况</li>
<li>第四部分绿色的为该用户当前评分的月数</li>
<li>第五部分棕色为该用户最新一次评分的电影</li>
</ol>
<p>这是一个经典的回归问题，最简单粗暴的方法就先上一个线性回归，其中对于绿色特征处理成binary，这样计算公式就是为<br><img src="/img/media/FM%20FFM/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-26%20%E4%B8%8A%E5%8D%889.45.36.png" alt=""><br>这样可能会过于简单粗暴，按照算法（特征）工程师的套路会对某些特征进行组合，这样为了方便，咱们就给他来一个全组合:<br><img src="/img/media/FM%20FFM/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-26%20%E4%B8%8A%E5%8D%889.53.57.png" alt=""><br>看似问题解决了，但是这样会存在这么几个问题:</p>
<ol>
<li>参数空间过大,这里为O(n2)，在处理互联网数据时，特征量级别可能是亿级别的。</li>
<li>需要人工经验，这里一般会选择某些特征来组合，此时人工/专家经验就会很重要。</li>
<li>样本量稀疏，实际上那这种方式拿到的特征会是很稀疏的，对于在训练样本中未出现过的组合该模型无能为力。</li>
</ol>
<h2 id="FM解法"><a href="#FM解法" class="headerlink" title="FM解法"></a>FM解法</h2><p>定理：对于一个正定矩阵W，始终存在一个矩阵使得<br><img src="/img/media/FM%20FFM/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-26%20%E4%B8%8A%E5%8D%8810.00.38.png" alt=""><br>成立（需要V的维数k足够大）<br>但是在巨大稀疏矩阵的情况下，当k并不是很大时也可以很接近W，因此可以用<br><img src="/img/media/FM%20FFM/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-26%20%E4%B8%8A%E5%8D%8810.01.54.png" alt=""><br>其中这里v为长度k的一个向量,⟨vi,vj⟩表示两个向量的点积，在FM中也称为<strong>隐向量</strong>,这样就有了FM的式子:<br><img src="/img/media/FM%20FFM/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-26%20%E4%B8%8A%E5%8D%8810.02.27.png" alt=""><br>其中&lt;&gt;表示两个向量的点积<br><img src="/img/media/FM%20FFM/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-26%20%E4%B8%8A%E5%8D%8810.05.42.png" alt=""><br>直观上看，FM的复杂度是 O(kn2)。但是，通过下列等式，FM的二次项可以化简，其复杂度可以优化到 O(kn)。由此可见，FM可以在线性时间对新样本作出预测。<br><img src="/img/media/FM%20FFM/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-26%20%E4%B8%8A%E5%8D%8810.08.06.png" alt=""><br>以下是详细证明过程。<br><img src="/img/media/FM%20FFM/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-26%20%E4%B8%8A%E5%8D%8810.09.13.png" alt=""><br>之后采用随机梯度下降SGD（Stochastic Gradient Descent）训练模型参数。那么，模型各个参数的梯度如下:<br><img src="/img/media/FM%20FFM/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-26%20%E4%B8%8A%E5%8D%8810.11.53.png" alt=""><br><img src="/img/media/FM%20FFM/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-26%20%E4%B8%8A%E5%8D%8810.19.33.png" alt=""></p>
<h2 id="FM总结"><a href="#FM总结" class="headerlink" title="FM总结"></a>FM总结</h2><p>首先是为什么使用向量的点积可以解决以上问题呢？</p>
<ol>
<li>参数的数量大幅度缩减，从n×(n−1)/2降低到nk</li>
<li>隐向量的点积可以表示原本两个毫无相关的参数之间的关系</li>
<li>可以解决稀疏向量问题，因为每个特征都有一个隐向量，就算是稀疏向量在训练样本没有出现过的组合在预测时也可以进行计算。</li>
</ol>
<h2 id="扩展"><a href="#扩展" class="headerlink" title="扩展"></a>扩展</h2><p>FM与矩阵分解MF与SVM有什么差别呢？</p>
<ol>
<li>FM是一种比较灵活的模型，通过合适的特征变换方式，FM可以模拟二阶多项式核的SVM模型、MF模型、SVD++模型等。</li>
<li>相比SVM的二阶多项式核而言，FM在样本稀疏的情况下是有优势的；而且，FM的训练/预测复杂度是线性的，而二项多项式核SVM需要计算核矩阵，核矩阵复杂度就是N平方。</li>
<li>相比MF而言，我们把MF中每一项的rating分改写为rui∼βu+γi+xTuyi，从此公式中可以看出，这相当于只有两类特征 β 和 γ 的FM模型。对于FM而言，我们可以加任意多的特征，比如user的历史购买平均值，item的历史购买平均值等，但是MF只能局限在两类特征。SVD++与MF类似，在特征的扩展性上都不如FM。</li>
</ol>
<h1 id="FFM"><a href="#FFM" class="headerlink" title="FFM"></a>FFM</h1><p>Field-aware Factorization Machine(FFM) 场感知分解机。</p>
<p>场感知说白了可以理解为分类。通过引入field的概念，FFM把相同性质的特征归于同一个field。比如， “MovieClass = romantic”、“MovieClass = action”这2个特征值都是代表电影分类的，可以放到同一个field中。简单来说，同一个类别的特征经过One-Hot编码生成的数值特征都可以放到同一个field。</p>
<p>在FFM中，每一维特征 xi，针对其它特征的每一种field fj，都会学习一个隐向量<br><img src="/img/media/FM%20FFM/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-26%20%E4%B8%8B%E5%8D%881.23.02.png" alt=""><br>因此，隐向量不仅与特征相关，也与field相关。也就是说，“MovieClass”这个特征与“UserRate”特征和“PlayTimes”特征进行关联的时候使用不同的隐向量，也是FFM中“field-aware”的由来。<br>通过修改FM的公式，我们可以得出：<br><img src="/img/media/FM%20FFM/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-26%20%E4%B8%8B%E5%8D%881.26.49.png" alt=""><br><img src="/img/media/FM%20FFM/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-26%20%E4%B8%8B%E5%8D%881.28.19.png" alt=""></p>
<h2 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h2><p>FFM其实是在FM的基础上做了一些更加细致化的工作:作者Yuchin认为相同性质的特征归于同一field，而当前特征在不同field上的表现应该是不同的.</p>
<p>比如在广告领域中性别对于广告商(Advertiser)和投放地(Publisher)的作用就是不一样的，比如:<br><img src="/img/media/FM%20FFM/ffm_case.png" alt=""><br>这里的特征被分为了三类，有投放地(Publisher)，广告商(Advertiser)和性别(Gender),如果使用<strong>FM</strong>来预估这个点击率则是:<br><img src="/img/media/FM%20FFM/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-26%20%E4%B8%8B%E5%8D%881.46.37.png" alt=""><br>这里可以看出FM中隐向量对于不同类别的特征进行组合时都是使用同一个向量，而基于Field-aware的FFM就是对这点进行修改，认为当前向量对于每一个类别都有一个不同的隐向量，比如性别和投放地进行组合的时候使用的隐向量为vMale,G,这样推广开来之后这个问题中FFM的二阶项就可以表述为:<br><img src="/img/media/FM%20FFM/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-26%20%E4%B8%8B%E5%8D%881.47.35.png" alt=""><br>这样,FFM使用通用化的学习公式表达了之后为:<br><img src="/img/media/FM%20FFM/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-26%20%E4%B8%8B%E5%8D%881.48.53.png" alt=""><br>因为FFM的参数空间为nfk,其计算复杂度为O(nk),但是FFM都是在特定的field的中来学习训练隐向量的，所以一般来说:<br><img src="/img/media/FM%20FFM/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-26%20%E4%B8%8B%E5%8D%881.49.50.png" alt=""><br>FFM的改进看上去还是有挺有道理的，但是其实最终实验做出来和FM的效果不相上下。<br><img src="/img/media/FM%20FFM/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-26%20%E4%B8%8B%E5%8D%881.50.29.png" alt=""></p>
<h1 id="美团机器学习实践"><a href="#美团机器学习实践" class="headerlink" title="美团机器学习实践"></a>美团机器学习实践</h1><p>逻辑回归无法学习到特征间的组合关系，而特征组合关系在推荐和CTR预估中却是比较常见的。在进行点击率预估时，特征通常来自于用户，广告和上下文环境，如果没有对这些特征进行组合，模型就无法学习到所有有用的信息。例如，同一个用户在不同时间或者地点感兴趣的广告是不同的，同一件商品在不同地区的受欢迎程度也是不同的。但是人工对特征组合需要做大量的特征工程工作，对特征做暴力组合模型又太复杂，参数太多。模型训练迭代无论是内存开销还是时间开销都让人很难接受，迭代效果往往也比较差。所以可以用因子分解机和场感知因子分解机来进行自动做特征组合，并且算法效率比较高。 </p>
<p>利用模型来做特征组合，很容易想到使用支持向量机的核函数来实现特征之间的交叉。但是多项式核函数的问题就是二次参数过多。设特征维数为n，则二次项的参数数目为n(n+1) / 2，特别是某些广告ID，用户ID类特征，其特征维数可能达到几百万维，这导致只有极少数的二阶组合模式才能找到，所以这些特征组合后得到的特征矩阵就是十分稀疏。而在训练样本不足的时候，特征矩阵的稀疏性很容易导致相关参数准确性较低，导致模型效果不好。而我们可以通过对二次项参数施加某种限制来减少参数的自由度。</p>
<p>因子分解机施加的限制就是要求二次项参数矩阵是低秩的，能够分解为低秩矩阵的乘积。所有二次项参数矩阵W就可以分解为<br><img src="/img/media/FM%20FFM/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-26%20%E4%B8%8A%E5%8D%8810.00.38%202.png" alt=""><br><strong>V</strong>的第j列便是第j维特征向量。<br>因子分解机的模型方程：<br><img src="/img/media/FM%20FFM/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-26%20%E4%B8%8B%E5%8D%881.26.49%202.png" alt=""><br>在多项式模型中，Whi和Wij是相互独立的，但参数因子化使得XhXi和XiXj的系数分别为&lt;Vh,Vi&gt;和&lt;Vi,Vj&gt;，它们有了共同项Vi。也就是说<strong>所有包含Xj的非零组合特征的样本都可以用来学习隐向量Vi</strong>，这在<strong>很大程度上避免了数据稀疏性造成的影响</strong>。</p>
<p>FM可以看做是FFM的特殊情况，是把所有特征都归属到一个场时的场感知因子分解机模型。</p>
<h2 id="FFM的应用"><a href="#FFM的应用" class="headerlink" title="FFM的应用"></a>FFM的应用</h2><p>FFM可以自动做特征组合和处理高维稀疏特征，因而它在处理大量离散特征的问题上往往有比较好的效果。使用场感知因子分解机时要注意对连续特征做归一化或离散化。</p>
<p>FM，FFM与其他模型的对比关系。</p>
<h3 id="FM与FFM"><a href="#FM与FFM" class="headerlink" title="FM与FFM"></a>FM与FFM</h3><p>场感知因子分解机对因子分解机模型引入场的概念，增加了模型复杂度和模型表达能力。可以将因子分解机理解为场感知因子分解机的特殊简化模式，即所有特征都属于同一个场。</p>
<h3 id="FM与神经网络"><a href="#FM与神经网络" class="headerlink" title="FM与神经网络"></a>FM与神经网络</h3><p>神经网络难以直接处理高维稀疏的离散特征，因为这导致神经元的连接参数太多。而因子分解机可以看做对高维稀疏的离散特征做嵌入（Embedding）。</p>
<h3 id="FM和梯度提升树"><a href="#FM和梯度提升树" class="headerlink" title="FM和梯度提升树"></a>FM和梯度提升树</h3><p>因子分解机与梯度提升树都可以做特征组合，Facebook就基于梯度提升树学习过特征的组合，梯度提升树可以方便对特征做高阶组合。当数据不是高度稀疏时，梯度提升树可以有效地学习到比较复杂的特征组合；但是在高度稀疏的数据中，特征二阶组合的数量就足以让绝大多数模式找不到样本，因而梯度提升树无法学习到这种高阶组合。</p>
<h3 id="因子分解机与其他模型"><a href="#因子分解机与其他模型" class="headerlink" title="因子分解机与其他模型"></a>因子分解机与其他模型</h3><p>因子分解机是一种比较灵活的模型，通过合适的特征变换方式，因子分解机可以模拟二阶多项式核的支持向量机模型、MF模型、SVD++模型等。但SVD++与MF在特征的扩展性上都不如因子分解机，而支持向量机核函数计算复杂度较高。</p>

          
        
      
    </div>

    

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://github.com/zdkswd/2019/02/25/百面机器学习  逻辑回归/"/>

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="ZDK"/>
      <meta itemprop="description" content=""/>
      <meta itemprop="image" content="/images/avatar.gif"/>
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ZDK's blog"/>
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/02/25/百面机器学习  逻辑回归/" class="post-title-link" itemprop="url">百面机器学习 美团 逻辑回归</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-02-25 16:31:47" itemprop="dateCreated datePublished" datetime="2019-02-25T16:31:47+08:00">2019-02-25</time>
            </span>
          

          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-02-26 21:51:38" itemprop="dateModified" datetime="2019-02-26T21:51:38+08:00">2019-02-26</time>
              </span>
            
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/知识总结/" itemprop="url" rel="index"><span itemprop="name">知识总结</span></a></span>

                
                
              
            </span>
          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">评论数：</span>
                <a href="/2019/02/25/百面机器学习  逻辑回归/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2019/02/25/百面机器学习  逻辑回归/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
            <span id="/2019/02/25/百面机器学习  逻辑回归/" class="post-meta-item leancloud_visitors" data-flag-title="百面机器学习 美团 逻辑回归">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              
                <span class="post-meta-item-text">阅读次数：</span>
              
                <span class="leancloud-visitors-count"></span>
            </span>
          

          

          

          <br/>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="百面机器学习-逻辑回归"><a href="#百面机器学习-逻辑回归" class="headerlink" title="百面机器学习  逻辑回归"></a>百面机器学习  逻辑回归</h1><p>逻辑回归可以说是机器学习领域最基础也是最常用的模型，逻辑回归的原理推导以及扩展应用几乎是算法工程师的必备技能。</p>
<h2 id="问题1"><a href="#问题1" class="headerlink" title="问题1"></a>问题1</h2><p>问：逻辑回归相比于线性回归，有何异同？</p>
<p>答：逻辑回归，乍一听名字和数学中的线性回归异派同源，但本质却是大相径庭。</p>
<p>首先，逻辑回归处理的是分类问题，线性回归处理的是回归问题，这是这两者最本质的区别。逻辑回归中，因变量取值是一个二元分布，模型学习得出的是E[y|x;θ]，即给定自变量和超参数后，得到因变量的期望，并基于此期望来处理预测分类问题。</p>
<p>分类和回归是如今机器学习中两个不同的任务，而属于分类算法的逻辑回归，其命名有一定的历史原因。</p>
<p>将逻辑回归的公式进行整理，我们可以得到<br><img src="/img/media/%E7%99%BE%E9%9D%A2%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%20%20%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/WechatIMG4.jpeg" alt=""><br>其中p=P(y=1|x)，也就是将给定输入x预测为正样本的概率。如果把一个事件的几率定义为该事件发生的概率与该事件不发生的概率的比值 p / 1-p，那么逻辑回归可以看做是对于“y=1|x”这一事件的对数几率的线性回归，于是“逻辑回归”这一称谓也就延续下来了。</p>
<p>在关于逻辑回归的讨论中，我们均认为y是因变量，而非p / 1-p ，这便引出逻辑回归与线性回归<strong>最大的区别</strong>。即逻辑回归中的因变量为离散的，而线性回归中的因变量是连续的。并且在自变量x与超参数θ确定的情况下，逻辑回归可以看作广义线性模型在因变量y服从二元分布时的一个特殊情况；而使用最小二乘法求解线性回归时，我们认为因变量y服从正态分布。</p>
<p><strong>相同之处</strong>：首先二者都使用了极大似然估计来对训练样本进行建模。线性回归使用最小二乘法，实际上就是在自变量x与超参数θ确定，因变量y服从正态分布的假设下，使用极大似然估计的一个化简；而逻辑回归中通过对似然函数的学习，得到最佳参数θ。另外，二者在求解超参数过程中，都可以使用梯度下降的方法，这也是监督学习中一个常见的相似之处。</p>
<h2 id="问题2"><a href="#问题2" class="headerlink" title="问题2"></a>问题2</h2><p>问：当使用逻辑回归处理多标签的分类问题时，有哪些常见做法，分别应用于哪些场景，它们之间又有着怎样的关系？</p>
<p>答：使用哪一种方法来处理多分类的问题取决于具体问题的定义。首先，如果一个样本只对应于一个标签，我们可以假设每个样本属于不同标签的概率服从于几何分布，使用多项逻辑回归（Softmax Regression）来进行分类。多项逻辑回归实际上是二分类逻辑回归在多标签分类下的一种扩展。</p>
<p>当存在样本可能属于多个标签的情况时，我们可以训练k个二分类的逻辑回归分类器。第i个分类器用以区分每个样本是否可以归为第i类，训练该分类器时，需要把标签重新整理为“第i类标签”与“非第i类标签”两类。通过这样的办法，我们就解决了每个样本可能拥有多个标签的情况。</p>
<h1 id="美团机器学习实践"><a href="#美团机器学习实践" class="headerlink" title="美团机器学习实践"></a>美团机器学习实践</h1><p>逻辑回归是一种广义线性模型，它与线性回归模型包含的线性函数十分相似。但逻辑回归通过对数概率函数将线性函数的结果进行映射，目标函数的取值空间从（-∞，+∞）映射到（0，1）。从而可以处理分类问题。逻辑回归虽然有“回归”二字，却是统计学习中经典分类方法。</p>
<h2 id="逻辑回归原理"><a href="#逻辑回归原理" class="headerlink" title="逻辑回归原理"></a>逻辑回归原理</h2><p>将线性回归与逻辑回归进行对比，可以发现线性回归模型在训练时在整个实数域上对异常点的敏感性是一致的，因而在处理分类问题时线性回归模型效果较差，线性回归模型不适合处理分类问题。对于二分类任务，逻辑回归输出标记y∈{0,1}，而线性回归模型产生的预测值是实数，所以需要一个映射函数将实值转换为0 / 1值。</p>
<p>最理想的映射函数是单位阶跃函数，即预测值大于零就判为正例，预测值小于零则判为负例，预测值为临界值则可任意判别。虽然单位阶跃函数看似完美解决了这个问题，但是单位阶跃函数不连续并且不充分光滑，因而无法进行求解。</p>
<p>所以我们希望找到一个近似函数来替代单位阶跃函数，并希望它单调可微。对数概率函数正是这样一个替代的函数，对数概率函数将θ^Tx（<strong>θTx是多项式</strong>）的值转化为接近0或1的值，并且其输出=0处变化很陡，将对数概率函数代入，就能得到逻辑回归的表达式。</p>
<p><strong>在进行分类的过程实际上是使用多项式来进行“围数据”，逻辑回归相对于在多项式上加了一层函数，输出0，1。</strong></p>
<p>为了提高算法收敛速度和节省内存，实际应用在迭代求解时往往会使用高效的优化算法，如LBFGS，信赖域算法，但这些求解方法是基于批量处理的，批处理算法无法高效处理超大规模的数据集，也无法对线上模型进行快速实时更新。</p>
<p>随机梯度下降是相对于批处理的另外一种优化方式，它每次只用一个样本来更新模型的权重，这样就可以更快地进行模型迭代。对于广告和新闻推荐这种数据和样本更新比较频繁场景，快速的模型更新能够更早捕捉到新数据的规律进而提升业务指标。谷歌FTRL就是基于随机梯度下降的一种逻辑回归优化算法。是一种在线学习算法。</p>
<h2 id="逻辑回归应用"><a href="#逻辑回归应用" class="headerlink" title="逻辑回归应用"></a>逻辑回归应用</h2><p>逻辑回归常用于疾病自动诊断，经济预测，点击率预测等领域。由于其处理速度快且容易并行，逻辑回归适合用来学习需要大规模训练的样本和特征，对于广告十亿量级的特征和亿量级的特征来说，逻辑回归有着天然的优势，因而逻辑回归在工业界获得了广泛的应用。而逻辑回归的缺点是需要大量的特征组合和离散工作来增加特征的表达性，模型表达能力弱，比较容易欠拟合。</p>
<p>业界对逻辑回归的研究热点主要集中在稀疏性，准确性和大规模计算上。实际应用逻辑回归前，经常会对特征进行独热（One Hot）编码，比如广告点击率应用中的用户ID，广告ID。为了实现计算效率和性能优化，逻辑回归求解有很多种优化方法，比如BFGS，LBFGS，共轭梯度法，信赖域法，其中前两个是牛顿法的变种，LBFGS算法是BFGS算法在受限内存下的近似优化。针对逻辑回归在线学习中遇到的稀疏性和准确性问题，谷歌和伯克利分校提出来稀疏性比较好的FOBOS算法，微软提出了RDA算法。谷歌综合了精度比较好的RDA和稀疏性比较好的FTRL，但在L1范数或者非光滑的正则项下，FTRL的效果会更好。</p>
<p>在实际应用中，逻辑回归也需要注意正则化的问题。L1正则（也称LASSO）假设模型参数取值满足拉普拉斯分布，L2正则（也称RIDGE）假设模型参数取值满足高斯分布。</p>

          
        
      
    </div>

    

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/8/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/8/">8</a><span class="page-number current">9</span><a class="page-number" href="/page/10/">10</a><span class="space">&hellip;</span><a class="page-number" href="/page/28/">28</a><a class="extend next" rel="next" href="/page/10/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <div class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/avatar.gif"
                alt="ZDK"/>
            
              <p class="site-author-name" itemprop="name">ZDK</p>
              <div class="site-description motion-element" itemprop="description"></div>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">191</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">9</span>
                    <span class="site-state-item-name">分类</span>
                  
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">48</span>
                    <span class="site-state-item-name">标签</span>
                  
                </div>
              
            </nav>
          

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="https://github.com/zdkswd" title="GitHub &rarr; https://github.com/zdkswd"><i class="fa fa-fw fa-github"></i>GitHub</a>
                </span>
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="mailto:2822464407@qq.com" title="E-Mail &rarr; mailto:2822464407@qq.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                </span>
              
            </div>
          

          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-block">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                Links
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="https://github.com/zdkswd" title="https://github.com/zdkswd">Title</a>
                  </li>
                
              </ul>
            </div>
          

          
        </div>
      </div>

      

      
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          
        </div>
      

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">ZDK</span>

  

  
</div>


  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v3.7.1</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v7.2.0</div>




        








        
      </div>
    </footer>

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>










  
  













  
  <script src="/lib/jquery/index.js?v=3.4.1"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>




  <script src="/js/utils.js?v=7.2.0"></script>

  <script src="/js/motion.js?v=7.2.0"></script>



  
  


  <script src="/js/affix.js?v=7.2.0"></script>

  <script src="/js/schemes/pisces.js?v=7.2.0"></script>




  

  <script src="/js/next-boot.js?v=7.2.0"></script>

  

  

  

  

  
  

<script src="//unpkg.com/valine/dist/Valine.min.js"></script>

<script>
  var GUEST = ['nick', 'mail', 'link'];
  var guest = 'nick,mail,link';
  guest = guest.split(',').filter(function(item) {
    return GUEST.indexOf(item) > -1;
  });
  new Valine({
    el: '#comments',
    verify: false,
    notify: true,
    appId: 'QiU7UFIdgTTauFTk89N47mQS-gzGzoHsz',
    appKey: 'gkBx5soQkBREmER84PWbNJeM',
    placeholder: 'have fun',
    avatar: 'mm',
    meta: guest,
    pageSize: '10' || 10,
    visitor: true,
    lang: '' || 'zh-cn'
  });
</script>





  
  <script>
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url).replace(/\/{2,}/g, '/');
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('5');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x"></i></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x"></i></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  

  

  

  

  

  

  

  

  

  


  

</body>
</html>
