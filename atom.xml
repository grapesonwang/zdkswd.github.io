<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>ZDK&#39;s blog</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://github.com/zdkswd/"/>
  <updated>2019-04-16T02:43:13.399Z</updated>
  <id>https://github.com/zdkswd/</id>
  
  <author>
    <name>ZDK</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>转 逻辑回归LR的特征为什么要先离散化</title>
    <link href="https://github.com/zdkswd/2019/04/16/%E8%BD%AC%20%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92LR%E7%9A%84%E7%89%B9%E5%BE%81%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E5%85%88%E7%A6%BB%E6%95%A3%E5%8C%96/"/>
    <id>https://github.com/zdkswd/2019/04/16/转 逻辑回归LR的特征为什么要先离散化/</id>
    <published>2019-04-16T02:42:56.000Z</published>
    <updated>2019-04-16T02:43:13.399Z</updated>
    
    <content type="html"><![CDATA[<h1 id="转-逻辑回归LR的特征为什么要先离散化"><a href="#转-逻辑回归LR的特征为什么要先离散化" class="headerlink" title="转 逻辑回归LR的特征为什么要先离散化"></a>转 逻辑回归LR的特征为什么要先离散化</h1><p><a href="https://blog.csdn.net/yang090510118/article/details/39478033" target="_blank" rel="noopener">逻辑回归LR的特征为什么要先离散化 - yang090510118的专栏 - CSDN博客</a></p><p>在工业界，很少直接将连续值作为特征喂给逻辑回归模型，而是将连续特征离散化为一系列0、1特征交给逻辑回归模型，这样做的优势有以下几点：</p><ol><li>稀疏向量内积乘法运算速度快，计算结果方便存储，容易scalable（扩展）。</li><li>离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄&gt;30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰。</li><li>逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合。</li><li>离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力。</li><li>特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问。</li></ol><p>模型是使用离散特征还是连续特征，其实是一个“海量离散特征+简单模型” 同 “少量连续特征+复杂模型”的权衡。既可以离散化用线性模型，也可以用连续特征加深度学习。就看是喜欢折腾特征还是折腾模型了。通常来说，前者容易，而且可以n个人一起并行做，有成功经验；后者目前看很赞，能走多远还须拭目以待。</p><p>大概的理解：</p><p>1）计算简单<br>2）简化模型<br>3）增强模型的泛化能力，不易受噪声的影响</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;转-逻辑回归LR的特征为什么要先离散化&quot;&gt;&lt;a href=&quot;#转-逻辑回归LR的特征为什么要先离散化&quot; class=&quot;headerlink&quot; title=&quot;转 逻辑回归LR的特征为什么要先离散化&quot;&gt;&lt;/a&gt;转 逻辑回归LR的特征为什么要先离散化&lt;/h1&gt;&lt;p&gt;&lt;a
      
    
    </summary>
    
      <category term="知识总结" scheme="https://github.com/zdkswd/categories/%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/"/>
    
    
      <category term="机器学习" scheme="https://github.com/zdkswd/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>featexp</title>
    <link href="https://github.com/zdkswd/2019/04/14/featexp/"/>
    <id>https://github.com/zdkswd/2019/04/14/featexp/</id>
    <published>2019-04-14T12:15:32.000Z</published>
    <updated>2019-04-14T12:15:42.157Z</updated>
    
    <content type="html"><![CDATA[<h1 id="featexp"><a href="#featexp" class="headerlink" title="featexp"></a>featexp</h1><p><a href="https://towardsdatascience.com/my-secret-sauce-to-be-in-top-2-of-a-kaggle-competition-57cff0677d3c" target="_blank" rel="noopener">https://towardsdatascience.com/my-secret-sauce-to-be-in-top-2-of-a-kaggle-competition-57cff0677d3c</a></p><h1 id="feature-understanding"><a href="#feature-understanding" class="headerlink" title="feature understanding"></a>feature understanding</h1><p>if target is binary, scatter is not very useful.<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/featexp/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-14%20%E4%B8%8B%E5%8D%885.41.15.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>And for continuous target, too many data points make it difficult to understand the target vs. feature trend.</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/featexp/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-14%20%E4%B8%8B%E5%8D%885.52.05.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>use above code,Featexp creates <strong>equal population bins (X-axis)</strong> of a numeric feature.It then calculates target’s <strong>mean</strong> in each bin and plots it in the left-hand side plot above. As you can see the plot on the right shows they are the same number.<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/featexp/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-14%20%E4%B8%8B%E5%8D%885.54.45.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><h1 id="Identifying-noisy-features"><a href="#Identifying-noisy-features" class="headerlink" title="Identifying noisy features"></a>Identifying noisy features</h1><p>Noisy features lead to overfitting and identifying them isn’t easy. In featexp, you can pass a test set and compare feature trends in train|test to identify noisy ones. This test set is not the actual test set. Its your local test set|validation set for which you know target.</p><blockquote><p>get_univariate_plots(data=data_train, target_col=’target’, data_test=data_test, features_list=[‘DAYS_EMPLOYED’])  </p></blockquote><p><img src="/img/media/featexp/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-14%20%E4%B8%8B%E5%8D%886.10.12.png" alt=""><br>Featexp calculates two metrics to display on these plots which help with gauging(计量；测量) noisiness:</p><p>1.<strong>Trend correlation</strong> (seen in test plot): If a feature doesn’t hold same trend w.r.t. target across train and evaluation sets, it can lead to overfitting. This happens because the model is learning something which is not applicable in test data. Trend correlation helps understand how similar train/test trends are and mean target values for bins in train &amp; test are used to calculate it. Feature above has 99% correlation. Doesn’t seem noisy!<br>2.<strong>Trend changes</strong>: Sudden and repeated changes in trend direction could imply noisiness. But, such trend change can also happen because that bin has a very different value in terms of <strong>other features</strong> and hence, its value can’t really be compared with other bins.</p><p>for example the nosiy feature.<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/featexp/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-14%20%E4%B8%8B%E5%8D%886.37.26.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><p>Dropping low trend-correlation features works well when <strong>there are a lot of features and they are correlated with each other</strong>. It leads to less overfitting and other correlated features avoid information loss. It’s also important to <strong>not drop too many important features</strong> as it might lead to a drop in performance. Also, <strong>you can’t identify these noisy features using feature importance</strong> because they could be fairly important and still be very noisy!</p><p><strong>Using test data from a different time period works better because then you would be making sure if feature trend holds over time.</strong></p><p><strong>get_trend_stats()</strong> function in featexp returns a dataframe with trend correlation and changes for each feature.</p><blockquote><p>from featexp import get_trend_stats  stats=get_trend_stats(data=data_train,target_col=’target’,data_test=data_test)  </p></blockquote><p><img src="/img/media/featexp/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-14%20%E4%B8%8B%E5%8D%886.54.44.png" alt=""><br> try dropping features with low trend-correlation in our data and see how results improve.<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/featexp/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-14%20%E4%B8%8B%E5%8D%886.56.50.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>We can see that higher the trend-correlation threshold to drop features, higher is the leaderboard (LB) AUC.</p><h1 id="Feature-Engineering"><a href="#Feature-Engineering" class="headerlink" title="Feature Engineering"></a>Feature Engineering</h1><p>The insights that you get by looking at these plots help with creating better features. Just having a better understanding of data can lead to better feature engineering. But, in addition to this, it can also help you in improving the existing features. Let’s look at another feature EXT_SOURCE_1:<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/featexp/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-14%20%E4%B8%8B%E5%8D%887.10.11.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><h1 id="Feature-importance"><a href="#Feature-importance" class="headerlink" title="Feature importance"></a>Feature importance</h1><p>i choose xgboost this part.</p><h1 id="Feature-debugging"><a href="#Feature-debugging" class="headerlink" title="Feature debugging"></a>Feature debugging</h1><p>check the trend is or not as you wish.</p><h1 id="Leakage-Detection"><a href="#Leakage-Detection" class="headerlink" title="Leakage Detection"></a>Leakage Detection</h1><h1 id="Model-Monitoring"><a href="#Model-Monitoring" class="headerlink" title="Model Monitoring"></a>Model Monitoring</h1>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;featexp&quot;&gt;&lt;a href=&quot;#featexp&quot; class=&quot;headerlink&quot; title=&quot;featexp&quot;&gt;&lt;/a&gt;featexp&lt;/h1&gt;&lt;p&gt;&lt;a href=&quot;https://towardsdatascience.com/my-secret-
      
    
    </summary>
    
      <category term="知识总结" scheme="https://github.com/zdkswd/categories/%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/"/>
    
    
      <category term="机器学习" scheme="https://github.com/zdkswd/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>美团  特征提取</title>
    <link href="https://github.com/zdkswd/2019/04/14/%E7%BE%8E%E5%9B%A2%20%20%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96/"/>
    <id>https://github.com/zdkswd/2019/04/14/美团  特征提取/</id>
    <published>2019-04-14T08:33:47.000Z</published>
    <updated>2019-04-14T08:33:37.891Z</updated>
    
    <content type="html"><![CDATA[<h1 id="美团-特征提取"><a href="#美团-特征提取" class="headerlink" title="美团  特征提取"></a>美团  特征提取</h1><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E7%BE%8E%E5%9B%A2%20%20%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96/WechatIMG77.jpeg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E7%BE%8E%E5%9B%A2%20%20%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96/WechatIMG76.jpeg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;美团-特征提取&quot;&gt;&lt;a href=&quot;#美团-特征提取&quot; class=&quot;headerlink&quot; title=&quot;美团  特征提取&quot;&gt;&lt;/a&gt;美团  特征提取&lt;/h1&gt;&lt;figure class=&quot;image-bubble&quot;&gt;
                &lt;div 
      
    
    </summary>
    
      <category term="知识总结" scheme="https://github.com/zdkswd/categories/%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/"/>
    
    
      <category term="机器学习" scheme="https://github.com/zdkswd/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Kaggle竞赛案例一</title>
    <link href="https://github.com/zdkswd/2019/04/14/Kaggle%E7%AB%9E%E8%B5%9B%E6%A1%88%E4%BE%8B%E4%B8%80/"/>
    <id>https://github.com/zdkswd/2019/04/14/Kaggle竞赛案例一/</id>
    <published>2019-04-14T07:37:47.000Z</published>
    <updated>2019-04-14T07:44:25.371Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Kaggle-CrowdFlower"><a href="#Kaggle-CrowdFlower" class="headerlink" title="Kaggle_CrowdFlower"></a>Kaggle_CrowdFlower</h1><p><a href="https://github.com/ChenglongChen/Kaggle_CrowdFlower">GitHub - ChenglongChen/Kaggle_CrowdFlower: 1st Place Solution for Search Results Relevance Competition on Kaggle (https://www.kaggle.com/c/crowdflower-search-relevance)</a><br>1st Place Solution for Search Results Relevance Competition on Kaggle<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/Kaggle%E7%AB%9E%E8%B5%9B%E6%A1%88%E4%BE%8B%E4%B8%80/FlowChart.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>问题描述：搜索结果相关挑战，给定搜索结果，搜索出的产品名称，产品描述，建立模型去预测搜索结果的相关得分。</p><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>解决方案分为两部分：特征工程和模型集成。</p><p>特征包括三部分的特征：<br>1.计数特征<br>2.距离特征<br>3.TF-IDF特征</p><p>在生产特征前，对数据进行拼写检查，同义词替换，词干提取是非常有用的。模型集成包括两个主要的步骤，首先，使用不同种，不同参数设置，不同特征子集去训练模型。然后使用训练的模型进行bagged集成选择。在训练集上使用交叉验证来评估表现。</p><h2 id="预处理"><a href="#预处理" class="headerlink" title="预处理"></a>预处理</h2><p>进行了几步去清洗文本。</p><h2 id="去除HTML标签"><a href="#去除HTML标签" class="headerlink" title="去除HTML标签"></a>去除HTML标签</h2><p>在商品描述中存在html标签的干扰，使用bs4去除之。</p><h2 id="单词替换"><a href="#单词替换" class="headerlink" title="单词替换"></a>单词替换</h2><p>在搜索中会出现词义相关的搜索，要考虑到。<br>1.拼写纠正<br>2.同义词替换<br>3.词干提取</p><h2 id="特征提取-选择"><a href="#特征提取-选择" class="headerlink" title="特征提取/选择"></a>特征提取/选择</h2><p>$$\left(q_{i}, t_{i}, d_{i}\right)$$是train.csv以及test.csv中的第i个样本，qi是查询，ti是产品名，di是产品描述。使用ri和vi来表示<strong>median_relevance</strong>和<strong>relevance_variance</strong>。使用函数ngram(s,n)去提取句子中的n个词。例如<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/Kaggle%E7%AB%9E%E8%B5%9B%E6%A1%88%E4%BE%8B%E4%B8%80/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-12%20%E4%B8%8B%E5%8D%883.09.02.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><h3 id="计数特征"><a href="#计数特征" class="headerlink" title="计数特征"></a>计数特征</h3><p>为$$\left{q_{i}, t_{i}, d_{i}\right}$$生成计数特征。</p><h4 id="基础计数特征"><a href="#基础计数特征" class="headerlink" title="基础计数特征"></a>基础计数特征</h4><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/Kaggle%E7%AB%9E%E8%B5%9B%E6%A1%88%E4%BE%8B%E4%B8%80/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-12%20%E4%B8%8B%E5%8D%883.17.52.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h4 id="交叉计数特征"><a href="#交叉计数特征" class="headerlink" title="交叉计数特征"></a>交叉计数特征</h4><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/Kaggle%E7%AB%9E%E8%B5%9B%E6%A1%88%E4%BE%8B%E4%B8%80/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-12%20%E4%B8%8B%E5%8D%883.20.07.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h4 id="交叉位置特征"><a href="#交叉位置特征" class="headerlink" title="交叉位置特征"></a>交叉位置特征</h4><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/Kaggle%E7%AB%9E%E8%B5%9B%E6%A1%88%E4%BE%8B%E4%B8%80/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-12%20%E4%B8%8B%E5%8D%883.23.08.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h3 id="距离特征"><a href="#距离特征" class="headerlink" title="距离特征"></a>距离特征</h3><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/Kaggle%E7%AB%9E%E8%B5%9B%E6%A1%88%E4%BE%8B%E4%B8%80/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-12%20%E4%B8%8B%E5%8D%883.24.35.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h3 id="TF-IDF特征"><a href="#TF-IDF特征" class="headerlink" title="TF-IDF特征"></a>TF-IDF特征</h3><h3 id="其他特征"><a href="#其他特征" class="headerlink" title="其他特征"></a>其他特征</h3><h4 id="查询ID"><a href="#查询ID" class="headerlink" title="查询ID"></a>查询ID</h4><p>将查询id进行独热编码。</p><h3 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h3><p>相同的模型经常被用来在特征集上进行交叉验证来测试与之前的特征集合相比是否得分有所提升。对于高维特征，使用XGBoost with linear booster(MSE为目标函数)，对于低维特征使用sklearn中的ExtraTreesRegressor。</p><p>值得注意的是，有了集成选择(<strong>ensemble selection</strong>)，我们可以利用不同的特征集合来训练特征库，并且利用集成选择去挑选出最佳的集成。但是特征选择依旧有用。使用上述的特征选择，可以首先明确一些表现好的特征集合，然后使用其去训练模型，这会在一定程度上减少计算负担。</p><h2 id="模型技术和训练"><a href="#模型技术和训练" class="headerlink" title="模型技术和训练"></a>模型技术和训练</h2><h3 id="交叉验证方法学"><a href="#交叉验证方法学" class="headerlink" title="交叉验证方法学"></a>交叉验证方法学</h3><h4 id="划分"><a href="#划分" class="headerlink" title="划分"></a>划分</h4><p>StratifiedKFold</p><h1 id="Kaggle-HomeDepot"><a href="#Kaggle-HomeDepot" class="headerlink" title="Kaggle_HomeDepot"></a>Kaggle_HomeDepot</h1><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/Kaggle%E7%AB%9E%E8%B5%9B%E6%A1%88%E4%BE%8B%E4%B8%80/FlowChart%202.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h1 id="Kaggle——销售量预测"><a href="#Kaggle——销售量预测" class="headerlink" title="Kaggle——销售量预测"></a>Kaggle——销售量预测</h1><p>比赛地址<a href="https://www.kaggle.com/c/competitive-data-science-predict-future-sales/data" target="_blank" rel="noopener">Predict Future Sales | Kaggle</a><br>这个比赛作为经典的时间序列问题之一，目标是为了预测下个月每种产品和商店的总销售额。</p><p>以下为<strong>1st solution</strong>。</p><h2 id="part1-hands-on-data"><a href="#part1-hands-on-data" class="headerlink" title="part1 hands on data"></a>part1 hands on data</h2><p><a href="https://www.kaggle.com/kyakovlev/1st-place-solution-part-1-hands-on-data/notebook" target="_blank" rel="noopener">https://www.kaggle.com/kyakovlev/1st-place-solution-part-1-hands-on-data/notebook</a></p><h3 id="数据域含义"><a href="#数据域含义" class="headerlink" title="数据域含义"></a>数据域含义</h3><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/Kaggle%E7%AB%9E%E8%B5%9B%E6%A1%88%E4%BE%8B%E4%B8%80/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-12%20%E4%B8%8B%E5%8D%888.55.03.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>数据集情况：<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/Kaggle%E7%AB%9E%E8%B5%9B%E6%A1%88%E4%BE%8B%E4%B8%80/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-12%20%E4%B8%8B%E5%8D%889.04.04.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/Kaggle%E7%AB%9E%E8%B5%9B%E6%A1%88%E4%BE%8B%E4%B8%80/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-12%20%E4%B8%8B%E5%8D%889.04.46.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h3 id="trick1"><a href="#trick1" class="headerlink" title="trick1"></a>trick1</h3><p><strong>downcasting DataFrame.</strong> It will save some memory, everyone will need all memory possible.</p><p>In this case from 134.4MB to 61.6 MB</p><h3 id="trick2"><a href="#trick2" class="headerlink" title="trick2"></a>trick2</h3><p>pd.pivot_table()透视表功能<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/Kaggle%E7%AB%9E%E8%B5%9B%E6%A1%88%E4%BE%8B%E4%B8%80/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-12%20%E4%B8%8B%E5%8D%888.52.35.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><h3 id="trick3"><a href="#trick3" class="headerlink" title="trick3"></a>trick3</h3><p>利用图像去除极端值<br>使用<strong>seaborn</strong>。boxplot</p><h3 id="item-id"><a href="#item-id" class="headerlink" title="item_id"></a>item_id</h3><p>以item_id为索引，月份为列名生成表格来观察数据。<br>分析每个月销售的总和的趋势。<br>分析每个平均一个商品销售的趋势（和👆趋势一致）<br>查看有多少6个月来没有销售记录的商品<br>查看测试数据中有多少这样过期的商品<br>查看价格和销售额的离群点</p><p>可能的特征：</p><ol><li>时间间隔</li><li>商品放出的日期</li><li>上月的销售</li><li>销售的日期</li><li>临近的商品（id1000与1001的商品可能有所相似）</li></ol><h3 id="shop-id"><a href="#shop-id" class="headerlink" title="shop_id"></a>shop_id</h3><p>以shop_id为索引，月份为列名生成表格来观察数据。<br>查看最近开张的商店数<br>查看最近倒闭的商店数</p><p>可能的特征：</p><ol><li>时间间隔（shop_id/shp_cnt_mth）</li><li>开业月份（可能的开业促销活动）</li><li>倒闭月份（可能的清仓大甩卖）<h3 id="price"><a href="#price" class="headerlink" title="price"></a>price</h3>可能的特征：</li><li>价格分档（1/10/20/等等），显然，更低价的物品拥有着更大的销量。</li><li>打折和打折期间</li><li>价格的时间间隔（显示打折）</li><li>价格修正</li><li>店铺的收入<h3 id="dates"><a href="#dates" class="headerlink" title="dates"></a>dates</h3>可能的日期特征：</li><li>周末和假期的销售额（去修正月度的销售）</li><li>该月有几天（去修正月度的销售）</li><li>是第几个月（与季节性的物品有关）</li></ol><h3 id="shop-info"><a href="#shop-info" class="headerlink" title="shop info"></a>shop info</h3><p>shop city | shop type | shop name</p><p>可能的商店特征：</p><ol><li>shop city</li><li>shop type</li></ol><h3 id="items-csv"><a href="#items-csv" class="headerlink" title="items.csv"></a>items.csv</h3><p>从items.csv中挖掘特征<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/Kaggle%E7%AB%9E%E8%B5%9B%E6%A1%88%E4%BE%8B%E4%B8%80/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-13%20%E4%B8%8B%E5%8D%883.18.54.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>可能的特征，1.item name 2.Encoded aditional feature</p><h3 id="category-csv"><a href="#category-csv" class="headerlink" title="category.csv"></a>category.csv</h3><p>category.csv中满足的格式<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/Kaggle%E7%AB%9E%E8%B5%9B%E6%A1%88%E4%BE%8B%E4%B8%80/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-13%20%E4%B8%8B%E5%8D%883.25.09.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>可能的种类特征：</p><ol><li>部分</li><li>主要种类的名字</li><li>主要子种类的名字</li><li>第二子种类的名字</li></ol><h3 id="test-set"><a href="#test-set" class="headerlink" title="test set"></a>test set</h3><p>对测试数据集进行分析<br>将测试条目分为三组：</p><ol><li>Item/shop pairs that are in train</li><li>Items without any data</li><li>Items that are in train</li></ol><h1 id="Kaggle——销售量预测-1"><a href="#Kaggle——销售量预测-1" class="headerlink" title="Kaggle——销售量预测"></a>Kaggle——销售量预测</h1><p>没有看错，接下来是另一个solution<br>主要是Feature Engineering，XGBoost<br><a href="https://www.kaggle.com/dlarionov/feature-engineering-xgboost" target="_blank" rel="noopener">https://www.kaggle.com/dlarionov/feature-engineering-xgboost</a></p><h2 id="part1-，perfect-features"><a href="#part1-，perfect-features" class="headerlink" title="part1 ，perfect features"></a>part1 ，perfect features</h2><p>同样使用sns 显示后，去除离群点<br>其中有一个物品的价格是负，使用价格中位数来替换之。<br>根据名字来看有些商店id重复出现了，fix it。<br>对于商店，种类，物品进行预处理</p><h3 id="Monthly-sales"><a href="#Monthly-sales" class="headerlink" title="Monthly sales"></a>Monthly sales</h3><p>新增特征revenue：<br>train[‘revenue’] = train[‘item_price’] *  train[‘item_cnt_day’]</p><p>测试集是34个月中一些商店和一些物品的组合，共有5100 items * 42 shops = 2142400对组合。363个物品在训练集中是没有的。因此，对于大多数测试集中的物品目标值应该是0.另一个方面，训练集只包含过去售出或者退回的对。主要的思路是计算月度的销售将其在当月的对中用0值进行扩展。这样训练数据将会与测试数据相似。</p><p>将训练集中的 shop/item对去聚合去计算目标聚合，然后将目标值截取为（0，20），这样训练目标值将会与测试预测相似。</p><h3 id="测试集"><a href="#测试集" class="headerlink" title="测试集"></a>测试集</h3><p>将测试集的月份设置为34，并与训练集进行合并</p><h3 id="Shops-Items-Cats-features"><a href="#Shops-Items-Cats-features" class="headerlink" title="Shops/Items/Cats features"></a>Shops/Items/Cats features</h3><p>将shop，item，item_category表进行合并</p><h3 id="Traget-lags"><a href="#Traget-lags" class="headerlink" title="Traget lags"></a>Traget lags</h3><p>相当于将窗口移动，[0,33]，lags为1则为[1,33]</p><h3 id="均值编码特征"><a href="#均值编码特征" class="headerlink" title="均值编码特征"></a>均值编码特征</h3><p>表格的特征的命名形式为  feature1_feature2_avg_feature_cnt<br>意思为选定feature1,feature2,来聚合feature_cnt求均值。<br>求每个月中物品售出的均值数 0.3左右<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/Kaggle%E7%AB%9E%E8%B5%9B%E6%A1%88%E4%BE%8B%E4%B8%80/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-13%20%E4%B8%8B%E5%8D%888.09.49.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><p>求每个月中每个物品所对应的均值（可以理解为平均每家商店售出的值）<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/Kaggle%E7%AB%9E%E8%B5%9B%E6%A1%88%E4%BE%8B%E4%B8%80/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-13%20%E4%B8%8B%E5%8D%888.11.42.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><p>选定date_block_num，shop_id，在item_cnt_month聚合求均值<br>可以理解为一个月一家店销售物品数量的均值数<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/Kaggle%E7%AB%9E%E8%B5%9B%E6%A1%88%E4%BE%8B%E4%B8%80/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-13%20%E4%B8%8B%E5%8D%888.27.11.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><p>同理还有：<br>选定date_block_num，item_category_id，在item_cnt_month聚合求均值<br>选定date_block_num，item_category_id，shop_id，在item_cnt_month聚合求均值<br>选定date_block_num，type_code，shop_id，在item_cnt_month聚合求均值<br>选定date_block_num，subtype_code，shop_id，在item_cnt_month聚合求均值<br>选定date_block_num，city_code，在item_cnt_month聚合求均值<br>选定date_block_num，city_code，item_id 在item_cnt_month聚合求均值<br>选定date_block_num，type_code 在item_cnt_month聚合求均值<br>选定date_block_num，subtype_code 在item_cnt_month聚合求均值</p><h3 id="trend-features"><a href="#trend-features" class="headerlink" title="trend features"></a>trend features</h3><p>上六个月的价格趋势。<br>上个月的商店的营收趋势。</p><h3 id="Special-features"><a href="#Special-features" class="headerlink" title="Special features"></a>Special features</h3><p>将月份中添加上天数</p><p>对于每个shop/item对上一笔销售的月，使用编程方法实现：<br>创建HashTable键值等于{shop_id,item_id},值等于date_block_num。对于数据表从上往下迭代。如果{row.shop_id,row.item_id}不在表中，则添加进表中，并将值设为row.date_block_num。如果HashTable中包含值，则计算cached value与row.date_block_num。</p><p>Months since the first sale for each shop/item pair and for item only.</p><h3 id="最终准备"><a href="#最终准备" class="headerlink" title="最终准备"></a>最终准备</h3><p>Because of the using 12 as lag value drop first 12 months. Also drop all the columns with this month calculated values (other words which can not be calcucated for the test set).</p><p>Producing lags brings a lot of nulls.</p><h2 id="part2-xgboost"><a href="#part2-xgboost" class="headerlink" title="part2 ,xgboost"></a>part2 ,xgboost</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Kaggle-CrowdFlower&quot;&gt;&lt;a href=&quot;#Kaggle-CrowdFlower&quot; class=&quot;headerlink&quot; title=&quot;Kaggle_CrowdFlower&quot;&gt;&lt;/a&gt;Kaggle_CrowdFlower&lt;/h1&gt;&lt;p&gt;&lt;a hre
      
    
    </summary>
    
      <category term="知识总结" scheme="https://github.com/zdkswd/categories/%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/"/>
    
    
      <category term="机器学习" scheme="https://github.com/zdkswd/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>XGBoost调参</title>
    <link href="https://github.com/zdkswd/2019/04/14/XGBoost%E8%B0%83%E5%8F%82/"/>
    <id>https://github.com/zdkswd/2019/04/14/XGBoost调参/</id>
    <published>2019-04-14T06:54:47.000Z</published>
    <updated>2019-04-14T07:34:58.217Z</updated>
    
    <content type="html"><![CDATA[<h1 id="XGBoost调参"><a href="#XGBoost调参" class="headerlink" title="XGBoost调参"></a>XGBoost调参</h1><p><a href="https://www.cnblogs.com/mfryf/p/6293814.html" target="_blank" rel="noopener">XGBoost参数调优完全指南（附Python代码） - 知识天地 - 博客园</a></p><h1 id="XGBoost的优势"><a href="#XGBoost的优势" class="headerlink" title="XGBoost的优势"></a>XGBoost的优势</h1><ol><li><strong>正则化</strong>，标准GBM的实现没有像XGBoost这样的正则化步骤。正则化对减少过拟合也是有帮助的。</li><li><strong>并行处理</strong>，XGBoost可以实现并行处理，相比GBM有了速度的飞跃。主要不是生成树的Boosting阶段，而是计算gain的阶段。</li><li><strong>高度的灵活性</strong>，XGBoost 允许用户定义自定义优化目标和评价标准 它对模型增加了一个全新的维度，所以我们的处理不会受到任何限制。</li><li><strong>缺失值处理</strong>，XGBoost内置处理缺失值的规则。 用户需要提供一个和其它样本不同的值，然后把它作为一个参数传进去，以此来作为缺失值的取值。XGBoost在不同节点遇到缺失值时采用不同的处理方法，并且会学习未来遇到缺失值时的处理方法。</li><li><strong>剪枝</strong>，当分裂时遇到一个负损失时，GBM会停止分裂。因此GBM实际上是一个贪心算法。 XGBoost会一直分裂到指定的最大深度(max_depth)，然后回过头来剪枝。如果某个节点之后不再有正值，它会去除这个分裂。 这种做法的优点，当一个负损失（如-2）后面有个正损失（如+10）的时候，就显现出来了。GBM会在-2处停下来，因为它遇到了一个负值。但是XGBoost会继续分裂，然后发现这两个分裂综合起来会得到+8，因此会保留这两个分裂。</li><li><strong>内置交叉验证</strong>，XGBoost允许在每一轮boosting迭代中使用交叉验证。因此，可以方便地获得最优boosting迭代次数。 而GBM使用网格搜索，只能检测有限个值。</li><li><strong>在已有的模型基础上继续</strong>，XGBoost可以在上一轮的结果上继续训练。这个特性在某些特定的应用上是一个巨大的优势。 sklearn中的GBM的实现也有这个功能，两种算法在这一点上是一致的。</li></ol><h1 id="XGBoost的参数"><a href="#XGBoost的参数" class="headerlink" title="XGBoost的参数"></a>XGBoost的参数</h1><h2 id="通用参数"><a href="#通用参数" class="headerlink" title="通用参数"></a>通用参数</h2><ol><li><strong>booster[默认gbtree]</strong>，选择每次迭代的模型，有两种选择：gbtree：基于树的模型，gbliner：线性模型。尽管有两种booster可供选择，这里只介绍tree booster，因为它的表现远远胜过linear booster，所以linear booster很少用到。</li><li><strong>silent[默认0]</strong>，当这个参数值为1时，静默模式开启，不会输出任何信息。一般这个参数就保持默认的0，因为这样能帮我们更好地理解模型。</li><li><strong>nthread[默认值为最大可能的线程数]</strong>，这个参数用来进行多线程控制，应当输入系统的核数。 如果你希望使用CPU全部的核，那就不要输入这个参数，算法会自动检测它。<h2 id="booster参数"><a href="#booster参数" class="headerlink" title="booster参数"></a>booster参数</h2></li><li><strong>eta[默认0.3]</strong>，和GBM中的 learning rate 参数类似。 通过减少每一步的权重，可以提高模型的鲁棒性。 典型值为0.01-0.2。</li><li><strong>min_child_weight[默认1]</strong>，决定最小叶子节点样本权重和。 和GBM的 min_child_leaf 参数类似，但不完全一样。XGBoost的这个参数是最小样本权重的和，而GBM参数是最小样本总数。 这个参数用于避免过拟合。当它的值较大时，可以避免模型学习到局部的特殊样本。 但是如果这个值过高，会导致欠拟合。这个参数需要使用CV  (cross_validition) 来调整。</li><li><strong>max_depth[默认6]</strong>，和GBM中的参数相同，这个值为树的最大深度。 这个值也是用来避免过拟合的。max_depth越大，模型会学到更具体更局部的样本。 需要使用CV函数来进行调优。 典型值：3-10</li><li><strong>max_leaf_nodes</strong>，树上最大的节点或叶子的数量。 可以替代max_depth的作用。因为如果生成的是二叉树，一个深度为n的树最多生成n2个叶子。 如果定义了这个参数，GBM会忽略max_depth参数。</li><li><strong>gamma[默认0]</strong>，在节点分裂时，只有分裂后损失函数的值下降了，才会分裂这个节点。Gamma指定了节点分裂所需的最小损失函数下降值。 这个参数的值越大，算法越保守。这个参数的值和损失函数息息相关，所以是需要调整的。</li><li><strong>max_delta_step[默认0]</strong>，这参数限制每棵树权重改变的最大步长。如果这个参数的值为0，那就意味着没有约束。如果它被赋予了某个正值，那么它会让这个算法更加保守。 通常，这个参数不需要设置。但是当各类别的样本十分不平衡时，它对逻辑回归是很有帮助的。 这个参数一般用不到，但是你可以挖掘出来它更多的用处。</li><li><strong>subsample[默认1]</strong>，和GBM中的subsample参数一模一样。这个参数控制对于每棵树，随机采样的比例。 减小这个参数的值，算法会更加保守，避免过拟合。但是，如果这个值设置得过小，它可能会导致欠拟合。 典型值：0.5-1</li><li><strong>colsample_bytree[默认1]</strong>，和GBM里面的max_features参数类似。用来控制每棵随机采样的列数的占比(每一列是一个特征)。 典型值：0.5-1</li><li><strong>colsample_bylevel[默认1]</strong>,用来控制树的每一级的每一次分裂，对列数的采样的占比。 我个人一般不太用这个参数，因为subsample参数和colsample_bytree参数可以起到相同的作用。但是如果感兴趣，可以挖掘这个参数更多的用处。</li><li><strong>lambda[默认1]</strong>,权重的L2正则化项。(和Ridge regression类似)。 这个参数是用来控制XGBoost的正则化部分的。虽然大部分数据科学家很少用到这个参数，但是这个参数在减少过拟合上还是可以挖掘出更多用处的。</li><li><strong>alpha[默认1]</strong>,权重的L1正则化项。(和Lasso regression类似)。 可以应用在很高维度的情况下，使得算法的速度更快。</li><li><strong>scale_pos_weight[默认1]</strong>,在各类别样本十分不平衡时，把这个参数设定为一个正值，可以使算法更快收敛。</li></ol><h2 id="学习目标参数"><a href="#学习目标参数" class="headerlink" title="学习目标参数"></a>学习目标参数</h2><ol><li><strong>objective[默认reg:linear]</strong>，这个参数定义需要被最小化的损失函数。最常用的值有：binary:logistic 二分类的逻辑回归，返回预测的概率(不是类别)。multi:softmax 使用softmax的多分类器，返回预测的类别(不是概率)。在这种情况下，你还需要多设一个参数：num_class(类别数目)。 multi:softprob 和multi:softmax参数一样，但是返回的是每个数据属于各个类别的概率。</li><li><strong>eval_metric[默认值取决于objective参数的取值]</strong>，对于有效数据的度量方法。 对于回归问题，默认值是rmse，对于分类问题，默认值是error。 典型值有：rmse 均方根误差，mae 平均绝对误差(∑Ni=1|?|N) logloss 负对数似然函数值 error 二分类错误率(阈值为0.5) merror 多分类错误率 mlogloss 多分类logloss损失函数 auc 曲线下面积。</li><li><strong>seed(默认0)</strong>，随机数的种子 设置它可以复现随机数据的结果，也可以用于调整参数。</li></ol><h1 id="参数调优的一般方法"><a href="#参数调优的一般方法" class="headerlink" title="参数调优的一般方法"></a>参数调优的一般方法</h1><p>我们会使用和GBM中相似的方法。需要进行如下步骤：</p><ol><li>选择较高的学习速率(learning rate)。一般情况下，学习速率的值为0.1。但是，对于不同的问题，理想的学习速率有时候会在0.05到0.3之间波动。选择对应于此学习速率的理想决策树数量。XGBoost有一个很有用的函数“cv”，这个函数可以在每一次迭代中使用交叉验证，并返回理想的决策树数量。</li><li>对于给定的学习速率和决策树数量，进行决策树特定参数调优(max_depth, min_child_weight, gamma, subsample, colsample_bytree)。在确定一棵树的过程中，可以选择不同的参数。</li><li>xgboost的正则化参数的调优。(lambda, alpha)。这些参数可以降低模型的复杂度，从而提高模型的表现。</li><li>降低学习速率，确定理想参数。<h1 id="用xgboost模型对特征重要性进行排序"><a href="#用xgboost模型对特征重要性进行排序" class="headerlink" title="用xgboost模型对特征重要性进行排序"></a>用xgboost模型对特征重要性进行排序</h1><h2 id="梯度提升算法是如何计算特征重要性的？"><a href="#梯度提升算法是如何计算特征重要性的？" class="headerlink" title="梯度提升算法是如何计算特征重要性的？"></a>梯度提升算法是如何计算特征重要性的？</h2>使用梯度提升算法的好处是在提升树被创建后，可以相对直接地得到每个属性的重要性得分。一般来说，重要性分数，衡量了特征在模型中的提升决策树构建中价值。一个属性越多的被用来在模型中构建决策树，它的重要性就相对越高。</li></ol><p>属性重要性是通过对数据集中的每个属性进行计算，并进行排序得到。在单个决策树中通过每个属性分裂点改进性能度量的量来计算属性重要性，由节点负责加权和记录次数。也就说一个属性对分裂点改进性能度量越大（越靠近根节点），权值越大；被越多提升树所选择，属性越重要。性能度量可以是选择分裂节点的Gini纯度，也可以是其他度量函数。</p><p>最终将一个属性在所有提升树中的结果进行加权求和后然后平均，得到重要性得分。</p><h2 id="根据xgboost特征重要性得分进行特征选择"><a href="#根据xgboost特征重要性得分进行特征选择" class="headerlink" title="根据xgboost特征重要性得分进行特征选择"></a>根据xgboost特征重要性得分进行特征选择</h2><p>特征重要性得分，可以用于在scikit-learn中进行特征选择。通过SelectFromModel类实现，该类采用模型并将数据集转换为具有选定特征的子集。这个类可以采取预先训练的模型，例如在整个数据集上训练的模型。然后，它可以阈值来决定选择哪些特征。当在SelectFromModel实例上调用transform()方法时，该阈值被用于在训练集和测试集上一致性选择相同特征。</p><h1 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h1><p>1、仅仅靠参数的调整和模型的小幅优化，想要让模型的表现有个大幅度提升是不可能的。GBM的最高得分是0.8487，XGBoost的最高得分是0.8494。确实是有一定的提升，但是没有达到质的飞跃。<br>2、要想让模型的表现有一个质的飞跃，需要依靠其他的手段，诸如，特征工程(feature egineering) ，模型组合(ensemble of model),以及堆叠(stacking)等。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;XGBoost调参&quot;&gt;&lt;a href=&quot;#XGBoost调参&quot; class=&quot;headerlink&quot; title=&quot;XGBoost调参&quot;&gt;&lt;/a&gt;XGBoost调参&lt;/h1&gt;&lt;p&gt;&lt;a href=&quot;https://www.cnblogs.com/mfryf/p/6
      
    
    </summary>
    
      <category term="知识总结" scheme="https://github.com/zdkswd/categories/%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/"/>
    
    
      <category term="机器学习" scheme="https://github.com/zdkswd/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>特征选择</title>
    <link href="https://github.com/zdkswd/2019/04/09/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/"/>
    <id>https://github.com/zdkswd/2019/04/09/特征选择/</id>
    <published>2019-04-09T02:09:47.000Z</published>
    <updated>2019-04-09T02:10:16.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h1><p>特征选择是特征工程里的一个重要问题，其目标是寻找最优特征子集。特征选择的目的有如下三个：</p><ol><li>简化模型，使模型更易于研究人员和用户理解。</li><li>改善性能。节省存储和计算开销。</li><li>改善通用性，降低过拟合的风险。</li></ol><p>特征选择的一般流程<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/v2-a69a37aaa14a8040b4d867d7058aafe9_hd.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>主要分为产生过程，评估过程，停止条件和验证过程。</p><p><strong>当特征数量很大的时候， 这个搜索空间会很大，如何找最优特征还是需要一些经验结论。</strong></p><h1 id="具体特征选择方法"><a href="#具体特征选择方法" class="headerlink" title="具体特征选择方法"></a>具体特征选择方法</h1><p>分为三大类：</p><ol><li>Filter：过滤法，按照发散性或者相关性对各个特征进行评分，设定阈值或者待选择阈值的个数，选择特征。</li><li>Wrapper：包装法，根据目标函数（通常是预测效果评分），每次选择若干特征，或者排除若干特征。</li><li>Embedded：嵌入法，先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数从大到小排序选择特征。类似于Filter方法，但是是通过训练来确定特征的优劣。</li></ol><h2 id="过滤特征选择"><a href="#过滤特征选择" class="headerlink" title="过滤特征选择"></a>过滤特征选择</h2><p>过滤特征选择法的想法是针对每个特征 x_i ，i 从 1 到 n ，计算 x_i 相对于类别标签 y 的信息量 S(i) ，得到 n 个结果，然后将 n 个 S(i) 按照从大到小排序，输出前 k  个特征。显然，这样复杂度大大降低。那么关键的问题就是使用什么样的方法来度量 S(i) ，我们的目标是选取与 y 关联最密切的一些 特征x_i 。</p><h3 id="Pearson相关系数"><a href="#Pearson相关系数" class="headerlink" title="Pearson相关系数"></a>Pearson相关系数</h3><p>皮尔森相关系数是一种最简单的，能帮助理解特征和响应变量之间关系的方法，该方法衡量的是变量之间的线性相关性，结果的取值区间为 [-1,1] ， -1 表示完全的负相关(这个变量下降，那个就会上升)， +1 表示完全的正相关， 0 表示没有线性相关。Pearson Correlation速度快、易于计算，经常在拿到数据(经过清洗和特征提取之后的)之后第一时间就执行。Scipy的pearsonr方法能够同时计算相关系数和p-value，<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-08%20%E4%B8%8A%E5%8D%8810.03.30.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>Pearson相关系数的一个<strong>明显缺陷</strong>是，作为特征排序机制，他只对<strong>线性关系敏感</strong>。如果关系是非线性的，即便两个变量具有一一对应的关系，Pearson相关性也可能会接近 0 。</p><h3 id="卡方验证"><a href="#卡方验证" class="headerlink" title="卡方验证"></a>卡方验证</h3><p>什么是卡方检验：<br>卡方检验就是检验两个变量之间有没有关系。<br>以运营为例:<br>卡方检验可以检验男性或者女性对线上买生鲜食品有没有区别；<br>不同城市级别的消费者对买SUV车有没有什么区别；<br>如果有显著区别的话，我们会考虑把这些变量放到模型或者分析里去。</p><p>注意：<strong>卡方检验针对分类变量。</strong></p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-08%20%E4%B8%8A%E5%8D%8810.42.27.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-08%20%E4%B8%8A%E5%8D%8810.42.41.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>置信度的话，我们按照我们自己意愿挑选，一般我们会挑90％或者95%。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-08%20%E4%B8%8A%E5%8D%8810.47.39.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-08%20%E4%B8%8A%E5%8D%8810.48.01.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-08%20%E4%B8%8A%E5%8D%8811.00.32.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>卡方检验就是统计样本的实际观测值与理论推断值之间的偏离程度，实际观测值与理论推断值之间的偏离程度就决定卡方值的大小，如果卡方值越大，二者偏差程度越大；反之，二者偏差越小；若两个值完全相等时，卡方值就为0，表明理论值完全符合。</p><p>不难发现，这个统计量的含义简而言之就是自变量对因变量的相关性。用sklearn中feature_selection库的SelectKBest类结合卡方检验来选择特征的代码如下：<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-08%20%E4%B8%8A%E5%8D%8810.31.35.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>sklearn.feature_selection模块中的类可以用于样本集中的特征选择/维数降低，以提高估计器的准确度分数或提高其在非常高维数据集上的性能</p><h3 id="互信息和最大信息系数"><a href="#互信息和最大信息系数" class="headerlink" title="互信息和最大信息系数"></a>互信息和最大信息系数</h3><p>经典的互信息也是评价定性自变量对定性因变量的相关性的，互信息公式如下：<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-08%20%E4%B8%8B%E5%8D%8812.39.16.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>当 x_i 是0/1离散值的时候，这个公式如上。很容易推广到 x_i 是多个离散值的情况。这里的 p(x_i,y) , p(x_i) 和 p(y) 都是从训练集上得到的。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-08%20%E4%B8%8B%E5%8D%8812.40.00.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>MI 衡量的是 x_i 和 y 的独立性。如果它俩独立 P(x_i,y)=p(x_i)p(y) ，那么 KL 距离值为0，也就是 x_i 和 y 不相关了，可以去除 x_i 。相反，<strong>如果两者密切相关，那么 MI 值会很大。</strong></p><p>在对 MI 进行排名后，最后剩余的问题就是如何选择 k 个值（前 k 个 x_i ）。我们继续使用交叉验证的方法，将 k 从 1 扫描到 n ，取最大的 F 。</p><p>想把互信息直接用于特征选择其实不是太方便：1、它不属于度量方式，也没有办法归一化，在不同数据及上的结果无法做比较；2、对于连续变量的计算不是很方便（ X 和 Y 都是集合, x_i, y 都是离散的取值），通常变量需要先离散化，而互信息的结果对离散化的方式很敏感。</p><p>最大信息系数克服了这两个问题。它首先寻找一种最优的离散化方式，然后把互信息取值转换成一种度量方式，取值区间在 [0,1] 。minepy提供了MIC功能。</p><p> 如y=x^2 这个例子，MIC算出来的互信息值为1(最大的取值)。代码如下：<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-08%20%E4%B8%8B%E5%8D%8812.50.37.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><h3 id="距离相关系数"><a href="#距离相关系数" class="headerlink" title="距离相关系数"></a>距离相关系数</h3><p>距离相关系数是为了克服Pearson相关系数的弱点而生的。在 x 和 x^2 这个例子中，即便Pearson相关系数是 0 ，我们也不能断定这两个变量是独立的（<strong>有可能是非线性相关</strong>）；但如果距离相关系数是 0 ，那么我们就可以说这两个变量是独立的。</p><p>尽管有MIC和距离相关系数在了，但当变量之间的关系接近线性相关的时候，Pearson相关系数仍然是不可替代的。第一、Pearson相关系数计算速度快，这在处理大规模数据的时候很重要。第二、Pearson相关系数的取值区间是[-1，1]，而MIC和距离相关系数都是[0，1]。这个特点使得Pearson相关系数能够表征更丰富的关系，符号表示关系的正负，绝对值能够表示强度。当然，Pearson相关性有效的前提是两个变量的变化关系是单调的。</p><h3 id="方差选择法"><a href="#方差选择法" class="headerlink" title="方差选择法"></a>方差选择法</h3><p>过滤特征选择法还有一种方法不需要度量特征 x_i 和类别标签 y 的信息量。这种方法先要计算各个特征的方差，然后根据阈值，选择方差大于阈值的特征。</p><p>例如，假设我们有一个具有布尔特征的数据集，并且我们要删所有01特征中出现0的概率超过80%的特征。布尔特征是伯努利随机变量，这些变量的方差由下式给出:<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-09%20%E4%B8%8A%E5%8D%889.06.28.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>VarianceThreshold是特征选择的简单基线方法。它删除方差不符合某个阈值的所有特征。默认情况下，它会删除所有零差异特征，即所有样本中具有相同值的特征。代码如下：<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-09%20%E4%B8%8A%E5%8D%889.08.42.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>输出结果：<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-09%20%E4%B8%8A%E5%8D%889.09.21.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>如预期的那样，VarianceThreshold已经删除了第一列，其具有 p=5/6&gt;0.8 包含零的概率。</p><h2 id="包装-wrapper-特征选择"><a href="#包装-wrapper-特征选择" class="headerlink" title="包装(wrapper)特征选择"></a>包装(wrapper)特征选择</h2><p>Wrapper这里指不断地使用不同的特征组合来测试学习算法进行特征选择。先选定特定算法， 一般会选用普遍效果较好的算法， 例如Random Forest， SVM， kNN等等。</p><h3 id="前向搜索"><a href="#前向搜索" class="headerlink" title="前向搜索"></a>前向搜索</h3><p>前向搜索说白了就是每次增量地从剩余未选中的特征选出一个加入特征集中，待达到阈值或者 n 时，从所有的 F 中选出错误率最小的。过程如下：</p><ol><li>初始化特征集 F 为空。</li><li>扫描 i 从 1 到 n如果第 i 个特征不在 F 中，那么特征 i 和F 放在一起作为 F_i (即取并集，在只使用 F_i 中特征的情况下，利用交叉验证来得到 F_i 的错误率。</li><li>从上步中得到的 n 个 F_i 中选出错误率最小的 F_i ,更新 F 为 F_i 。</li><li>如果 F 中的特征数达到了 n 或者预定的阈值（如果有的话），那么输出整个搜索过程中最好的 ；若没达到，则转到 2，继续扫描。</li></ol><h3 id="后向搜索"><a href="#后向搜索" class="headerlink" title="后向搜索"></a>后向搜索</h3><p>既然有增量加，那么也会有增量减，后者称为后向搜索。先将 F 设置为 {1,2,…,n} ，然后每次删除一个特征，并评价，直到达到阈值或者为空，然后选择最佳的 F 。</p><p><strong>这两种算法都可以工作，但是计算复杂度比较大。</strong>时间复杂度为<br>O(n+(n-1)+(n-2)+…+1)=O(n^2)</p><h3 id="递归特征消除法"><a href="#递归特征消除法" class="headerlink" title="递归特征消除法"></a>递归特征消除法</h3><p>递归消除特征法使用一个基模型来进行多轮训练，每轮训练后，消除若干权值系数的特征，再基于新的特征集进行下一轮训练。使用feature_selection库的RFE类来选择特征的代码如下：<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-09%20%E4%B8%8A%E5%8D%889.30.05.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><h2 id="嵌入-Embedded-特征选择"><a href="#嵌入-Embedded-特征选择" class="headerlink" title="嵌入(Embedded)特征选择"></a>嵌入(Embedded)特征选择</h2><h3 id="基于惩罚项的特征选择法"><a href="#基于惩罚项的特征选择法" class="headerlink" title="基于惩罚项的特征选择法"></a>基于惩罚项的特征选择法</h3><p>通过L1正则项来选择特征：L1正则方法具有稀疏解的特性，因此天然具备特征选择的特性，但是要注意，L1没有选到的特征不代表不重要，原因是两个具有高相关性的特征可能只保留了一个，如果要确定哪个特征重要应再通过L2正则方法交叉检验。</p><h3 id="基于学习模型的特征排序"><a href="#基于学习模型的特征排序" class="headerlink" title="基于学习模型的特征排序"></a>基于学习模型的特征排序</h3><p>这种方法的思路是直接使用你要用的机器学习算法，针对每个单独的特征和响应变量建立预测模型。假如某个特征和响应变量之间的关系是非线性的，可以用基于树的方法（决策树、随机森林）、或者扩展的线性模型等。基于树的方法比较易于使用，因为他们对非线性关系的建模比较好，并且不需要太多的调试。但要注意过拟合问题，因此树的深度最好不要太大，再就是运用交叉验证。通过这种训练对特征进行打分获得相关性后再训练最终模型。</p><p>在波士顿房价数据集上使用sklearn的随机森林回归给出一个单变量选择的例子：<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-09%20%E4%B8%8A%E5%8D%889.52.18.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;特征选择&quot;&gt;&lt;a href=&quot;#特征选择&quot; class=&quot;headerlink&quot; title=&quot;特征选择&quot;&gt;&lt;/a&gt;特征选择&lt;/h1&gt;&lt;p&gt;特征选择是特征工程里的一个重要问题，其目标是寻找最优特征子集。特征选择的目的有如下三个：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;简化模型
      
    
    </summary>
    
      <category term="知识总结" scheme="https://github.com/zdkswd/categories/%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/"/>
    
    
      <category term="机器学习" scheme="https://github.com/zdkswd/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>百面  特征工程</title>
    <link href="https://github.com/zdkswd/2019/04/04/%E7%99%BE%E9%9D%A2%20%20%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"/>
    <id>https://github.com/zdkswd/2019/04/04/百面  特征工程/</id>
    <published>2019-04-04T12:00:47.000Z</published>
    <updated>2019-04-04T12:00:50.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="百面-特征工程"><a href="#百面-特征工程" class="headerlink" title="百面  特征工程"></a>百面  特征工程</h1><p>Garbage in, garbage out。对于一个机器学习问题，<strong>数据和特征</strong>往往<strong>决定</strong>结果的<strong>上限</strong>，而模型，<strong>算法</strong>的选择及优化是在逐步<strong>接近</strong>这个<strong>上限</strong>。</p><p>特征工程，顾名思义，是对原始数据进行一系列工程处理，将其提炼为特征，作为<strong>输入</strong>供算法和模型使用。从本质上来讲，特征工程是一个表示和展现数据的过程。在实际工作中，特征工程旨在去除原始数据中的杂质和冗余，设计更高效的特征以刻画求解的问题与预测模型之间的关系。<strong>特征就是指input的x！</strong></p><p>有两种常用的数据类型：<br>(1) 结构化数据。结构化数据类型可以看作关系型数据库的一张表，每列都有清晰的定义，包含了数值型、类别型两种基本类型，每一行数据表示一个样本的信息。</p><p>（2）非结构化数据。非结构化数据主要包括文本，图像，音频，视频数据，其包含的信息无法用一个简单的数值表示，也没有清晰的类别定义，并且每条数据的大小各不相同。</p><h1 id="特征归一化"><a href="#特征归一化" class="headerlink" title="特征归一化"></a>特征归一化</h1><p>为了消除数据特征之间的量纲影响，我们需要对特征进行归一化处理，使得不同指标之间具有可比性。例如，分析一个人的身高和体重对健康的影响，如果<br>使用米(m)和千克(kg)作为单位，那么身高特征会在1.6~1.8m的数值范围内，体重特征会在50~100kg的范围内，分析出来的结果显然会倾向于数值差别比较大的体重特征。想要得到更为准确的结果，就需要进行特征归一化(Normalization)处理，使各指标处于同一数值量级，以便进行分析。</p><h2 id="为什么需要对数值类型的特征做归一化？"><a href="#为什么需要对数值类型的特征做归一化？" class="headerlink" title="为什么需要对数值类型的特征做归一化？"></a>为什么需要对数值类型的特征做归一化？</h2><p>对数值类型的特征做归—化可以将所有的特征都统一到一个大致相同的数值区间内。最常用的方法主要有以下两种。</p><p>(1)线性函数归一化(Min-Max Scaling) 。它对原始数据进行线性变换，使结果映射到[0, 1]的范围，实现对原始数据的等比缩放。归一化公式如下<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E7%99%BE%E9%9D%A2%20%20%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-03%20%E4%B8%8B%E5%8D%881.35.54.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>（2）零均值归一化(Z-Score Normalization)。它会将原始数据映射到均值为0、标准差为1的分布上。具体来说，假设原始特征的均值为μ、标准差为δ，那么归一化公式定义为<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E7%99%BE%E9%9D%A2%20%20%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-03%20%E4%B8%8B%E5%8D%881.49.37.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><p>对数值型特征做归一化的原因在于，<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E7%99%BE%E9%9D%A2%20%20%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/WechatIMG70.jpeg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><p>数据归一化不是万能的，在实际应用中，通过<strong>梯度下降法</strong>求解的模型通常是需要归一化的，包括线性回归，逻辑回归，支持向量机，神经网络等模型。但对于决策树模型则并不适用，以C4.5为例，决策树在进行节点分裂时主要依据数据集D关于特征x的信息增益比，而信息增益比跟特征是否经过归一化是无关的，因为归一化不会改变样本在特征x上的信息增益。</p><h1 id="类别型特征"><a href="#类别型特征" class="headerlink" title="类别型特征"></a>类别型特征</h1><p>类别型特征(Categorical Feature)主要是指性别(男、女)、血型(A、B、AB、O)等只在有限选项内取值的特征。类别型特征原始输入通常是字符串形式，除了决策树等少数模型能直接处理字符串形式的输入，<strong>对于逻辑回归、支持向量机等模型来说，类别型特征必须经过处理转换成数值型特征才能正确工作。</strong></p><h2 id="与处理时如何处理类别型特征？"><a href="#与处理时如何处理类别型特征？" class="headerlink" title="与处理时如何处理类别型特征？"></a>与处理时如何处理类别型特征？</h2><h3 id="序号编码"><a href="#序号编码" class="headerlink" title="序号编码"></a>序号编码</h3><p>序号编码通常用于处理类别间具有大小关系的数据。例如成绩，可以分为低、中、高三档，并且存在“高&gt;中&gt;低”的排序关系。序号编码会按照大小关系对类别型特征赋予一个数值ID,例如高表示为3、中表示为2、低表示为1,转换后依然保留了大小关系。</p><h3 id="独热编码"><a href="#独热编码" class="headerlink" title="独热编码"></a>独热编码</h3><p>独热编码通常用于处理类别间不具有大小关系的特征。例如血型，一共有4个取值(A型血、B型血、AB型血、O型血)，独热编码会把血型变成一个4维稀疏向量，A型血表示为(1,0,0,0) ，B型血表示为(0, 1,0,0)，AB型表示为(0, 0,1,0)，O型血表示为(0,0,0,1)。对于类别取值较多的情况下使用独热编码需要<br>注意以下问题。</p><p>(1)使用稀疏向量来节省空间。在独热编码下，特征向量只有某一维取值为1，其他位置取值均为0。因此可以利用向量的稀疏表示有效地节省空间，并且目<br>前大部分的算法均接受稀疏向量形式的输入。</p><p>(2)配合特征选择来降低维度。高维度特征会带来几方面的问题。一是在K近邻算法中，高维空间下两点之间的距离很难得到有效的衡量;二是在逻辑回归模型中，参数的数量会随着维度的增高而增加，容易引起过拟合问题;三是通常只有部分维度是对分类、预测有帮助，因此可以考虑配合特征选择来降低维度。</p><h3 id="二进制编码"><a href="#二进制编码" class="headerlink" title="二进制编码"></a>二进制编码</h3><p>二进制编码主要分为两步，先用序号编码给每个类别赋予一个类别ID,然后将类别ID对应的二进制编码作为结果。以A、B、AB、O血型为例，A型血的ID为1，二进制表示为001; B型血的ID为2，二进制表示为010;以此类推可以得到AB型血和O型血的二进制表示。可以看出，二进制编码本质上是利用二进制对ID进行哈希映射，最终得到0/1特征向量，且维数少于独热编码，节省了存储空间。</p><h1 id="组合特征"><a href="#组合特征" class="headerlink" title="组合特征"></a>组合特征</h1><h2 id="什么是组合特征？如何处理高维组合特征？"><a href="#什么是组合特征？如何处理高维组合特征？" class="headerlink" title="什么是组合特征？如何处理高维组合特征？"></a>什么是组合特征？如何处理高维组合特征？</h2><p>为了提高复杂关系的拟合能力，在特征工程中经常会把一阶离散特征两两组合，构成高阶组合特征。以广告点击预估问题为例，原始数据有语言和类型两种离散特征，表1.2是语言和类型对点击的影响。为了提高拟合能力，语言和类型可以组成二阶特征，表1.3是语言和类型的组合特征对点击的影响。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E7%99%BE%E9%9D%A2%20%20%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/WechatIMG71.jpeg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br><strong>表1.3乃组合特征矩阵。</strong><br>以逻辑回归为例，假设数据的特征向量为X=(x1,x2,…,xk)，则有，<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E7%99%BE%E9%9D%A2%20%20%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-03%20%E4%B8%8B%E5%8D%889.42.46.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>其中&lt;xi,xj&gt;表示xi和xj的组合特征,wij的维度等于|xi| * |xj|，|xi|和|xj|分别代表第i个特征和第j个特征不同取值的个数。在上述表中，wij即为表1.3中的0或1。</p><p>若是关于用户和物品的矩阵，用户数量为m，物品数量为n，当m，n的数量巨大时，几乎无法学习m•n规模的参数。此时，有效的办法是将用户和物品分别用k维的低维向量表示，k远小于m和n。此时<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E7%99%BE%E9%9D%A2%20%20%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/page29image88376.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>分别表示xi和xj对应的低维向量。需要学习的参数规模变为了m•k+n•k。这也就是推荐系统中的矩阵分解。</p><p>上述内容即为利用<strong>降维方法来减少两个高维特征组合后需要学习的参数。</strong></p><h1 id="组合特征-1"><a href="#组合特征-1" class="headerlink" title="组合特征"></a>组合特征</h1><p>在实际应用中，常常需要面对多种高维特征，简单地两两组合依然容易存在参数过多，过拟合等问题，而且并不是所有的特征组合都是有意义的。可以采用GBDT来构建特征。</p><h1 id="文本表示模型"><a href="#文本表示模型" class="headerlink" title="文本表示模型"></a>文本表示模型</h1><p>文本是一类非常重要的非结构化数据,如何表示文本数据一直是机器学习领域的一个重要研究方向。</p><h2 id="有哪些文本表示模型？各有什么优缺点？"><a href="#有哪些文本表示模型？各有什么优缺点？" class="headerlink" title="有哪些文本表示模型？各有什么优缺点？"></a>有哪些文本表示模型？各有什么优缺点？</h2><h3 id="词袋模型和N-gram模型"><a href="#词袋模型和N-gram模型" class="headerlink" title="词袋模型和N-gram模型"></a>词袋模型和N-gram模型</h3><p>最基础的文本表示模型是<strong>词袋模型</strong>。顾名思义,就是将每篇文章看成一袋子词,并忽略每个词出现的顺序。具体地说,就是将整段文本以词为单位切分开然后每篇文章可以表示成一个<strong>长向量</strong>,向量中的每一维代表一个单词,而该维对应的权重则反映了这个词在原文章中的重要程度。<strong>常用TF-IDF来计算权重</strong>,公式为<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E7%99%BE%E9%9D%A2%20%20%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-04%20%E4%B8%8B%E5%8D%886.43.52.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>其中TF(t,d)为单词t在文档d中出现的频率,IDF(t)是<strong>逆文档频率</strong>,用来衡量单词t对表达语义所起的重要性,表示为<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E7%99%BE%E9%9D%A2%20%20%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-04%20%E4%B8%8B%E5%8D%886.46.07.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>直观的解释是,如果一个单词在非常多的文章里面都出现,那么它可能是一个比较通用的词汇,对于区分某篇文章特殊语义的贡献较小,因此对权重做一定惩罚。</p><p>将文章进行单词级别的划分有时候并不是一种好的做法,比如英文中的natural  language processing(自然语言处理)一词,如果将 natural, language, processing这3个词拆分开来,所表达的含义与三个词连续出现时大相径庭。通常,可以将连续出现的n个词(n≤N)组成的词组(<strong>N-gram</strong>)也作为一个单独的特征放到向量表示中去,构成<strong>N-gram模型</strong>。另外,同一个词可能有多种词性变化,却具有相似的含义。在实际应用中,一般会对单词进行词干抽取(Word Stemming)处理,即将不同词性的单词统一成为同一词干的形式。</p><h3 id="主题模型"><a href="#主题模型" class="headerlink" title="主题模型"></a>主题模型</h3><p>主题模型用于从文本库中发现有代表性的主题，并且能够计算出每篇文章的主题分布。</p><h3 id="词嵌入与深度学习模型"><a href="#词嵌入与深度学习模型" class="headerlink" title="词嵌入与深度学习模型"></a>词嵌入与深度学习模型</h3><p>词嵌入是一类将词向量化的模型的统称,核心思想是将每个词都映射成低维空间(通常K=50~300维)上的一个稠密向量(<strong>Dense vector</strong>)。K维空间的每一维也可以看作一个隐含的主题,只不过不像主题模型中的主题那样直观。</p><p>由于词嵌入将每个词映射成一个K维的向量,如果一篇文档有N个词,就可以用一个NxK维的矩阵来表示这篇文档,但是这样的表示过于底层。在实际应用中,如果仅仅把这个矩阵作为原文本的表示特征输入到机器学习模型中,通常很难得到令人满意的结果。</p><p>因此,还需要在此基础之上加工出更高层的特征。在传统的浅层机器学习模型中,一个好的特征工程往往可以带来算法效果的显著提升。而深度学习模型正好为我们提供了一种自动地进行特征工程的方式,模型中的每个隐层都可以认为对应着不同抽象层次的特征。从这个角度来讲,<strong>深度学习模型</strong>能够打败浅层模型也就顺理成章了。卷积神经网络和循环神经网络的结构在文本表示中取得了很好的效果,主要是由于它们能够更好地对文本进行建模,抽取出一些高层的语义特征。与全连接的网络结构相比,卷积神经网络和循环神经网络一方面很好地抓住了文本的特性,另一方面又减少了网络中待学习的参数,提高了训练速度,并且降低了过拟合的风险。</p><p>所以说嵌入加深度模型成为了标配。</p><h1 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec"></a>Word2Vec</h1><p>谷歌2013年提出的word2vec是目前最常用的词嵌入模型之一。Word2Vec实际是一种<strong>浅层的神经网络模型</strong>,它有两种网络结构,分别是CBOW(Continues Bag of words)和Skip-gram。</p><h2 id="word2vec是如何工作的-它和LDA有什么区别与联系"><a href="#word2vec是如何工作的-它和LDA有什么区别与联系" class="headerlink" title="word2vec是如何工作的?它和LDA有什么区别与联系?"></a>word2vec是如何工作的?它和LDA有什么区别与联系?</h2><p><strong>CBOW</strong>的目标是根据上下文出现的词语来预测当前词的生成概率;而Skip-gram是根据当前词来预测上下文中各词的生成概率。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E7%99%BE%E9%9D%A2%20%20%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-04%20%E4%B8%8B%E5%8D%887.15.32.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>其中w()是当前所关注的词,w(t-2)、w(t-1)、w(t+1)、w(t+2)是上下文中出现的词。这里前后滑动窗口大小均设为2。</p><p>CBOW和Skip-gram都可以表示成由输入层(Input)、映射层(Projection)和输出层(Output)组成的神经网络。</p><p><strong>输入层</strong>中的每个词由<strong>独热编码</strong>方式表示,即所有词均表示成一个N维向量,其中N为词汇表中单词的总数。</p><p><strong>映射层</strong>(又称<strong>隐含层</strong>)中,K个隐含单元(Hidden units)的取值可以由N维输入向量以及连接输入和隐含单元之间的NxK维权重矩阵计算得到。在CBOW中,还需要将各个输入词所计算出的隐含单元求和。</p><p>同理,输出层向量的值可以通过隐含层向量(K维),以及连接隐含层和输出层之间的KxN维权重矩阵计算得到。输出层也是一个N维向量,每维与词汇表中的个单词相对应。最后,对输出层向量应用 Softmax激活函数,可以计算出每个单词的生成概率。 Softmax激活函数的定义为<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E7%99%BE%E9%9D%A2%20%20%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-04%20%E4%B8%8B%E5%8D%887.34.02.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>其中x代表N维的原始输出向量,xn为在原始输出向量中,与单词wn所对应维度的取值。</p><p>接下来的任务就是训练神经网络的权重,使得语料库中所有单词的整体生成概率最大化。从输入层到隐含层需要一个维度为NxK的权重矩阵,从隐含层到输出层又需要一个维度为KxN的权重矩阵,学习权重可以用反向传播算法实现,每次迭代时将权重沿梯度更优的方向进行一小步更新。但是由于Softmax激活函数中存在归一化项的缘故,推导出来的迭代公式需要对词汇表中的所有单词进行遍历,使得每次迭代过程非常缓慢,由此产生了<strong>Hierarchical Softmax</strong>和<strong>Negative Sampling</strong>两种改进方法。</p><p>谈到word2Vec与LDA的区别和联系,首先,LDA是利用文档中单词的共现关系来对单词按主题聚类,也可以理解为对“文档-单词”矩阵进行分解,得到“文档主题”和“主题-单词”两个概率分布。而Word2vec其实是对“上下文单词矩阵进行 学习,其中上下文由周围的几个单词组成,由此得到的词向量表示更多地融入了上下文共现的特征。也就是说,如果两个单词所对应的Word2Vec向量相似度较      高,那么它们很可能经常在同样的上下文中出现。需要说明的是,上述分析的是 LDA与Word2veC的不同,不应该作为主题模型和词嵌入两类方法的主要差异。主题模型通过一定的结构调整可以基于“上下文-单词”矩阵进行主题推理。同样地,  词嵌入方法也可以根据“文档-单词”矩阵学习出词的隐含向量表示。主题模型和词嵌入两类方法最大的不同其实在于模型本身,主题模型是一种基于概率图模型的生成式模型,其似然函数可以写成若干条件概率连乘的形式,其中包括需要推测的隐含变量(即主题);而词嵌入模型一般表达为神经网络的形式,似然函数定义在网络的输出之上,需要通过学习网络的权重以得到单词的稠密向量表示。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;百面-特征工程&quot;&gt;&lt;a href=&quot;#百面-特征工程&quot; class=&quot;headerlink&quot; title=&quot;百面  特征工程&quot;&gt;&lt;/a&gt;百面  特征工程&lt;/h1&gt;&lt;p&gt;Garbage in, garbage out。对于一个机器学习问题，&lt;strong&gt;数据和特征
      
    
    </summary>
    
      <category term="知识总结" scheme="https://github.com/zdkswd/categories/%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/"/>
    
    
      <category term="机器学习" scheme="https://github.com/zdkswd/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Happy Birthday to ZDK’s Blog🎁🎁🎁</title>
    <link href="https://github.com/zdkswd/2019/04/04/Happy%20Birthday%20to%20ZDK%E2%80%99s%20Blog%F0%9F%8E%81%F0%9F%8E%81%F0%9F%8E%81/"/>
    <id>https://github.com/zdkswd/2019/04/04/Happy Birthday to ZDK’s Blog🎁🎁🎁/</id>
    <published>2019-04-04T06:23:32.000Z</published>
    <updated>2019-04-04T06:23:58.000Z</updated>
    
    <content type="html"><![CDATA[<p>去年的今天，ZDK的blog诞生了。</p><p>建立这个博客的初衷在于当做一个自己的读书笔记，在线免费云笔记。刚开始时是完全把大部分的内容照搬下来，基本上是读到了什么东西就写下来什么东西。基本上这个阶段主要就是体力劳动。此时博客的用处在于，当我看到一个什么东西时，我会想起来，我好像以前在博客中记录过这个东西，然后打开我的博客查看相关的内容来加深印象。这个阶段我的知识储备还不够多，所以处于知识积累的阶段。所以说博客这个时候更像是一个备忘录，什么知识点记得不太清了，可以翻翻博客加深记忆。</p><p>后来，我在《认知天性》这本书中找到了科学的解释，最适合人类大脑习性的学习方法包括三个步骤：<strong>编码，巩固和检索</strong>。使用博客来进行知识的积累貌似正好符合这几个阶段。第一个阶段我将博客当做我大脑的外存，我的大脑只需记忆知识的索引，在需要使用时，迅速从外存中调入我的大脑。随着调入次数的增多以及知识体系的逐渐建立，我对特定知识的印象也更深，很多时候也会在不同时期有着新的理解。知识慢慢地开始可以串联起来。而这个时候，在看到很多知识点时，就会有新的疑问，即为检索问题阶段。随着对问题的解决以及总结，慢慢发现自己对于知识点的理解更加深入了。善用检索，即问题导向。而发现问题更好的途径是对比知识点，以及真实场景。</p><p>随着不断的学习，越来越发现参考一本书是远远不够的，所以第二个阶段，这个博客开始出现了针对某个问题的总结，在这个阶段，需要翻阅多样参考文献来针对特定的问题作出总结，慢慢地，我发现，这个阶段大量的东西总结发生在纸上，真正到博客中的可能是经过总结后的。纸上总结有一个好处，就是不用鼠标的滚轮，在一个页面中即可构建知识的网络结构。下一个阶段，要更多的思考问题，更多的输出总结。形成自己的知识表征。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/Happy%20Birthday%20to%20ZDK%E2%80%99s%20Blog%F0%9F%8E%81%F0%9F%8E%81%F0%9F%8E%81/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-04-04%20%E4%B8%8B%E5%8D%8812.10.28.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><p><strong>主题阅读</strong><br><strong>问题导向</strong><br><strong>真实场景</strong><br><strong>刻意练习</strong></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;去年的今天，ZDK的blog诞生了。&lt;/p&gt;
&lt;p&gt;建立这个博客的初衷在于当做一个自己的读书笔记，在线免费云笔记。刚开始时是完全把大部分的内容照搬下来，基本上是读到了什么东西就写下来什么东西。基本上这个阶段主要就是体力劳动。此时博客的用处在于，当我看到一个什么东西时，我会想
      
    
    </summary>
    
      <category term="个人总结" scheme="https://github.com/zdkswd/categories/%E4%B8%AA%E4%BA%BA%E6%80%BB%E7%BB%93/"/>
    
    
      <category term="总结" scheme="https://github.com/zdkswd/tags/%E6%80%BB%E7%BB%93/"/>
    
  </entry>
  
  <entry>
    <title>广告竞价  综述</title>
    <link href="https://github.com/zdkswd/2019/04/02/%E5%B9%BF%E5%91%8A%E7%AB%9E%E4%BB%B7%20%20%E7%BB%BC%E8%BF%B0/"/>
    <id>https://github.com/zdkswd/2019/04/02/广告竞价  综述/</id>
    <published>2019-04-02T04:01:47.000Z</published>
    <updated>2019-04-02T04:02:05.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="基于第二价位的广告竞拍"><a href="#基于第二价位的广告竞拍" class="headerlink" title="基于第二价位的广告竞拍"></a>基于第二价位的广告竞拍</h1><p>点击率预测可以提供对广告优劣的一种预测，还需要机制来决定如何从众多的广告中进行选取,这就是<strong>广告的竞价排名</strong>。</p><p>广告位竞价排名的出现有两个原因。第一，发布商的广告位是有限的。不管是搜索广告还是展示广告，绝大多数的发布商都以一定的比例在原生的内容，例如新闻、社交媒体内容里插入一些广告位。但是这些广告位的数目是有限的，特别是在优质的发布商资源里，就会出现一些广告位有着很大的竞争。第二，既然有竞争，那么如果引入一种竞价机制的话，势必有可能抬高广告的单价，从而让广告中间平台例如DSP，或者是发布商从中获取更高的价值。</p><h2 id="基于第一价位的竞拍"><a href="#基于第一价位的竞拍" class="headerlink" title="基于第一价位的竞拍"></a>基于第一价位的竞拍</h2><p>所谓基于第一价位的竞拍，指的是所有的投标方都定好自己的出价，然后一次性统一出价。 在出价的过程中，所有的投标方都是看不见竞争对手的出价的，这保证了出价的诚实性。当竞拍平台接到所有投标方的出价以后，按照出价由高到低排序，出价最高的投标方获得投标的胜利。</p><p>在广告系统中，如果要采用这样的形式，那么，决定最后投标顺序的不再是单纯的价格，而往往是一个投标价格和点击率的函数，最简单的函数就是<strong>点击率乘以投标价格</strong>。这其实也可以被认为是一种“<strong>期望收入</strong>”。也就是说，如果发布商或者DSP是按照广告的每一次点击来收取费用的话，那么，点击率乘以投标价格就是这种收入的一个数学期望值。</p><p>所以，基于第一价位竞价的广告系统，按照<strong>广告收入的期望值</strong>进行竞价排名。排名第一的广告被选为显示的广告。</p><p>这种机制在早期的互联网广告平台中曾被大量使用。但是一段时间以后，大家发现，基于第一价位竞价的竞价结果往往是“虚高”的。这也很容易形象地解释，在大家都不知道对方出价的情况下，如果希望自己能在竞拍中胜出，势必就可能报出比较高的价格。另外一个方面，投标方并不清楚这个广告位的真实价值，大家只能在条件允许的情况下，尽量抬高价格来获取这个机会。从某种意义上来说，这样的竞价并不利于广告商的长远发展，也打击了广告商的积极性。</p><h2 id="基于第二价位的竞拍"><a href="#基于第二价位的竞拍" class="headerlink" title="基于第二价位的竞拍"></a>基于第二价位的竞拍</h2><p>就是在基于第一价位竞价的基础上， 互联网广告界逐渐衍生出了一种新的竞拍方法一基于第二价位的竞拍。首先，和基于第一价位的竞拍模式一样，基于第二价位的模式也是按照广告的期望收入，也就是根据点击率和出价的乘积来进行排序。但和基于第一价位模式不一 样的是，中间商或者发布商并不按照第一位的价格来收取费用，而是按照竞价排位第二位的广告商的出价来收取费用。也就是说，<strong>虽然第一名利用自己的出价赢得了排名，但是只需要付第二名所出的价格。时至今日，基于第二价位的竞拍方式已经成为了互联网广告的主流竞拍模式。</strong></p><p>基于第二价位的竞拍方式好处在于，在这种形式下，广告商按照自己对广告为价值的理解来竞拍是相对较优的策略。技术难点在于对于广告商来说，主要是希望知道在当前出价的情况下，究竟有多大的概率赢得当前的竞拍。这也就是所谓的“<strong>赢的概率</strong>”，这对于广告商调整自己的出价有非常重要的指导意义。对于整个出价的概率分布的一个估计，有时候又叫作<strong>竞价全景观</strong>(Bid Landscape)预测。这是一个非常形象的说法，因为广告商希望知道整个赢的概率随着出价变化的整个分布，从而调整自己的安排。</p><p>这样的预测工作会用到一些简单的模型。比如，有学者认为，赢的价格服从一个“<strong>对数正态分布</strong>”(Log-normal) 也就是说，广告商出的价格并且最终赢得竞拍的这些价格，在取了对数的情况下，服从一个正态分布。当然，这是一个假设。但是有了这么一个假设以后，我们就可以从数据中估计这个对数正态分布的参数，从而能够对整个“竞价全景观”进行估计。</p><p>对于“竞价全景观”或者是赢的价格分布的估计有一个比较困难的地方，那就是，作为广告商来说，往往并不知道所有其他竞争对手的出价，以及在没有赢得竞拍的情况下，那些赢得竞拍的出价是多少。简而言之，也就是我们<strong>只观测到了一部分数据</strong>，那就是我们赢得这些广告位的出价。在这种只有一部分信息的情况下，所做的估计就会不准确。</p><h1 id="广告竞价策略"><a href="#广告竞价策略" class="headerlink" title="广告竞价策略"></a>广告竞价策略</h1><h2 id="竞价策略"><a href="#竞价策略" class="headerlink" title="竞价策略"></a>竞价策略</h2><p>其实这个问题主要是在“<strong>实时竞价</strong>”，或简称<strong>RTB</strong>的背景下来探讨的。RTB是DSP目前流行的竞价模式，也就是广告商等利用计算机程序来自动对广告竞拍进行出价。从实际的运作中来看，这样的自动竞价模式要比人工竞价更加方便快捷，也更加高效。然而，在自动竞价的模式下，我们势必需要一种指导思想，来让我们的计算机程序能够随着形式的变化来进行出价。</p><p>在RTB中，竞价策略的环境。竞价的一个重要特征，作为一个竞价方，并不知道其他竞标方的点击率和出价，因此<strong>处在一个信息不完整的竞价环境中</strong>。只能根据自己的点击率估计和自己的出价，以及过去出价的成功与否来对整个市场形势进行判断。这就是RTB竞价策略的一大挑战和难点。</p><p>在这样的背景下， RTB竞价策略的研究和开发集中在以下两种思路上。</p><p>一种思路是<strong>把整个竞价策略当做一种“博弈”</strong>(Game) ，从而根据博弈论中的方法来对竞价环境中各个竞标方的行为和收益进行研究。 用博弈论的方法来对竞价进行研究有一个最大的特点，那就是博弈论主要是对各个竞标方行为之间的<strong>关联性</strong>进行建模，这种关联性包括他们之间的<strong>收益</strong>和他们的<strong>动机</strong>。</p><p>另外一种思路是<strong>把整个竟价策略当做是纯粹的统计决策</strong>,也就是直接对广告商的行为进行建模,而把整个竟价环境中的种种联系都当做是当前决策下的不确定因素。在这样的思路下,各个竞标方之间的行为关联变得不那么重要,而对于整个不确定性的建模则变得至关重要。</p><p>第一种思路,也就是利用博弈论的方法来对竟价策略进行硏究主要存在于学术界。虽然从理论上来说,博弈论可能提供一种比较有说服力的解释,但是这种思路需要对整个竟价环境有非常多的假设(例如竞标方是不是理性,市场是不是充分竞争等等)。而<strong>第二种思路</strong>,仅仅需要从广告商自身的角度出发,因此在现实中,这种思路的操作性更强,从而<strong>受到工业界的青睐</strong>。</p><p>总的来说,第二种思路其实就是根据当前的输入信息,例如页面信息、广告信息、用户信息以及上下文信息等,学到一个<strong>输出价格的函数</strong>,也就是说,这个函数的输出就是在现在情况下当前广告的出价。当然,这个函数势必需要考虑各种不确定的因素。</p><h2 id="搜索广告和展示广告的竞价"><a href="#搜索广告和展示广告的竞价" class="headerlink" title="搜索广告和展示广告的竞价"></a>搜索广告和展示广告的竞价</h2><p>搜索广告和展示广告的竞标存在着不小的区别，因此，从技术上来讲，就发展出了一系列不同的方法。</p><p>对于搜索广告来讲，在大多数情况下，每-一个出价都是针对某-个搜索关键词的。利用机器学习方法对搜索广告的出价进行建模的工作。在这个工作里，每一个关键词的出价来自于一一个线性函数的输出，而这个线性函数是把用户信息、关键词以及其他的页面信息当做特性，学习了一个<strong>从特性到出价的线性关系</strong>。这可以算是最早的利用线性函数来进行出价的例子了。</p><p>展示广告的竞价则面临着不同的挑战。首先，在展示广告中，场景中并不存在搜索关键词这种概念。因此，很多广告商无法针对场景事先产生出价。这也就要求RTB的提供商要能够在不同的场景中帮助广告商进行出价。</p><p>同时，相比于搜索广告针对<strong>每一个关键词</strong>的出价方式来说，针对<strong>每一个页面显示机会</strong>出价的挑战则更大。理论上讲，每一个页面显示机会的价格都可能有很大的不同。很多RTB都利用一种叫作<strong>CPM的收费模式</strong>，也就是说，一旦某一个广告位被赢得之后，对于广告商来说，这往往就意味着需要被收取费用。所以，在展示广告的情况下，如何针对当前的页面显示机会以及目前的预<br>算剩余等等因素进行统一建模，就成为一个必不可少的步骤。</p><h2 id="竞价策略的其他问题"><a href="#竞价策略的其他问题" class="headerlink" title="竞价策略的其他问题"></a>竞价策略的其他问题</h2><p>因此，在广告竞价策略中，还存在着一个叫“<strong>预算步调</strong>”(Budget Pacing)的技术，也就是希望能够让广告的展示相对平缓而不至于在短时间内使用完全部的预算。这势必对于广告如何出价有着直接的影响。</p><p>另外，对于平台而言，虽然竞价保证了一定的竞争，但是也并不是所有的展示机会都有非常充分的竞争。因此，从平台的角度来说，如何能够保证一定的收益就变得十分重要。在这样的情况下，有的平台有一种叫作“<strong>保留价格</strong>”(Reserved Price)的做法，用来设置一个 最低的竞价价格。保留价格虽然能够来保证收益，但是也可能会让广告商觉得不划算，因此如何来设置这个保留价格，也就成为了出价策略中的一个重要组成部分。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;基于第二价位的广告竞拍&quot;&gt;&lt;a href=&quot;#基于第二价位的广告竞拍&quot; class=&quot;headerlink&quot; title=&quot;基于第二价位的广告竞拍&quot;&gt;&lt;/a&gt;基于第二价位的广告竞拍&lt;/h1&gt;&lt;p&gt;点击率预测可以提供对广告优劣的一种预测，还需要机制来决定如何从众多的
      
    
    </summary>
    
      <category term="知识总结" scheme="https://github.com/zdkswd/categories/%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/"/>
    
    
      <category term="机器学习" scheme="https://github.com/zdkswd/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>RS矩阵分解  概述</title>
    <link href="https://github.com/zdkswd/2019/03/29/RS%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3%20%20%E6%A6%82%E8%BF%B0/"/>
    <id>https://github.com/zdkswd/2019/03/29/RS矩阵分解  概述/</id>
    <published>2019-03-29T13:36:47.000Z</published>
    <updated>2019-04-01T08:35:52.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="RS矩阵分解-概述"><a href="#RS矩阵分解-概述" class="headerlink" title="RS矩阵分解  概述"></a>RS矩阵分解  概述</h1><p>源自于评分预测问题。评分预测问题很典型但是并不大众，与之相对的另一类问题行为预测，才是平民级推荐问题，处处可见。</p><h1 id="为什么要矩阵分解"><a href="#为什么要矩阵分解" class="headerlink" title="为什么要矩阵分解"></a>为什么要矩阵分解</h1><p>矩阵分解确实可以解决一些近邻模型无法解决的问题。</p><p>近邻模型存在的问题：</p><ol><li>物品建存在相关性，信息量并不随着向量维度增加而线性增加。</li><li>矩阵元素稀疏，计算结果不稳定，增减一个向量维度，导致近邻结果差异很大的情况存在。</li></ol><p>上述两个问题，在矩阵分解中可以得到解决。矩阵分解，直观上说来简单，就是把原来的大矩阵，近似分解成两个小矩阵的乘积，在实际推荐计算时不再使用大矩阵，而是使用分解得到的两个小矩阵。</p><p>具体说来就是，假设用户物品的评分矩阵A是m乘以n维，即共有m个用户，n个物品。我们选一个很小的数k，这个k比m和n都小很多，比如小两个数量级这样，通过一套算法得到两个矩阵U和V,矩阵U的维度是m乘以k,代表用户偏好的用户隐因子向量。矩阵V的维度是n乘以k，代表物品语义主题的隐因子向量。</p><p>矩阵UV需要能够通过公式复原矩阵A：<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/RS%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3%20%20%E6%A6%82%E8%BF%B0/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-29%20%E4%B8%8B%E5%8D%882.02.13.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>得到的这两个矩阵有这么几个特点:<br>1.每个用户对应一个k维向量，每个物品也对应一个k维向量，就是所谓的隐因子向量，因为是无中生有变出来的，所以叫做“隐因子”;<br>2.两个矩阵相乘后，就得到了任何一个用户对任何一个物品的预测评分，具体这个评分靠不靠谱，那就是看功夫了。就需要定义损失函数去优化。</p><p>所以说矩阵分解，所做的事就是矩阵填充。</p><h1 id="基础的SVD算法"><a href="#基础的SVD算法" class="headerlink" title="基础的SVD算法"></a>基础的SVD算法</h1><p>SVD全称奇异值分解，属于线性代数的知识，在推荐算法中实际使用的并不是正统的奇异值分解，而是一个伪奇异值分解。传统SVD在实际应用场景中面临着稀疏性问题和效率问题。所以，提出了一个新的方法Funk-SVD算法，其主要思路是将原始评分矩阵M（m*n）分解成两个矩阵P（m*k）和Q(k*n),同时仅考察原始评分矩阵中有评分的项分解结果是否准确，而判别标准是均方差。这种方法被称为隐语义模型（Latent factor）<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/add/28.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><p>用户评分矩阵通常是稀疏的，如果只有少数项有值，极易造成过拟合，此时就要通过矩阵分解将矩阵分解为稠密的矩阵。矩阵分解，就是把用户和物品都映射到一个k维空间中，这个k维空间不是我们能直接看到的，也不一定具有非常好的可解释性，每一个维度也没有名字，所以常常叫做<strong>隐因子</strong>，代表藏在直观的矩阵数据下。</p><p>每一个物品都获得一个向量q，每一个用户也得到一个向量p。</p><p>对于物品，与它对应的向量q中的元素有正有负，代表这个物品背后隐藏的一些用户关注的因素。</p><p>对于用户，与它对应的向量p的元素，也有正有负，代表这个用户在若干因素上的偏好。物品被关注的因素，和用户偏好的因素，它们的数量和意义是一致的，就是矩阵分解之初人为指定的k。</p><p>例如，用户u的向量是pu，物品i的向量是qi，要计算物品i推荐给用户u的推荐分数，直接计算点积即可：<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/RS%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3%20%20%E6%A6%82%E8%BF%B0/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-29%20%E4%B8%8B%E5%8D%882.16.24.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><p>难点在于，如何得到每一个用户，每一个物品的k维向量。这是一个机器学习问题。两个要素，损失函数和优化算法。</p><p>SVD的损失函数定义如下：<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/RS%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3%20%20%E6%A6%82%E8%BF%B0/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-29%20%E4%B8%8B%E5%8D%882.22.50.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>加号前一部分控制着模型的偏差，加号后一部分控制着模型的方差。<br>前一部分就是用分解后的矩阵预测分数，要和实际的用户评分之间误差越小越好。后一部分就是，得到的隐因子向量要越简单越好，以控制这个模型的方差，也就是让它在真正执行推荐任务时要发挥稳定，以免产生“过拟合”。</p><p>整个SVD的学习过程就是：</p><ol><li>准备好用户物品的评分矩阵，每一条评分数据看做一条训练样本。</li><li>给分解后的U矩阵和V矩阵随机初始化元素值。</li><li>用U和V计算预测后的分数。</li><li>计算预测的分数和实际的分数误差。</li><li>按照梯度下降的方向更新U和V中的元素值。</li><li>重复步骤3到5，直到到达停止条件。</li></ol><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/add/29.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>得到分解后的矩阵之后，实质上就是得到了每个用户和每个物品的隐因子向量，拿着这个向量再做推荐计算就简单了。</p><h1 id="增加偏置信息"><a href="#增加偏置信息" class="headerlink" title="增加偏置信息"></a>增加偏置信息</h1><p>实际情况下，一些用户会给出偏高的分数，比如标准宽松的用户，有些物品也会受到偏高的评分，比如一些目标观众为铁粉的电影，甚至有可能整个平台的全局评分就偏高。</p><p>原装的SVD就有了第一个变种:把偏置信息抽出来的SVD。</p><p>一个用户给一个物品的评分会由四部分相加:<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/RS%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3%20%20%E6%A6%82%E8%BF%B0/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-29%20%E4%B8%8B%E5%8D%883.34.44.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>从左至右分别代表:全局平均分、物品的评分偏置、用户评分的偏置、用户和物品之间的兴趣偏好。</p><p>针对前面三项偏置分数，我在这里举个例子，假如一个电影评分网站全局平均分是3分,《肖申克的救赎》的平均分比全局平均分要高1分。</p><p>你是一个对电影非常严格的人，你一般打分比平均分都要低0.5,所以前三项从左到右分别就是3, 1, -0.5。 如果简单的就靠这三项，也可以给计算出一个你会给《肖申克的救赎》打的分数，就是3.5。</p><p>增加了偏置信息的SVD模型目标函数稍有改变:<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/RS%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3%20%20%E6%A6%82%E8%BF%B0/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-29%20%E4%B8%8B%E5%8D%883.37.24.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>和基本的SVD相比，要想学习两个参数:用户偏置和物品偏置。学习的算法还是一样的。</p><h1 id="增加历史行为"><a href="#增加历史行为" class="headerlink" title="增加历史行为"></a>增加历史行为</h1><p>相比沉默的大多数，主动点评电影或者美食的用户是少数。也就是显示反馈比隐式反馈少，但是可以利用隐式反馈来弥补这一点，对于用户的个人属性，比如性别，也可以加入到模型中来弥补冷启动的不足。</p><p>在SVD中结合用户的隐式反馈行为和属性，这套模型叫做SVD++。</p><p>加入隐式反馈的方法是除了假设评分矩阵中的物品有一个隐因子向量外，用户有过行为的物品集合也都有一个隐因子向量，维度是一样的。把用户操作过的物品隐因子向量加起来，用来表达用户的兴趣偏好。</p><p>综合两者，SVD++的目标函数中，只需要把推荐分数预测部分稍作修改，原来的用户向量部分增加了隐式反馈向量和用户属性向量。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/RS%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3%20%20%E6%A6%82%E8%BF%B0/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-29%20%E4%B8%8B%E5%8D%884.36.29.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>学习算法依然不变，只是要学习的参数多了两个向量: x和y。其中N(U)表示为用户U提供了隐式反馈的物品的集合。xi所在的项是隐式反馈的物品向量，ya所在的项是用户属性的向量。这样一来，在用户没有评分时，也可以用他的隐式反馈和属性做出一定的预测。</p><h1 id="考虑时间因素"><a href="#考虑时间因素" class="headerlink" title="考虑时间因素"></a>考虑时间因素</h1><p>人是善变的，今天的我们和十年前的我们很可能不一样了，在SVD中考虑时间因素也变得顺理成章。</p><p>在SVD中考虑时间因素，有几种做法:<br>1.对评分按照时间加权，让久远的评分更趋近平均值;<br>2.对评分时间划分区间，不同的时间区间内分别学习出隐因子向量，使用时按照区间使用对应的隐因子向量来计算;<br>3.对特殊的期间，如节日、周末等训练对应的隐因子向量。</p><h1 id="交替最小二乘原理（ALS）"><a href="#交替最小二乘原理（ALS）" class="headerlink" title="交替最小二乘原理（ALS）"></a>交替最小二乘原理（ALS）</h1><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/RS%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3%20%20%E6%A6%82%E8%BF%B0/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-29%20%E4%B8%8B%E5%8D%885.39.58.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>有了目标函数，就要用到优化算法找到使它最小的参数，优化算法常用的有两个，一个是<strong>随机梯度下降（SGD）</strong>，是针对qi或者pu整条向量进行更新而不是单个元素。<br>损失函数求偏导：<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/add/30.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><p>根据随机梯度下降法得到的递推公式:<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/add/31.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>另一个是<strong>交替最小二乘(ALS)</strong>。</p><p>在实际应用中，交替最小二乘更常用一些，这也是社交巨头Facebook在他们的推荐系统中选择的主要矩阵分解方法。</p><p>交替最小二乘的核心是交替,任务是找到两个矩阵P和Q,让它们相乘后约等于原矩阵R：<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/RS%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3%20%20%E6%A6%82%E8%BF%B0/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-29%20%E4%B8%8B%E5%8D%887.56.11.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>如果知道其中一个，那另一Q也就已知了，交替最小二乘法通过迭代解决了这个鸡生蛋蛋生鸡的问题。<br>1.初始化随机矩阵Q里面的元素值;<br>2.把Q矩阵当做已知的，直接用线性代数的方法求得矩阵P;<br>3.得到了矩阵P后，把P当做已知的，故技重施，回去求解矩阵Q;<br>4.上面两个过程交替进行，直到目标函数可以接受为止。</p><p>交替最小二乘的好处:<br>1.在交替的其中一步，也就是假设已知其中一个矩阵求解另一个时，要优化的参数是很容易并行化的;<br>2.在不那么稀疏的数据集合上，交替最小二乘通常比随机梯度下降要更快地得到结果。</p><h1 id="隐式反馈"><a href="#隐式反馈" class="headerlink" title="隐式反馈"></a>隐式反馈</h1><p>矩阵分解算法，是为解决评分预测问题而生的，然而事实上却是，用户首先必须先去浏览商品，然后是购买，最后才可能打分。相比“预测用户会打多少分”，“预测用户会不会去浏览”更加有意义，而且，用户浏览数据远远多于打分评价数据。也就是说，实际上推荐系统关注的是预测行为，行为也就是一再强调的隐式反馈。</p><p>最方便的数据时高质量的显式反馈，其中包括用户对产品感兴趣的显式输入。通常，显式反馈包含一个稀疏矩阵，因为任何一个用户可能只对可能的项目进行了一小部分的评分。隐式反馈通常指示一个事件存在或不存在，因此它通常由一个密集的矩阵表示。</p><p>如果把预测用户行为看成一个二分类问题， 猜用户会不会做某件事，但实际上收集到的数据只有明确的一类:用户干了某件事，而用户明确“不干”某件事的数据却没有明确表达。所以这就是One-Class的由来，One-Class数据也是隐式反馈的通常特点。</p><p>对隐式反馈的矩阵分解，需要将交替最小二乘做一些改进，改进后的算法叫做<strong>加权交替最小二乘</strong>: <strong>Weighted-ALS</strong>。</p><p>用户对物品的反馈，通常是多次的，行为的次数是对行为的置信度反应，也就是所谓的加权。</p><p>加权交替最小二乘法对待隐式反馈：<br>1.如果用户对物品无隐式反馈则认为评分是0;<br>2.如果用户对物品有至少一次隐式反馈则认为评分是1,次数作为该评分的置信度。</p><p>现在的目标函数变为：<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/RS%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3%20%20%E6%A6%82%E8%BF%B0/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-29%20%E4%B8%8B%E5%8D%888.30.19.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>多出来的Cui就是置信度，在计算误差时考虑反馈次数,次数越多，就越可信。置信度一般也不是直接等于反馈次数，根据一些经验，置信度Cui这样计算:<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/RS%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3%20%20%E6%A6%82%E8%BF%B0/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-29%20%E4%B8%8B%E5%8D%888.43.16.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>其中阿尔法是一个超参数，需要调教，默认值取40可以得到差不多的效果，C就是次数了。</p><p>此时那些没有反馈的缺失值，就是在我们的设定下，取值为0的评分就非常多，有两个原因导致在实际使用时要注意这个问题:<br>1.本身隐式反馈就只有正类别是确定的，负类别是我们假设的。<br>2.这会导致正负类别样本非常不平衡，严重倾斜到0评分这边。</p><p>应对这个问题的做法就是负样本采样：挑一部分缺失值作为负类别样本即可。有两种挑的方法。<br>1.随机均匀采样和正类别一样多。<br>2.按照物品的热门程度采样。</p><p>在实践中，第一种不是很靠谱，第二种经受住了检验，其思想是一个越热门的物品，用户越可能知道它的存在。那这种情况下，用户还没对它有反馈就表明:这很可能就是真正的负样本。</p><h1 id="推荐计算"><a href="#推荐计算" class="headerlink" title="推荐计算"></a>推荐计算</h1><p>在得到了分解后的矩阵后，相当于每个用户得到了隐因子向量，这是一个稠密向量，用于代表他的兴趣。同时每个物品也得到了-个稠密向量，代表它的语义或主题。而且可以认为这两者是一一对应的，用户的兴趣就是表现在物品的语义维度上的。</p><p>看上去，让用户和物品的隐因子向量两两相乘,计算点积就可以得到所有的推荐结果了。但是实际上复杂度还是很高，尤其对于用户数量和物品数量都巨大的应用，如Facebook,就更不现实。于是Facebook提出了两个办法得到真正的推荐结果。</p><p><strong>第一种</strong>， 利用一些专门设计的数据结构存储所有物品的隐因子向量，从而实现通过一个用户向量可以返回最相似的K个物品。Facebook给出了自己的开源实现Faiss,类似的开源实现还有Annoy, KGraph, NMSL IB。其中Facebook开源的Faiss 和NMSLIB (Non-Metric Space Library)都用到了ball tree来存储物品向量。</p><p>如果需要动态增加新的物品向量到索引中，推荐使用Faiss,如果不是，推荐使用NMSLIB或者KGraph。用户向量则可以存在内存数据中，这样可以在用户访问时，实时产生推荐结果。</p><p><strong>第二种</strong>，就是拿着物品的隐因子向量先做聚类，海量的物品会减少为少量的聚类。然后再逐一计算用户和每个聚类中心的推荐分数，给用户推荐物品就变成了给用户推荐物品聚类。</p><p>得到给用户推荐的聚类后，再从每个聚类中挑选少许几个物品作为最终推荐结果。这样做的好处除了大大减小推荐计算量之外，还可以控制推荐结果的多样性，因为可以控制在每个类别中选择的物品数量。</p><h1 id="矩阵分解的优缺点"><a href="#矩阵分解的优缺点" class="headerlink" title="矩阵分解的优缺点"></a>矩阵分解的优缺点</h1><p>矩阵分解有如下的优点：</p><ol><li>能将高维的矩阵映射成两个低维矩阵的乘积，很好地解决了数据稀疏的问题；</li><li>具体实现和求解都很简洁，预测的精度也比较好；</li><li>模型的可扩展性也非常优秀，其基本思想也能广泛运用于各种场景中。</li></ol><p>矩阵分解的缺点有：</p><ol><li>可解释性很差，其隐空间中的维度无法与现实中的概念对应起来；</li><li>训练速度慢，很难实现实时推荐，不过可以通过离线训练来弥补这个缺点；</li><li>实际推荐场景中往往只关心topn结果的准确性，此时考察全局的均方差显然是不准确的。</li></ol><p>矩阵分解作为推荐系统中的经典模型，已经经过了十几年的发展，时至今日依然被广泛应用于推荐系统当中，其基本思想更是在各式各样的模型中发挥出重要作用。但是对于推荐系统来说，仅仅有一个好的模型是远远不够的。影响推荐系统效果的因素非常之多。想要打造一个一流的推荐系统，除了一个强大的算法模型之外，更需要想方设法结合起具体业务，不断进行各种尝试、升级，方能取得最终的胜利。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;RS矩阵分解-概述&quot;&gt;&lt;a href=&quot;#RS矩阵分解-概述&quot; class=&quot;headerlink&quot; title=&quot;RS矩阵分解  概述&quot;&gt;&lt;/a&gt;RS矩阵分解  概述&lt;/h1&gt;&lt;p&gt;源自于评分预测问题。评分预测问题很典型但是并不大众，与之相对的另一类问题行为预测
      
    
    </summary>
    
      <category term="知识总结" scheme="https://github.com/zdkswd/categories/%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/"/>
    
    
      <category term="机器学习" scheme="https://github.com/zdkswd/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="矩阵分解" scheme="https://github.com/zdkswd/tags/%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3/"/>
    
  </entry>
  
  <entry>
    <title>熵</title>
    <link href="https://github.com/zdkswd/2019/03/27/%E7%86%B5/"/>
    <id>https://github.com/zdkswd/2019/03/27/熵/</id>
    <published>2019-03-27T11:46:47.000Z</published>
    <updated>2019-03-27T11:47:41.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="熵"><a href="#熵" class="headerlink" title="熵"></a>熵</h1><p><a href="https://www.jianshu.com/p/09b70253c840" target="_blank" rel="noopener">https://www.jianshu.com/p/09b70253c840</a><br><a href="https://baijiahao.baidu.com/s?id=1618702220267847958&amp;wfr=spider&amp;for=pc" target="_blank" rel="noopener">完美解释交叉熵</a></p><h1 id="随机变量"><a href="#随机变量" class="headerlink" title="随机变量"></a>随机变量</h1><p>包含不确定性的变量称为随机变量，统计学就是研究这类不确定性变量的工具。刻画随机变量最有力的一个工具就是它的概率分布。有了概率分布，我们可以说对一个随机变量有了完全的掌握，因为我们可以知道它可能取哪些值，某个值的概率是多少。</p><h1 id="什么是熵"><a href="#什么是熵" class="headerlink" title="什么是熵"></a>什么是熵</h1><p>概率分布是对随机变量的刻画，不同的随机变量有着相同或不同的概率分布，熵，就是对不同概率分布的刻画！本质上，是为了描述不确定的程度，并以此对不同的概率分布进行比较。</p><p>如果我们知道一枚硬币抛出后正面朝上概率为0.8，要比知道概率为0.5，更容易猜对硬币抛出后哪面朝上。即0.8的概率分布比0.5的概率分布对我们来说，具有更大的信息量。</p><p>需要的是一个定量的指标，来衡量概率分布的不确定性。这个指标就是熵。</p><h1 id="熵的数学表达"><a href="#熵的数学表达" class="headerlink" title="熵的数学表达"></a>熵的数学表达</h1><p>以抛硬币为例，我们看正面这个取值。可以看到，取正面的概率越大，则不确定性就越小。即概率越大，不确定性越小。能够表达出概率越大，不确定性越小的表达式就是：<br>-logP，图像如下：<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E7%86%B5/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-26%20%E4%B8%8B%E5%8D%889.14.29.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>图中的纵轴为-logP，横轴为P。</p><p>-logP只是衡量了某个概率的不确定性，对于抛硬币，是正面的不确定性小了，那是反面的不确定性就大了。所以衡量一个概率的分布的不确定性，就要综合衡量所有概率表达的不确定性，也就是求一个概率分部综合的不确定性。<strong>熵</strong>的表达式即为：<strong>-∑PlogP</strong></p><p>这个指标可以理解成概率分布的不确定性的期望值。这个值越大，表示该概率分布不确定性越大。它为我们人类提供的“信息”就越小，我们越难利用这个概率分布做出一个正确的判断。从这个角度，我们可以看到，熵是对概率分布信息含量的衡量，这与它是不确定性的衡量，其实是两种解读方式而已。</p><h1 id="从另一个角度理解熵"><a href="#从另一个角度理解熵" class="headerlink" title="从另一个角度理解熵"></a>从另一个角度理解熵</h1><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E7%86%B5/u=2536703424,2008652302&fm=173&app=49&f=JPEG.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>上图例中，每种颜色的概率都是1/4。此时的最优策略。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E7%86%B5/u=766989654,2959505221&fm=173&app=49&f=JPEG.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>上图为1/8绿色, 1/8 橘色, 1/4 红色, 1/2 蓝色时的最优策略。</p><p>对于同一问题，目标是问最少次数的问题，就能得到正确的答案。在这个问题中, 问题个数的期望是<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E7%86%B5/u=1914240327,2567469862&fm=173&app=49&f=JPEG.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>即为<strong>熵</strong>。<br>第一幅图的熵为 1/4(机率) <em> 2道题目 </em> 4颗球 = 2，平均需要问两道题目才能找出不同颜色的球，也就是说期望值为2，就是熵。<br>第二幅图为1/2×1+1/4×2+1/8×3+1/8×3=1.75。<br>极端情况下全是一种颜色，就不用问问题，熵为0。</p><p>熵的意义就是在<strong>最优化</strong>策略下, <strong>猜到颜色所需要的问题的个数</strong>。也就是一种情况下，只对应一个熵。在不同的情况中，熵越大，需要问的东西越多，说明其不确定性大，熵越小，<strong>需要判断的次数就越少</strong>。</p><h1 id="伯努利分布的熵"><a href="#伯努利分布的熵" class="headerlink" title="伯努利分布的熵"></a>伯努利分布的熵</h1><p>抛硬币即为伯努利分布，它的熵为：H(p) = -plogp -(1-p)log(1-p)</p><p>画出来即为下图：<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E7%86%B5/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-27%20%E4%B8%8A%E5%8D%889.09.49.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br><strong>当p=0.5时，熵最大</strong>，因为此时硬币朝上朝下完全是随机的，<strong>不确定性最大</strong>。在两侧当其中一面向上的概率为1时，则完全没有不确定性。所以熵就是0。<strong>熵越大，不确定性越大，熵越小，信息越多。</strong>所以熵越小越好，可以把熵看做是优化的目标。</p><p>事实上方差可以用来描述变量变化程度，和熵有一致性，方差越大，不确定性就越大，熵就越大。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E7%86%B5/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-27%20%E4%B8%8A%E5%8D%889.18.40.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><h1 id="联合熵"><a href="#联合熵" class="headerlink" title="联合熵"></a>联合熵</h1><p>联合熵与联合概率分布有关，对于随机变量X和Y，二者的联合概率分布为p(x,y),则这个联合概率分布的熵就叫做<strong>联合熵</strong>：<br><strong>H(x,y) = -Σp(x,y)log(p(x,y))</strong></p><p>H(x,y)肯定是大于等于H(x)或H(y)的。仅当X没有不确定性时，比如永远是正面朝上，此时，在Y的基础上联合X，并没有引入新的不确定性，所以，H(x,y)=H(y)。</p><h1 id="条件熵"><a href="#条件熵" class="headerlink" title="条件熵"></a>条件熵</h1><p>x,y的联合熵大于等于x和y单独的熵，对于y来说，引入的x增大了熵，那么，衡量x的引入增加了多大的熵呢，这就是<strong>条件熵</strong>。<br><strong>H(x|y) = H(x,y) - H(y)</strong></p><p>注意H(x|y)叫做条件熵，它可不是条件概率p(x|y)的熵。因为p(x|y)不是一个概率分布。</p><p>条件熵的计算，和条件概率还是有点关系的。<br>H(x|y) = - Σp(x,y)log(p(x|y))</p><p><strong>条件熵是在Y上引入X后增加的不确定性</strong>，从感觉上，增加的不确定性无论如何不可能大于X本身自有的不确定性，也就是：<br><strong>H(x|y) &lt;= H(x)</strong>  仅当x，y相互独立时，等号才成立。</p><h1 id="李航-信息增益"><a href="#李航-信息增益" class="headerlink" title="李航 信息增益"></a>李航 信息增益</h1><p>当熵和条件熵中的概率有数据统计（特别是极大似然估计）得到时，所对应的熵与条件熵分别称为<strong>经验熵</strong>和<strong>经验条件熵</strong>。此时如果有0概率，令0log0=0</p><p><strong>信息增益</strong>表示得知<strong>特征X</strong>的信息而使得<strong>类Y</strong>的信息的不确定性减少的程度。</p><p>特征A对训练数据集D的信息增益g(D,A)，定义为集合D的经验熵H(D)与特征A给定条件下D的经验条件熵H(D|A)之差，即<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E7%86%B5/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-27%20%E4%B8%8B%E5%8D%8812.29.58.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>一般地，熵H(Y)与条件熵H(Y|X)之差称为<strong>互信息</strong>。决策树学习中的信息增益等价于训练数据集中<strong>类与特征的互信息</strong>。详见决策树个人总结。</p><h1 id="相对熵（又叫互熵，KL散度）"><a href="#相对熵（又叫互熵，KL散度）" class="headerlink" title="相对熵（又叫互熵，KL散度）"></a>相对熵（又叫互熵，KL散度）</h1><p>相对熵和前面的几个概念联系不大。互熵D(q||p)实际上就是在衡量我们通过计算得出的真实分布的表达式q，究竟与由样本统计得来的分布p有多接近，在衡量多接近这个概念时，我们运用到了熵的形式。PQ完全相同，互熵就等于0了。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E7%86%B5/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-27%20%E4%B8%8B%E5%8D%881.44.20.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>其中P代表真实分布，Q代表模型分布。也可写为：<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E7%86%B5/9922720e0cf3d7ca31c6e0d3ff1fbe096b63a967.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><p><strong>KL散度大于等于0</strong>.</p><p>其值等于<strong>真实分布</strong>与<strong>拟合分布</strong>的<strong>交叉熵</strong>与真实分布的<strong>信息熵</strong>之差。<br>此时是两个分布间的运算，与前面同个分布两个变量的运算不是一个概念。</p><h1 id="交叉熵"><a href="#交叉熵" class="headerlink" title="交叉熵"></a>交叉熵</h1><p>P的熵是-Σp(x)log(p(x))，Q的熵是-ΣQ(x)log(Q(x))<br>交叉熵即为<strong>-ΣP(x)log(Q(x))</strong><br>交叉熵是不满足交换律的。<br>如果说对于第二个例子，仍然使用第一个例子中的策略，如下<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E7%86%B5/u=3840435294,1181281422&fm=173&app=49&f=JPEG.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>即认为小球分布是(1/4，1/4，1/4，1/4），这个分布就是非真实分布。平均来说, 需要的问题数是 1/8×2+1/8×2+1/4×2+1/2×2=2。 因此, 在例子二中使用例子一的策略是一个比较差的策略. 其中2是这个方案中的<strong>交叉熵</strong>。而最优方案的交叉熵是1.75。那样预测模型与真实最佳模型是一样的。</p><p>即为给定一个策略（分布）, 交叉熵就是在该策略（分布）下猜中颜色所需要的问题的期望值。更普遍的说，<strong>交叉熵用来衡量在给定的真实分布下，使用非真实分布所指定的策略产生的熵。</strong></p><h2 id="为什么使用交叉熵损失函数"><a href="#为什么使用交叉熵损失函数" class="headerlink" title="为什么使用交叉熵损失函数"></a>为什么使用交叉熵损失函数</h2><p>由于KL散度是衡量真实与模型分布的准则，后一项为常数值，KL值大于等于0，所以损失函数为交叉熵即可，使交叉熵越小越好。交叉熵的最小值即为真实分布的熵。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;熵&quot;&gt;&lt;a href=&quot;#熵&quot; class=&quot;headerlink&quot; title=&quot;熵&quot;&gt;&lt;/a&gt;熵&lt;/h1&gt;&lt;p&gt;&lt;a href=&quot;https://www.jianshu.com/p/09b70253c840&quot; target=&quot;_blank&quot; rel=&quot;noop
      
    
    </summary>
    
      <category term="知识总结" scheme="https://github.com/zdkswd/categories/%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/"/>
    
    
      <category term="机器学习" scheme="https://github.com/zdkswd/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>统计学习方法  k近邻法</title>
    <link href="https://github.com/zdkswd/2019/03/24/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%20%20k%E8%BF%91%E9%82%BB%E6%B3%95/"/>
    <id>https://github.com/zdkswd/2019/03/24/统计学习方法  k近邻法/</id>
    <published>2019-03-24T11:16:47.000Z</published>
    <updated>2019-03-24T11:16:18.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="统计学习方法-k近邻法"><a href="#统计学习方法-k近邻法" class="headerlink" title="统计学习方法  k近邻法"></a>统计学习方法  k近邻法</h1><p>k近邻法(k-nearest neighbor, k-NN)是一种基本<strong>分类</strong>与<strong>回归</strong>方法，下面只讨论分类问题中的k近邻法，k近邻法的输入为实例的特征向量，对应于特征空间的点;输出为实例的类别，可以取多类. k近邻法假设给定一个训练数据集，其中的实例类别已定.分类时，对新的实例,根据其k个最近邻的训练实例的类别，通过多数表决等方式进行预测.因此，k近邻法不具有显式的学习过程，k近邻法实际上利用训练数据集对特征向量空间进行划分，并作为其分类的“模型”。<strong>k值的选择、距离度量及分类决策规则</strong>是k近邻法的三个基本要素. </p><h1 id="k近邻算法"><a href="#k近邻算法" class="headerlink" title="k近邻算法"></a>k近邻算法</h1><p>k近邻算法简单、直观:给定一个训练数据集，对新的输入实例，在训练数据集中找到与该实例最邻近的k个实例，这k个实例的多数属于某个类，就把该输入实例分为这个类.</p><p><strong>k近邻法没有显式的学习过程.</strong></p><h1 id="k近邻模型"><a href="#k近邻模型" class="headerlink" title="k近邻模型"></a>k近邻模型</h1><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>特征空间中，对每个训练实例点，距离该点比其他点更近的所有点组成一个区域，叫作单元(cell). 每个训练实例点拥有一个单元，所有训练实例点的单元构成对特征空间的一个划分.最近邻法将实例xi的类yi作为其单元中所有点的类标记(class label). 这样，每个单元的实例点的类别是确定的。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%20%20k%E8%BF%91%E9%82%BB%E6%B3%95/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-24%20%E4%B8%8B%E5%8D%886.30.48.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><h2 id="距离度量"><a href="#距离度量" class="headerlink" title="距离度量"></a>距离度量</h2><p>欧式距离，曼哈顿距离等。</p><h2 id="k值的选择"><a href="#k值的选择" class="headerlink" title="k值的选择"></a>k值的选择</h2><p>如果选择较小的k值，就相当于用较小的邻域中的训练实例进行预测，“学习”的近似误差(approximation error)会减小，只有与输入实例较近的(相似的)训练实例才会对预测结果起作用.但缺点是“学习”的估计误差(estimation error)会增大，预测结果会对近邻的实例点非常敏感.如果邻近的实例点恰巧是噪声,预测就会出错.换句话说，<strong>k值的减小就意味着整体模型变得复杂，容易发生过拟合.</strong></p><p>如果选择较大的k值，就相当于用较大邻域中的训练实例进行预测.其优点是可以减少学习的估计误差.但缺点是学习的近似误差会增大.这时与输入实例较远的(不相似的)训练实例也会对预测起作用，使预测发生错误. <strong>k值的增大就意味着整体的模型变得简单.</strong></p><p>在应用中，k值一般取一个比较小的数值.通常采用交叉验证法来选取最优的k值.</p><h2 id="分类决策规则"><a href="#分类决策规则" class="headerlink" title="分类决策规则"></a>分类决策规则</h2><p>k近邻法中的分类决策规则往往是多数表决，即由输入实例的k个邻近的训练实例中的多数类决定输入实例的类.</p><h1 id="k近邻法的实现：kd树"><a href="#k近邻法的实现：kd树" class="headerlink" title="k近邻法的实现：kd树"></a>k近邻法的实现：kd树</h1><p>实现k近邻法时,主要考虑的问题是如何对训练数据进行快速k近邻搜索.这点在特征空间的维数大及训练数据容量大时尤其必要.</p><p>k近邻法最简单的实现方法是线性扫描(linear scan).这时要计算输入实例与每一个训练实例的距离.当训练集很大时，计算非常耗时,这种方法是不可行的.</p><p>为了提高k近邻搜索的效率，可以考虑<strong>使用特殊的结构存储训练数据</strong>,以<strong>减少计算距离的次数</strong>.比如kd tree方法。</p><h2 id="构造kd树"><a href="#构造kd树" class="headerlink" title="构造kd树"></a>构造kd树</h2><p>kd树是一种对k维空间中的实例点进行存储以便对其进行快速检索的<strong>树形数据结构</strong>. kd 树是<strong>二叉树</strong>，表示对k维空间的一个<strong>划分</strong>(patition). 构造kd树相当于不断地用垂直于坐标轴的超平面将k维空间切分,构成一系列的k维超矩形区域、kd树的每个结点对应于一个k维超矩形区域.</p><p>通常,依次选择坐标轴对空间切分，选择训练实例点在选定坐标轴上的中位数(median)为切分点，这样得到的kd树是平衡的.注意，平衡的kd树搜索时的效率未必是最优的.<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%20%20k%E8%BF%91%E9%82%BB%E6%B3%95/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-24%20%E4%B8%8B%E5%8D%887.03.29.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%20%20k%E8%BF%91%E9%82%BB%E6%B3%95/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-24%20%E4%B8%8B%E5%8D%887.03.48.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%20%20k%E8%BF%91%E9%82%BB%E6%B3%95/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-24%20%E4%B8%8B%E5%8D%887.04.07.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><h2 id="搜索kd树"><a href="#搜索kd树" class="headerlink" title="搜索kd树"></a>搜索kd树</h2><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%20%20k%E8%BF%91%E9%82%BB%E6%B3%95/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-24%20%E4%B8%8B%E5%8D%887.05.52.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>利用kd树可以省去对大部分数据点的搜索，从而减少搜索的计算量。以最近邻为例，给定一个目标点，搜索其最近邻.首先找到包含目标点的叶结点;然后从该叶结点出发,依次回退到父结点;不断查找与目标点最邻近的结点，当确定不可能存在更近的结点时终止.这样搜索就被限制在空间的局部区域上，效率大为提高.</p><p>如果实例点是随机分布的, kd树搜索的平均计算复杂度是O(logN),这里N是训练实例数. kd树更适用于训练实例数远大于空间维数时的k近邻搜索.当空间维数接近训练实例数时，它的效率会迅速下降，几乎接近线性扫描.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;统计学习方法-k近邻法&quot;&gt;&lt;a href=&quot;#统计学习方法-k近邻法&quot; class=&quot;headerlink&quot; title=&quot;统计学习方法  k近邻法&quot;&gt;&lt;/a&gt;统计学习方法  k近邻法&lt;/h1&gt;&lt;p&gt;k近邻法(k-nearest neighbor, k-NN)是一
      
    
    </summary>
    
      <category term="读书笔记" scheme="https://github.com/zdkswd/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="机器学习" scheme="https://github.com/zdkswd/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>莫凡 sklearn</title>
    <link href="https://github.com/zdkswd/2019/03/22/%E8%8E%AB%E5%87%A1%20sklearn/"/>
    <id>https://github.com/zdkswd/2019/03/22/莫凡 sklearn/</id>
    <published>2019-03-22T12:12:56.000Z</published>
    <updated>2019-03-22T12:12:54.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="莫凡-sklearn"><a href="#莫凡-sklearn" class="headerlink" title="莫凡 sklearn"></a>莫凡 sklearn</h1><p>Sklearn 包含了很多种机器学习的方式:</p><p>Classification 分类<br>Regression 回归<br>Clustering 非监督分类<br>Dimensionality reduction 数据降维<br>Model Selection 模型选择<br>Preprocessing 数据预处理<br>我们总能够从这些方法中挑选出一个适合于自己问题的, 然后解决自己的问题.</p><p>看图选方法<br>由图中，可以看到算法有四类，分类，回归，聚类，降维。<figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%8E%AB%E5%87%A1%20sklearn/2_1_1.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><h1 id="交叉验证"><a href="#交叉验证" class="headerlink" title="交叉验证"></a>交叉验证</h1><h2 id="Model基础验证法"><a href="#Model基础验证法" class="headerlink" title="Model基础验证法"></a>Model基础验证法</h2><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%8E%AB%E5%87%A1%20sklearn/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-22%20%E4%B8%8B%E5%8D%885.05.50.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h2 id="Model交叉验证法"><a href="#Model交叉验证法" class="headerlink" title="Model交叉验证法"></a>Model交叉验证法</h2><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%8E%AB%E5%87%A1%20sklearn/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-22%20%E4%B8%8B%E5%8D%885.08.19.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>其中scoring还可以取neg_mean_squared_error,则返回的就是平均方差的数组。</p><p>sklearn.learning_curve 中的 learning curve 可以很直观的看出我们的 model 学习的进度, 对比发现有没有 overfitting 的问题. 然后我们可以对我们的 model 进行调整, 克服 overfitting 的问题.<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%8E%AB%E5%87%A1%20sklearn/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-22%20%E4%B8%8B%E5%8D%885.20.33.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%8E%AB%E5%87%A1%20sklearn/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-22%20%E4%B8%8B%E5%8D%885.20.44.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>连续三节的交叉验证(cross validation)让我们知道在机器学习中验证是有多么的重要, 这一次的 sklearn 中我们用到了sklearn.learning_curve当中的另外一种, 叫做validation_curve,用这一种曲线我们就能更加直观看出改变模型中的参数的时候有没有过拟合(overfitting)的问题了. 这也是可以让我们更好的选择参数的方法.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;莫凡-sklearn&quot;&gt;&lt;a href=&quot;#莫凡-sklearn&quot; class=&quot;headerlink&quot; title=&quot;莫凡 sklearn&quot;&gt;&lt;/a&gt;莫凡 sklearn&lt;/h1&gt;&lt;p&gt;Sklearn 包含了很多种机器学习的方式:&lt;/p&gt;
&lt;p&gt;Classifi
      
    
    </summary>
    
      <category term="知识总结" scheme="https://github.com/zdkswd/categories/%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/"/>
    
    
      <category term="机器学习" scheme="https://github.com/zdkswd/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>译 Numpy Vs Pandas 表现比较</title>
    <link href="https://github.com/zdkswd/2019/03/22/%E8%AF%91%20Numpy%20Vs%20Pandas%20%E8%A1%A8%E7%8E%B0%E6%AF%94%E8%BE%83/"/>
    <id>https://github.com/zdkswd/2019/03/22/译 Numpy Vs Pandas 表现比较/</id>
    <published>2019-03-22T02:50:56.000Z</published>
    <updated>2019-03-22T02:51:08.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="译-Numpy-Vs-Pandas-表现比较"><a href="#译-Numpy-Vs-Pandas-表现比较" class="headerlink" title="译 Numpy Vs Pandas 表现比较"></a>译 Numpy Vs Pandas 表现比较</h1><p>原文链接：<a href="http://gouthamanbalaraman.com/blog/numpy-vs-pandas-comparison.html" target="_blank" rel="noopener">http://gouthamanbalaraman.com/blog/numpy-vs-pandas-comparison.html</a></p><ol><li>Numpy比起Pandas消耗更少的内存</li><li>对于5w行或更少的数据，Numpy的表现普遍要好。</li><li>对于50w行或更多的数据，pandas的表现普遍要好。</li><li>对于5w到50w行的数据，就要取决于使用哪种操作。</li></ol><p>对于15MM行的数据，pandas要使用内存1506m，Numpy要使用内存686m，pandas的内存要求是Numpy的两倍多。</p><h1 id="对列进行操作"><a href="#对列进行操作" class="headerlink" title="对列进行操作"></a>对列进行操作</h1><p>聚合操作mean，Numpy与pandas速度的比较。分界点在于10w行。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%AF%91%20Numpy%20Vs%20Pandas%20%E8%A1%A8%E7%8E%B0%E6%AF%94%E8%BE%83/download.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><p>对于向量化操作符log，10w行以下Numpy更快，对于10w行以上两者差不多，但是pandas占用的内存要更大。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%AF%91%20Numpy%20Vs%20Pandas%20%E8%A1%A8%E7%8E%B0%E6%AF%94%E8%BE%83/download%202.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><p>对于去重函数，pandas使用unique，numpy使用species。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%AF%91%20Numpy%20Vs%20Pandas%20%E8%A1%A8%E7%8E%B0%E6%AF%94%E8%BE%83/download%203.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><h1 id="对于有过滤条件的操作"><a href="#对于有过滤条件的操作" class="headerlink" title="对于有过滤条件的操作"></a>对于有过滤条件的操作</h1><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%AF%91%20Numpy%20Vs%20Pandas%20%E8%A1%A8%E7%8E%B0%E6%AF%94%E8%BE%83/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-22%20%E4%B8%8A%E5%8D%8810.17.54.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%AF%91%20Numpy%20Vs%20Pandas%20%E8%A1%A8%E7%8E%B0%E6%AF%94%E8%BE%83/download%204.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%AF%91%20Numpy%20Vs%20Pandas%20%E8%A1%A8%E7%8E%B0%E6%AF%94%E8%BE%83/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-22%20%E4%B8%8A%E5%8D%8810.20.34.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%AF%91%20Numpy%20Vs%20Pandas%20%E8%A1%A8%E7%8E%B0%E6%AF%94%E8%BE%83/download%205.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h1 id="对于列的向量化操作"><a href="#对于列的向量化操作" class="headerlink" title="对于列的向量化操作"></a>对于列的向量化操作</h1><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%AF%91%20Numpy%20Vs%20Pandas%20%E8%A1%A8%E7%8E%B0%E6%AF%94%E8%BE%83/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-22%20%E4%B8%8A%E5%8D%8810.23.26.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%AF%91%20Numpy%20Vs%20Pandas%20%E8%A1%A8%E7%8E%B0%E6%AF%94%E8%BE%83/download%206.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%AF%91%20Numpy%20Vs%20Pandas%20%E8%A1%A8%E7%8E%B0%E6%AF%94%E8%BE%83/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-22%20%E4%B8%8A%E5%8D%8810.24.00.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%AF%91%20Numpy%20Vs%20Pandas%20%E8%A1%A8%E7%8E%B0%E6%AF%94%E8%BE%83/download%207.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h1 id="留下一个疑问"><a href="#留下一个疑问" class="headerlink" title="留下一个疑问"></a>留下一个疑问</h1><p>pandas对于大量行的数据做了哪些优化，为什么性能得到了提升？</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;译-Numpy-Vs-Pandas-表现比较&quot;&gt;&lt;a href=&quot;#译-Numpy-Vs-Pandas-表现比较&quot; class=&quot;headerlink&quot; title=&quot;译 Numpy Vs Pandas 表现比较&quot;&gt;&lt;/a&gt;译 Numpy Vs Pandas 表现
      
    
    </summary>
    
      <category term="知识总结" scheme="https://github.com/zdkswd/categories/%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/"/>
    
    
      <category term="Python" scheme="https://github.com/zdkswd/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>莫凡 使用Numpy的技巧</title>
    <link href="https://github.com/zdkswd/2019/03/22/%E8%8E%AB%E5%87%A1%20%E4%BD%BF%E7%94%A8Numpy%E7%9A%84%E6%8A%80%E5%B7%A7/"/>
    <id>https://github.com/zdkswd/2019/03/22/莫凡 使用Numpy的技巧/</id>
    <published>2019-03-22T01:30:12.000Z</published>
    <updated>2019-03-22T11:01:57.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="莫凡-使用Numpy的技巧"><a href="#莫凡-使用Numpy的技巧" class="headerlink" title="莫凡 使用Numpy的技巧"></a>莫凡 使用Numpy的技巧</h1><p>pandas要比Numpy慢，所以尽量避免使用Pandas。</p><h1 id="为什么用Numpy？"><a href="#为什么用Numpy？" class="headerlink" title="为什么用Numpy？"></a>为什么用Numpy？</h1><p>Python 是慢的, 简单来说, 因为 Python 执行代码的时候会执行很多复杂的“check” 功能, 比如赋值<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%8E%AB%E5%87%A1%20%E4%BD%BF%E7%94%A8Numpy%E7%9A%84%E6%8A%80%E5%B7%A7/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-20%20%E4%B8%8B%E5%8D%889.42.23.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>在计算机内部, b 首先要从一个整数 integer 转换成浮点数 float, 才能进行后面的 b/0.5, 因为得到的要是一个小数.提到 Numpy, 它就是一个 Python 的救星. 能把简单好用的 Python 和高性能的 C 语言合并在一起. 当你调用 Numpy 功能的时候, 他其实调用了很多 C 语言而不是纯 Python. 这就是为什么大家都爱用 Numpy 的原因.</p><h1 id="创建Numpy-Array的结构"><a href="#创建Numpy-Array的结构" class="headerlink" title="创建Numpy Array的结构"></a>创建Numpy Array的结构</h1><p>其实 Numpy 就是 C 的逻辑, 创建存储容器 “Array” 的时候是寻找内存上的一连串区域来存放, 而 Python 存放的时候则是不连续的区域, 这使得 Python 在索引这个容器里的数据时不是那么有效率. Numpy 只需要再这块固定的连续区域前后走走就能不费吹灰之力拿到数据.<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%8E%AB%E5%87%A1%20%E4%BD%BF%E7%94%A8Numpy%E7%9A%84%E6%8A%80%E5%B7%A7/4-1-2.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>在运用 Numpy 的时候, 我们通常不是用一个一维 Array 来存放数据, 而是用二维或者三维的块来存放<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%8E%AB%E5%87%A1%20%E4%BD%BF%E7%94%A8Numpy%E7%9A%84%E6%8A%80%E5%B7%A7/4-1-3.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>因为 Numpy 快速的<strong>矩阵相乘</strong>运算, 能将乘法运算分配到计算机中的多个核, 让运算并行. 这种并行运算大大加速了运算速度.</p><p>不管是1D，2D，3D 的 Array, 从根本上, 它都是一个 1D array。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%8E%AB%E5%87%A1%20%E4%BD%BF%E7%94%A8Numpy%E7%9A%84%E6%8A%80%E5%B7%A7/4-1-4.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>2D Array, 如果追溯到计算机内存里, 它其实是储存在一个连续空间上的. 而对于这个连续空间, 我们如果创建 Array 的方式不同, 在这个连续空间上的排列顺序也有不同. 这将影响之后所有的事情!</p><p>在 Numpy 中, 创建 2D Array 的默认方式是 “C-type” 以 row 为主在内存中排列, 而如果是 “Fortran” 的方式创建的, 就是以 column 为主在内存中排列.<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%8E%AB%E5%87%A1%20%E4%BD%BF%E7%94%A8Numpy%E7%9A%84%E6%8A%80%E5%B7%A7/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-21%20%E4%B8%8B%E5%8D%886.52.43.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><h1 id="在-Axis-上的动作"><a href="#在-Axis-上的动作" class="headerlink" title="在 Axis 上的动作"></a>在 Axis 上的动作</h1><p>当你的计算中涉及合并矩阵, 不同形式的矩阵创建方式会有不同的时间效果. 因为在 Numpy 中的矩阵合并等, 都是发生在一维空间里 ! 不是二维空间中!</p><p>从上面的那张图, 可以想到, row 为主的存储方式, 如果在 row 的方向上合并矩阵, 将会更快. 因为只要我们将思维放在 1D array 那, 直接再加一个 row 放在1D array 后面就好了。</p><p>但是在以 column 为主的系统中, 往 1D array 后面加 row 的规则变复杂了, 消耗的时间也变长. 如果以 axis=1 的方式合并, “F” 方式将会比 “C” 方式更好.</p><p>在数组的拼接中，对比<strong>np.stack</strong>和<strong>np.concatenate</strong>,通过测试发现，为了速度，尽量使用<strong>np.concatenate</strong>。</p><h1 id="copy慢，view快"><a href="#copy慢，view快" class="headerlink" title="copy慢，view快"></a>copy慢，view快</h1><p>在 Numpy 中, 有两个很重要的概念, copy 和 view. copy 顾名思义, 会将数据 copy 出来存放在内存中另一个地方, 而 view 则是不 copy 数据, 直接取源数据的索引部分.<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%8E%AB%E5%87%A1%20%E4%BD%BF%E7%94%A8Numpy%E7%9A%84%E6%8A%80%E5%B7%A7/4-1-5.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>简单说, a_view 的东西全部都是 a 的东西, 动 a_view 的任何地方, a 都会被动到, 因为他们在内存中的位置是一模一样的, 本质上就是自己. 而 a_copy 则是将 a copy 了一份, 然后把 a_copy 放在内存中的另外的地方, 这样改变 a_copy, a 是不会被改变的.</p><p>因为 view 不会复制东西, 速度快。</p><p>对于 view 还有一点要提,<strong>把一个矩阵展平</strong>, 用到 <strong>np.flatten()</strong> 或者<strong>np.ravel()</strong>. 他俩是不同的!ravel 返回的是一个 view (官方说如果用 ravel, 需要 copy 的时候才会被 copy , 我想这个时候可能是把 ravel 里面 order 转换的时候, 如 ‘C-type’ -&gt; ‘Fortran’), 而 flatten 返回的总是一个 copy. 相比于 flatten, <strong>ravel</strong> 是神速.</p><h1 id="选择数据"><a href="#选择数据" class="headerlink" title="选择数据"></a>选择数据</h1><p>选择数据的时候, 我们常会用到 view 或者 copy 的形式. 我们知道了, 如果能用到 view 的, 我们就尽量用 view, 避免 copy 数据.下面举例的都是 view 的方式:<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%8E%AB%E5%87%A1%20%E4%BD%BF%E7%94%A8Numpy%E7%9A%84%E6%8A%80%E5%B7%A7/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-22%20%E4%B8%8A%E5%8D%889.10.18.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>使用copy的方式：<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%8E%AB%E5%87%A1%20%E4%BD%BF%E7%94%A8Numpy%E7%9A%84%E6%8A%80%E5%B7%A7/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-22%20%E4%B8%8A%E5%8D%889.10.48.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>对于fancy indexing的形式，我们也是可以对它加速。<br>1.使用<strong>np.take()</strong>, 替代用 index 选数据的方法.<br>像 a_copy1 = a[ [1,4,6] ,  [2,4,6] ], 用 take 在大部分情况中会比这样的 a_copy1 要快.<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%8E%AB%E5%87%A1%20%E4%BD%BF%E7%94%A8Numpy%E7%9A%84%E6%8A%80%E5%B7%A7/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-22%20%E4%B8%8A%E5%8D%889.15.16.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>2 使用 np.compress(), 替代用 mask 选数据的方法.<br>a_copy2 = a[ [ True, True] ,  [ False, True]] 这种就是用 TRUE, FALSE 来选择数据的.<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%8E%AB%E5%87%A1%20%E4%BD%BF%E7%94%A8Numpy%E7%9A%84%E6%8A%80%E5%B7%A7/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-22%20%E4%B8%8A%E5%8D%889.17.11.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><h1 id="非常有用的out参数"><a href="#非常有用的out参数" class="headerlink" title="非常有用的out参数"></a>非常有用的out参数</h1><p>下面两个其实在功能上是没差的, 不过运算时间上有差, 我觉得可能是 a=a+1 要先转换成 np.add() 这种形式再运算, 所以前者要用更久一点的时间.<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%8E%AB%E5%87%A1%20%E4%BD%BF%E7%94%A8Numpy%E7%9A%84%E6%8A%80%E5%B7%A7/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-22%20%E4%B8%8A%E5%8D%889.19.23.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>这两个被赋值的 a, 都是原来 a 的一个 copy, 并不是 a 的 view. 但是在功能里面有一个 out 参数, 让我们不必要重新创建一个 a. 所以下面两个是一样的功能, 都不会创建另一个 copy. 可能是上面提到的那个原因, 这里的运算时间也有差.<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%8E%AB%E5%87%A1%20%E4%BD%BF%E7%94%A8Numpy%E7%9A%84%E6%8A%80%E5%B7%A7/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-22%20%E4%B8%8A%E5%8D%889.22.03.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>所以只要是已经存在了一个 placeholder (比如 a), 我们就没有必要去再创建一个, 用 out 方便又有效.</p><h1 id="给数据一个名字"><a href="#给数据一个名字" class="headerlink" title="给数据一个名字"></a>给数据一个名字</h1><p>使用Numpy的structured array.</p><p>pandas 为什么比 numpy 慢, 因为 pandas data 里面还有很多七七八八的数据, 记录着这个 data 的种种其他的特征. </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;莫凡-使用Numpy的技巧&quot;&gt;&lt;a href=&quot;#莫凡-使用Numpy的技巧&quot; class=&quot;headerlink&quot; title=&quot;莫凡 使用Numpy的技巧&quot;&gt;&lt;/a&gt;莫凡 使用Numpy的技巧&lt;/h1&gt;&lt;p&gt;pandas要比Numpy慢，所以尽量避免使用Pan
      
    
    </summary>
    
      <category term="知识总结" scheme="https://github.com/zdkswd/categories/%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/"/>
    
    
      <category term="Python" scheme="https://github.com/zdkswd/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>百面 模型评估</title>
    <link href="https://github.com/zdkswd/2019/03/20/%E7%99%BE%E9%9D%A2%20%20%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/"/>
    <id>https://github.com/zdkswd/2019/03/20/百面  模型评估/</id>
    <published>2019-03-20T05:57:47.000Z</published>
    <updated>2019-04-02T10:33:07.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="百面-模型评估"><a href="#百面-模型评估" class="headerlink" title="百面  模型评估"></a>百面  模型评估</h1><p>模型评估主要分为离线评估和在线评估两个阶段。针对分类、排序、回归、序列预测等不同类型的机器学习问题，评估指标的选择也有所不同。知道每种评估指标的精确定义、有针对性地选择合适的评估指标、根据评估指标的反馈进行模型调整，这些都是机器学习在模型评估阶段的关键问题，也是一名合格的算法工程师应当具备的基本功。</p><h1 id="评估指标的局限性"><a href="#评估指标的局限性" class="headerlink" title="评估指标的局限性"></a>评估指标的局限性</h1><p>在模型评估过程中，分类问题、排序问题、回归问题往往需要使用不同的指标进行评估。在诸多的评估指标中，大部分指标只能片面地反映模型的一部分性能。如果不能合理地运用评估指标，不仅不能发现模型本身的问题，而且会得出错误的结论。</p><h2 id="问题1-准确率-Accuracy-的局限性"><a href="#问题1-准确率-Accuracy-的局限性" class="headerlink" title="问题1  准确率(Accuracy)的局限性"></a>问题1  准确率(Accuracy)的局限性</h2><p>问：Hulu的奢侈品广告主们希望把广告定向投放给奢侈品用户。Hulu通过第三方的数据管理平台(Data Management Platform, DMP)拿到了一部分奢侈品用户的数据，并以此为训练集和测试集，训练和测试奢侈品用户的分类模型。该模型的分类准确率超过了95%，但在实际广告投放过程中，该模型还是把大部分广告投给了非奢侈品用户，这可能是什么原因造成的?</p><p>答：准确率是指分类正确的样本占总样本个数的比例，即<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E7%99%BE%E9%9D%A2%20%20%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/page40image4976.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>其中n_corrrct为被正确分类的样本个数，n_total为总样本的个数。</p><p>准确率是分类问题中最简单也是最直观的评价指标，但存在明显的缺陷。比如，当负样本占99%时，分类器把所有样本都预测为负样本也可以获得99%的准确率。所以，<strong>当不同类别的样本比例非常不均衡时，占比大的类别往往成为影响准确率的最主要因素。</strong></p><p>明确了这一点，这个问题也就迎刃而解了。显然，奢侈品用户只占Hulu全体用户的一小部分，虽然模型的整体分类准确率高，但是不代表对奢侈品用户的分类准确率也很高。在线上投放过程中，我们只会对模型判定的“奢侈品用户”进行投放，因此，对“奢侈品用户”判定的准确率不够高的问题就被放大了。为了解决这个问题，可以使用更为有效的平均准确率(每个类别下的样本准确率的算术平均)作为模型评估的指标。</p><h2 id="问题2-精确率-Precision-与召回率的权衡"><a href="#问题2-精确率-Precision-与召回率的权衡" class="headerlink" title="问题2  精确率(Precision)与召回率的权衡"></a>问题2  精确率(Precision)与召回率的权衡</h2><p>问题描述：Hulu提供视频的模糊搜索功能，搜索排序模型返回的Top5的精确率非常高,但在实际使用过程中，用户还是经常找不到想要的视频，特别是一些比较冷门的剧集，这可能是哪个环节出了问题呢?</p><p>答：<strong>精确率</strong>是指<strong>分类正确的正样本个数占分类器判定为正样本的样本</strong>个数的比例。描述返回值的质量。<strong>召回率</strong>是指<strong>分类正确的正样本个数占真正的正样本</strong>个数的比例。</p><p>在排序问题中，通常没有一个确定的阈值把得到的结果直接判定为正样本或负样本，而是采用Top N返回结果的Precision值和Recall值来衡量排序模型的性能，即认为模型返回的Top N的结果就是模型判定的正样本，然后计算前N个位置上的准确率Precision@N和前N个位置上的召回率Recall@N。</p><p>Precision值和Recall值是既矛盾又统一的两个指标，为了提高<strong>Precision</strong>值，分类器需要尽量在“<strong>更有把握</strong>”时才把样本预测为正样本,但此时往往会因为过于保守而漏掉很多“没有把握”的正样本，导致<strong>Recall</strong>值降低。</p><p>模型返回的Precision@5的结果非常好，也就是说排序模型Top 5的返回值的质量是很高的。但在实际应用过程中，用户为了找一些冷门的视频,往往会寻找排在较靠后位置的结果，甚至翻页去查找目标视频。但根据题目描述，用户经常找不到想要的视频，这说明模型没有把相关的视频都找出来呈现给用户。显然，问题出在<strong>召回率</strong>上。如果相关结果有100个，即使Precision@5达到了100%，Recall@5也仅仅是5%。在模型评估时，我们是否应该同时关注Precision值和Recall值。进一步而言， 应该选取不同的Top N的结果进行观察，应该选取更高阶的评估指标来更全面地反映模型在Precision值和Recall值两方面的表现。最好绘制出模型的PR曲线。</p><p><strong>P-R曲线</strong>的横轴是召回率，纵轴是精确率。对于一个排序模型来说，其P-R曲线上的一个点代表着，在某一阈值下，模型将大于该阈值的结果判定为正样本，小于该阈值的结果判定为负样本，此时返回结果对应的召回率和精确率。整条P-R.曲线是通过将阈值从高到低移动而生成的。图P-R曲线样例图中，其中实线代表模型A的P-R曲线，虚线代表模型B的P-R曲线。原点附近代表当阈值最大时模型的精确率和召回率。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E7%99%BE%E9%9D%A2%20%20%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-19%20%E4%B8%8B%E5%8D%887.32.06.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>由图可见，当召回率接近于0时，模型A的精确率为0.9，模型B的精确率是1,这说明模型B得分前几位的样本全部是真正的正样本，而模型A即使得分最高的几个样本也存在预测错误的情况。并且，随着召回率的增加，精确率整体呈下降趋势。但是，当召回率为1时，模型A的精确率反而超过了模型B。这充分说明，<strong>只用某个点对应的精确率和召回率是不能全面地衡量模型的性能，只有通过PR曲线的整体表现，才能够对模型进行更为全面的评估。</strong></p><p>F1 score能综合反映一个排序模型的性能。F1 score是精确率和召回率的调和平均值，定义为：<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E7%99%BE%E9%9D%A2%20%20%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/page43image400.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>F1分数可以看作是模型准确率和召回率的一种加权平均，它的最大值是1，最小值是0。</p><p>更一般的，我们定Fβ分数为<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E7%99%BE%E9%9D%A2%20%20%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/c2cec3fdfc039245afef86848594a4c27c1e25e6.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><p>Fβ的物理意义就是将准确率和召回率这两个分值合并为一个分值，在合并的过程中，召回率的权重是准确率的β倍。F1分数认为召回率和准确率同等重要，F2分数认为召回率的重要程度是准确率的2倍，而F0.5分数认为召回率的重要程度是准确率的一半。</p><h2 id="问题3-平方根误差的“意外”"><a href="#问题3-平方根误差的“意外”" class="headerlink" title="问题3   平方根误差的“意外”"></a>问题3   平方根误差的“意外”</h2><p>问题描述：Hulu作为一家流媒体公司，拥有众多的美剧资源，预测每部美剧的流量趋势对于广告投放、用户增长都非常重要。我们希望构建一个回归模型来预测某部美剧的流量趋势，但无论采用哪种回归模型，得到的RMSE指标都非常高。然而事实是，模型在95%的时间区间内的预测误差都小于1%，取得了相当不错的预测结果。那么，造成RMSE指标居高不下的最可能的原因是什么?</p><p>答：RMSE经常被用来衡量回归模型的好坏，但根据题目的叙述，RMSE这个指标却失效了。RMSE的计算公式如下：<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E7%99%BE%E9%9D%A2%20%20%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/page43image6176.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>其中，yi是第i个样本点的真实值，y帽i是第i个样本点的预测值，n是样本点的个数。</p><p>一般情况下，RMSE能够很好地反映回归模型预测值与真实值的偏离程度。但在实际问题中，如果存在个别偏离程度非常大的离群点(Outlier) 时，即使离群点数量非常少，也会让RMSE指标变得很差。 </p><p>回到问题中来，模型在95%的时间区间内的预测误差都小于1%，这说明，在大部分时间区间内，模型的预测效果都是非常优秀的。然而，RMSE却一直很差，这很可能是由于在其他的5%时间区间内存在非常严重的离群点。事实上，在流量预估这个问题中，噪声点确实是很容易产生的，比如流量特别小的美剧、刚上映的美剧或者刚获奖的美剧，甚至一-些相关社交媒体突发事件带来的流量，都可能会造成离群点。</p><p>针对该问题可以有三种不同的解决方案。1，如果认定这些离群点是噪声点的话，就需要在数据预处理的阶段把这些噪声点过滤掉。第二，如果不认为这些离群点是噪声点的话，就需要进一步提高模型的预测能力，将离散点产生的机制建模进去。第三，可以找一个更合适的指标来评估该模型。关于评估指标，其实是存在比RMSE的鲁棒性更好的指标，比如平均绝对百分比误差，MAPE，定义为：<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E7%99%BE%E9%9D%A2%20%20%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/page44image3632.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>相比RMSE, MAPE相当于把每个点的误差进行了归一化，降低了个别离群点带来的绝对误差的影响。</p><h1 id="ROC曲线"><a href="#ROC曲线" class="headerlink" title="ROC曲线"></a>ROC曲线</h1><p>二值分类器( Binary Classifier)是机器学习领域中最常见也是应用最广泛的分类器。评价二值分类器的指标很多,比如 precision、 recall F1 score、PR曲线等。上一小节已对这些指标做了一定的介绍,但也发现这些指标或多或少只能反映模型在某一方面的性能。相比而言,ROC曲线则有很多优点,经常作为评估二值分类器最重要的指标之一。</p><h2 id="问题1-什么是ROC曲线？"><a href="#问题1-什么是ROC曲线？" class="headerlink" title="问题1  什么是ROC曲线？"></a>问题1  什么是ROC曲线？</h2><p>ROC曲线的横坐标为假阳性率(False Positive Rate，FPR) ;纵坐标为真阳性率, (True Positive Rate, TPR)。FPR和TPR的计算方法分别为<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E7%99%BE%E9%9D%A2%20%20%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/page45image4736.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E7%99%BE%E9%9D%A2%20%20%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/page45image6248.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>上式中，P是真实的正样本的数量，N是真实的负样本的数量，TP是P个正样本中被分类器预测为正样本的个数，FP是N个负样本中被分类器预测为正样本的个数。<strong>发现TPR与Recall是相等的。</strong>FPR即为误诊率，公式为<br>（N被认为P） / N。</p><p>举个例子，假设有10位疑似癌症患者，其中有3位很不幸确实患了癌症(P=3)，另外7位不是癌症患者(N=7) 。医院对这10位疑似患者做了诊断，诊断出3位癌症患者，其中有2位确实是真正的患者(TP=2) 。那么真阳性率TPR=TP \ P=2\ 3。对于7位非癌症患者来说，有一位很不幸被误诊为癌症患者(FP= 1)，那么假阳性率FPR=FP \ N=1\ 7。对于“该医院”这个分类器来说，这组分类结果就对应ROC曲线上的一个点(1\7, 2\3)。</p><h2 id="问题2-如何绘制ROC曲线？"><a href="#问题2-如何绘制ROC曲线？" class="headerlink" title="问题2 如何绘制ROC曲线？"></a>问题2 如何绘制ROC曲线？</h2><p>事实上，ROC曲线是通过不断移动分类器的“截断点”来生成曲线上的一组关键点的。</p><p>在二值分类问题中，模型的输出一般都是预测样本为正例的概率，所以下表中的概率值都为<strong>正值的概率</strong>，在输出最终的正例，负例之前，我们需要指定一个<strong>阈值</strong>，预测概率大于该阈值的样本会被判为正例，小于该阈值的样本则会被判为负例。<strong>截断点</strong>指的是区分<strong>正负预测结果的阈值</strong>。下图是模型的输出结果,样本按照预测概率从高到低排序。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/add/32.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure> </p><p>通过动态地调整截断点，从最高的得分开始(实际上是从正无穷开始，对应着ROC曲线的零点)，逐渐调整到最低得分，每一个截断点都会对应一个FPR和TPR，在ROC图上绘制出每个截断点对应的位置，再连接所有点就得到最终的ROC曲线。</p><p>例如下图的ROC曲线。<br>当截断点选择为正无穷时，模型把全部样本预测为负例，那么FP和TP必然都为0，FPR和TPR也都为0，因此曲线的第一个点的坐标就是(0,0)。当把截断点调整为0.9时，模型预测1号样本为正样本，并且该样本确实是正样本，因此，TP=1, 20个样本中，所有正例数量为P=10,故TPR=TP/ P=1/ 10;这里没有预测错的正样本，即FP=0, 负样本总数N=10,故FPR=FP /N=0 /10=0,对应ROC曲线上的点(0,0.1) 。依次调整截断点，直到画出全部的关键点，再连接关键点即得到最终的ROC曲线。<strong>不是说以假阳性率为自变量以真阳性率为因变量哦</strong>。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E7%99%BE%E9%9D%A2%20%20%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-20%20%E4%B8%8A%E5%8D%8810.09.37.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><p>还有一种更直观地绘制ROC曲线的方法。首先根据样本标签统计出正负样本的数量，假设正样本数量为P，负样本数量为N。接下来把横轴刻度间隔设置为1 / N,纵轴的刻度间隔设置为1 / P。再根据模型输出的预测概率对样本进行排序（从高到低）；依次遍历样本，同时从零点开始绘制ROC曲线，每遇到一个正样本就沿着纵轴方向绘制一个刻度间隔的曲线，每遇到一个负样本就沿横轴方向绘制一个刻度间隔的曲线，直到遍历完所有样本，曲线最终停在（1，1）整个ROC曲线绘制完成。这样就很好理解<strong>为什么面积越大，分类性能越好了</strong>，想象这个过程即可。</p><h2 id="问题3-如何计算AUC？"><a href="#问题3-如何计算AUC？" class="headerlink" title="问题3  如何计算AUC？"></a>问题3  如何计算AUC？</h2><p>AUC指的是ROC曲线下的面积大小，该值能够量化地反映基于ROC曲线衡量出的模型性能。ROC曲线一般都处于y=x这条直线的上方，如果不是的话，只要把模型预测的概率反转成1-p就可以得到一个更好的分类器。所以AUC的取值一般在0.5至1之间。<strong>AUC越大，说明分类器越可能把真正的正样本排在前面，分类性能越好。</strong>比如极端好，那就是条x=0这条直线啊。</p><h2 id="树的AUC的计算"><a href="#树的AUC的计算" class="headerlink" title="树的AUC的计算"></a>树的AUC的计算</h2><p>很多机器学习的模型对分类问题的预测结果都是概率，如果要计算accuracy，需要把概率转化为类别，这就需要手动设置一个阈值。可以结合着第一种画AUC的方法来看，对于决策树，这种，可以使用第二种方法比较好理解，当然都用第二种都比较好理解。</p><h2 id="AUC缺点"><a href="#AUC缺点" class="headerlink" title="AUC缺点"></a>AUC缺点</h2><p>AUC的最大问题在于它并不在乎所有实例的绝对预测数值，而只在乎它们的相对数值。</p><h2 id="问题4-ROC曲线相比PR曲线有什么特点？"><a href="#问题4-ROC曲线相比PR曲线有什么特点？" class="headerlink" title="问题4 ROC曲线相比PR曲线有什么特点？"></a>问题4 ROC曲线相比PR曲线有什么特点？</h2><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E7%99%BE%E9%9D%A2%20%20%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-20%20%E4%B8%8A%E5%8D%8810.37.59.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>将测试集中的负样本数量增加10倍后，可以看出PR曲线发生了明显的变化，而ROC曲线形状基本不变。这个特点让ROC曲线能够尽量降低不同测试集带来的干扰，更加客观地衡量模型本身的性能。实际问题中，正负样本的数量往往很不均衡。比如计算广告领域经常涉及转化率模型，正样本的数量往往是负样本数量的1比1000甚至1比10000。若选择不同的测试集，P-R曲线的变化就会非常大，而ROC曲线则能够更加稳定地反映模型本身的好坏。所以，ROC曲线的适用场景更多，被广泛用于排序、推荐、广告等领域。但需要注意的是，选择P-R曲线还是ROC曲线是因实际问题而异的，如果研究者希望更多地看到模型在特定数据集上的表现，P-R曲线则能够更直观地反映其性能。</p><h1 id="AB测试"><a href="#AB测试" class="headerlink" title="AB测试"></a>AB测试</h1><h2 id="问题1"><a href="#问题1" class="headerlink" title="问题1"></a>问题1</h2><p>问：在对模型进行过充分的离线评估后，为什么还要进行在线AB测试？</p><p>答：需要进行在线A/B测试的原因如下。<br>(1)离线评估无法完全消除模型过拟合的影响，因此，得出的离线评估结果无法完全替代线上评估结果。</p><p>(2)离线评估无法完全还原线上的工程环境。一般来讲， 离线评估往往不会考虑线上环境的延迟、数据丢失、标签数据缺失等情况。因此，离线评估的结果是理想工程环境下的结果。</p><p>(3)线上系统的某些商业指标在离线评估中无法计算。离线评估一般是针对模型本身进行评估，而与模型相关的其他指标，特别是商业指标，往往无法直接获得。比如，上线 了新的推荐算法，离线评估往往关注的是ROC曲线、P-R曲线等的改进，而线上评估可以全面了解该推荐算法带来的用户点击率、留存时长、PV访问量等的变化。这些都要由A/B测试来进行全面的评估。</p><h2 id="问题2-如何进行线上AB测试？"><a href="#问题2-如何进行线上AB测试？" class="headerlink" title="问题2  如何进行线上AB测试？"></a>问题2  如何进行线上AB测试？</h2><p>答：进行AB测试的主要手段是进行用户分桶，即将用户分成实验组和对照组，对实验组的用户施以新模型，对对照组的用户施以旧模型。在分桶的过程中，要注意<strong>样本的独立性</strong>和<strong>采样方式的无偏性</strong>，确保同一个用户每次只能分到同一个桶中，在分桶过程中所选取的user_id需要是一个随机数，这样才能保证桶中的样本是无偏的。</p><h1 id="模型评估的方法"><a href="#模型评估的方法" class="headerlink" title="模型评估的方法"></a>模型评估的方法</h1><p>在机器学习中，我们通常把样本分为训练集和测试集，训练集用于训练模型，测试集用于评估模型。在样本划分和模型验证的过程中，存在着不同的抽样方法和验证方法。</p><h2 id="问题1-1"><a href="#问题1-1" class="headerlink" title="问题1"></a>问题1</h2><p>问：在模型评估过程中，有哪些主要的验证方法，它们的优缺点是什么？</p><p>答：<br>（1）<strong>Holdout检验</strong><br>Holdout检验是最简单也是最直接的验证方法，它将原始的样本集合随机划分成训练集和验证集两部分。比方说，对于一个点击率预测模型，我们把样本按照70%~30%的比例分成两部分，70%的样本用于模型训练; 30%的样本用于模型验证，包括绘制ROC曲线、计算精确率和召回率等指标来评估模型性能。</p><p>Holdout检验的缺点很明显，即在验证集上计算出来的最后评估指标与原始分组有很大关系。为了消除随机性，研究者们引入了“交叉检验”的思想。<br>（2）<strong>交叉检验</strong><br>k-fold交叉验证:首先将全部样本划分成k个大小相等的样本子集;依次遍历这k个子集，每次把当前子集作为验证集，其余所有子集作为训练集，进行模型的训练和评估;最后把k次评估指标的平均值作为最终的评估指标。在实际实验中，经常取10。</p><p>（3）<strong>自助法</strong><br>不管是Holdout检验还是交叉检验，都是基于划分训练集和测试集的方法进行模型评估的。然而，<strong>当样本规模比较小时</strong>，将样本集进行划分会让训练集进一步减小，这可能会影响模型训练效果。</p><p>自助法是基于自助采样法的检验方法。对于总数为n的样本集合，进行n次有放回的随机抽样，得到大小为n的训练集。n次采样过程中，有的样本会被重复采样，有的样本没有被抽出过，将这些没有被抽出的样本作为验证集,进行模型验证，这就是自助法的验证过程。</p><h2 id="问题2"><a href="#问题2" class="headerlink" title="问题2"></a>问题2</h2><p>问：在自助法的采样过程中，对n个样本进行n次自助抽样，当n趋于无穷大时，最终有多少数据从未被选择过?<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E7%99%BE%E9%9D%A2%20%20%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-20%20%E4%B8%8B%E5%8D%881.55.13.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>根据重要极限，当样本量很大时，大约有0.368的样本从未被选择过，可做为验证集。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;百面-模型评估&quot;&gt;&lt;a href=&quot;#百面-模型评估&quot; class=&quot;headerlink&quot; title=&quot;百面  模型评估&quot;&gt;&lt;/a&gt;百面  模型评估&lt;/h1&gt;&lt;p&gt;模型评估主要分为离线评估和在线评估两个阶段。针对分类、排序、回归、序列预测等不同类型的机器学习问
      
    
    </summary>
    
      <category term="知识总结" scheme="https://github.com/zdkswd/categories/%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/"/>
    
    
      <category term="机器学习" scheme="https://github.com/zdkswd/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="模型评估" scheme="https://github.com/zdkswd/tags/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/"/>
    
  </entry>
  
  <entry>
    <title>CNN 个人总结</title>
    <link href="https://github.com/zdkswd/2019/03/19/CNN%20%E4%B8%AA%E4%BA%BA%E6%80%BB%E7%BB%93/"/>
    <id>https://github.com/zdkswd/2019/03/19/CNN 个人总结/</id>
    <published>2019-03-19T07:45:12.000Z</published>
    <updated>2019-03-19T11:37:23.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="尺寸问题"><a href="#尺寸问题" class="headerlink" title="尺寸问题"></a>尺寸问题</h1><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/CNN%20%E4%B8%AA%E4%BA%BA%E6%80%BB%E7%BB%93/20180404150134375.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>图中，输入层个数为1，深度为3，所以每个filter对应深度也要为。filter数量为2，对应输出层的深度为2，个数为1。</p><p>即输入层的深度与filter深度相等，输出层深度与filter个数相等。</p><h1 id="卷积层的梯度传递"><a href="#卷积层的梯度传递" class="headerlink" title="卷积层的梯度传递"></a>卷积层的梯度传递</h1><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/CNN%20%E4%B8%AA%E4%BA%BA%E6%80%BB%E7%BB%93/2256672-52295dad2641037f.png" alt="图1" title="">                </div>                <div class="image-caption">图1</div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/CNN%20%E4%B8%AA%E4%BA%BA%E6%80%BB%E7%BB%93/2256672-2fb37b0a3ff0e1f9.png" alt="图2" title="">                </div>                <div class="image-caption">图2</div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/CNN%20%E4%B8%AA%E4%BA%BA%E6%80%BB%E7%BB%93/2256672-754f37eb7603e99f.png" alt="图3" title="">                </div>                <div class="image-caption">图3</div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/CNN%20%E4%B8%AA%E4%BA%BA%E6%80%BB%E7%BB%93/2256672-af2da9701a03dc3c.png" alt="图4" title="">                </div>                <div class="image-caption">图4</div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/CNN%20%E4%B8%AA%E4%BA%BA%E6%80%BB%E7%BB%93/WechatIMG601552981169_.pic_hd.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h1 id="卷积层filter权重梯度的计算"><a href="#卷积层filter权重梯度的计算" class="headerlink" title="卷积层filter权重梯度的计算"></a>卷积层filter权重梯度的计算</h1><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/CNN%20%E4%B8%AA%E4%BA%BA%E6%80%BB%E7%BB%93/2256672-afe6d3a863b7cbcc.png" alt="图5" title="">                </div>                <div class="image-caption">图5</div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/CNN%20%E4%B8%AA%E4%BA%BA%E6%80%BB%E7%BB%93/WechatIMG571552981164_.pic.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h1 id="Pooling层的训练"><a href="#Pooling层的训练" class="headerlink" title="Pooling层的训练"></a>Pooling层的训练</h1><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/CNN%20%E4%B8%AA%E4%BA%BA%E6%80%BB%E7%BB%93/2256672-af77e98c09fad84c.png" alt="图6" title="">                </div>                <div class="image-caption">图6</div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/CNN%20%E4%B8%AA%E4%BA%BA%E6%80%BB%E7%BB%93/2256672-c3a6772cb07b416a.png" alt="图7" title="">                </div>                <div class="image-caption">图7</div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/CNN%20%E4%B8%AA%E4%BA%BA%E6%80%BB%E7%BB%93/WechatIMG591552981167_.pic_hd.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h1 id="CNN的反向传播"><a href="#CNN的反向传播" class="headerlink" title="CNN的反向传播"></a>CNN的反向传播</h1><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/CNN%20%E4%B8%AA%E4%BA%BA%E6%80%BB%E7%BB%93/WechatIMG581552981167_.pic.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;尺寸问题&quot;&gt;&lt;a href=&quot;#尺寸问题&quot; class=&quot;headerlink&quot; title=&quot;尺寸问题&quot;&gt;&lt;/a&gt;尺寸问题&lt;/h1&gt;&lt;figure class=&quot;image-bubble&quot;&gt;
                &lt;div class=&quot;img-lig
      
    
    </summary>
    
      <category term="知识总结" scheme="https://github.com/zdkswd/categories/%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/"/>
    
    
      <category term="机器学习" scheme="https://github.com/zdkswd/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>EM GMM 个人总结</title>
    <link href="https://github.com/zdkswd/2019/03/18/EM%20GMM%20%E4%B8%AA%E4%BA%BA%E6%80%BB%E7%BB%93/"/>
    <id>https://github.com/zdkswd/2019/03/18/EM GMM 个人总结/</id>
    <published>2019-03-18T08:40:47.000Z</published>
    <updated>2019-03-18T08:40:20.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="EM-GMM-个人总结"><a href="#EM-GMM-个人总结" class="headerlink" title="EM GMM 个人总结"></a>EM GMM 个人总结</h1><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/EM%20GMM%20%E4%B8%AA%E4%BA%BA%E6%80%BB%E7%BB%93/WechatIMG541552897534_.pic_hd.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/EM%20GMM%20%E4%B8%AA%E4%BA%BA%E6%80%BB%E7%BB%93/WechatIMG551552897535_.pic_hd.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/EM%20GMM%20%E4%B8%AA%E4%BA%BA%E6%80%BB%E7%BB%93/WechatIMG531552897534_.pic_hd.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/EM%20GMM%20%E4%B8%AA%E4%BA%BA%E6%80%BB%E7%BB%93/WechatIMG501552897530_.pic.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/EM%20GMM%20%E4%B8%AA%E4%BA%BA%E6%80%BB%E7%BB%93/WechatIMG511552897532_.pic.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/EM%20GMM%20%E4%B8%AA%E4%BA%BA%E6%80%BB%E7%BB%93/WechatIMG521552897533_.pic.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;EM-GMM-个人总结&quot;&gt;&lt;a href=&quot;#EM-GMM-个人总结&quot; class=&quot;headerlink&quot; title=&quot;EM GMM 个人总结&quot;&gt;&lt;/a&gt;EM GMM 个人总结&lt;/h1&gt;&lt;figure class=&quot;image-bubble&quot;&gt;
       
      
    
    </summary>
    
      <category term="知识总结" scheme="https://github.com/zdkswd/categories/%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/"/>
    
    
      <category term="机器学习" scheme="https://github.com/zdkswd/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>百面 非监督学习</title>
    <link href="https://github.com/zdkswd/2019/03/15/%E7%99%BE%E9%9D%A2%20%E9%9D%9E%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    <id>https://github.com/zdkswd/2019/03/15/百面 非监督学习/</id>
    <published>2019-03-15T06:48:47.000Z</published>
    <updated>2019-03-29T13:42:48.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="EM算法（期望最大）"><a href="#EM算法（期望最大）" class="headerlink" title="EM算法（期望最大）"></a>EM算法（期望最大）</h1><p>解决隐变量混合模型的参数估计。</p><p>是软性的聚类，某个数据点可以不同强弱程度地同时属于不同的聚类。</p><p>极大似然估计，只是一种概率论在统计学的应用，它是参数估计的方法之一。说的是已知某个随机样本满足某种概率分布，但是其中具体的参数不清楚，通过若干次试验，观察其结果，利用结果推出参数的大概值。</p><p>如何感性地理解EM算法<br><a href="https://www.jianshu.com/p/1121509ac1dc" target="_blank" rel="noopener">https://www.jianshu.com/p/1121509ac1dc</a></p><h1 id="K均值聚类"><a href="#K均值聚类" class="headerlink" title="K均值聚类"></a>K均值聚类</h1><p>它的基本思想是，通过迭代方式寻找K个簇(Cluster) 的一种划分方案，使得聚类结果对应的代价函数最小。特别地，代价函数可以定义为各个样本距离所属簇中心点的误差平方和<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E7%99%BE%E9%9D%A2%20%E9%9D%9E%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/page110image5024.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>其中xi代表第i个样本，ci是xi所属于的簇，μci代表簇对应的中心点，M是样本总数。</p><h2 id="问题1"><a href="#问题1" class="headerlink" title="问题1"></a>问题1</h2><p>问：简述K均值算法的具体步骤。</p><p>答：K均值聚类的核心目标是将给定的数据集划分成K个簇，并给出每个数据对应的簇中心点。<br>（1）数据预处理，如归一化、离群点处理等。<br>（2）随机选取K个簇中心，记为<figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E7%99%BE%E9%9D%A2%20%E9%9D%9E%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/page111image2648.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>（3）定义代价函数:<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E7%99%BE%E9%9D%A2%20%E9%9D%9E%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/page111image3160.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>（4）令t=0,1,2,…为迭代步数，重复下面过程直到J收敛:<br>●对于每一个样本xi,将其分配到距离最近的簇<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E7%99%BE%E9%9D%A2%20%E9%9D%9E%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-12%20%E4%B8%8B%E5%8D%886.53.47.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>● 对于每一个类簇k，重新计算该类簇的中心<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E7%99%BE%E9%9D%A2%20%E9%9D%9E%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-12%20%E4%B8%8B%E5%8D%886.54.23.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><p>K均值算法在迭代时,假设当前J没有达到最小值,那么首先固定簇中心{μx},调整每个样例xi所属的类别ci来让J函数减少;然后固定{ci},调整簇中心{μk}使J减小。这两个过程交替循环,J单调递减:当J递减到最小值时,{μk}和{ci}也同时收敛。<figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E7%99%BE%E9%9D%A2%20%E9%9D%9E%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/page112image848.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>如图所示，首先初始化中心点，叉子代表中心点，根据中心点的位置计算每个样本所属的簇，用不同颜色表示，然后根据每个簇中的所有点平均值计算新的中心点位置。</p><h2 id="问题2"><a href="#问题2" class="headerlink" title="问题2"></a>问题2</h2><p>问：K均值算法的优缺点是什么?如何对其进行调优?</p><p>答：K均值算法有一些缺点，例如受初值和离群点的影响每次的结果不稳定、结果通常不是全局最优而是局部最优解、无法很好地解决数据簇分布差别比较大的情况(比如一类是另一类样本数量的100倍)、不太适用于离散分类等。但是瑕不掩瑜，K均值聚类的优点也是很明显和突出的，主要体现在:对于大数据集，K均值聚类算法相对是可伸缩和高效的，它的计算复杂度是O(NKt)接近于线性，其中N是数据对象的数目，<strong>K是聚类的簇数</strong>，堤迭代的轮数。尽管算法经常以局部最优结束，但一般情况下达到的局部最优已经可以满足聚类的需求。</p><p>K均值算法的调优一般可以从以下几个角度出发。<br>（1）<strong>数据归一化和离群点处理</strong>。<br>K均值聚类本质上是—种基于欧式距离度量的数据划分方法，均值和方差大的<br>维度将对数据的聚类结果产生决定性的影响，所以未做归一化处理和统一单位的数据是无法直接参与运算和比较的。同时，离群点或者少量的噪声数据就会对均值产生较大的影响，导致中心偏移，因此使用K均值聚类算法之前通常需要对数据做预处理。<br>（2）<strong>合理选择K值</strong><br>K值的选择是K均值聚类最大的问题之一，这也是K均值聚类算法的主要缺点。实际上，我们希望能够找到一些可行的办法来弥补这一缺点，或者说找到K值的合理估计方法。但是，K值的选择一般基于经验和多次实验结果。例如采用手肘法，我们可以尝试不同的K值，并将不同K值所对应的损失函数画成折线，横轴为K的取值，纵轴为误差平方和所定义的损失函数。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E7%99%BE%E9%9D%A2%20%E9%9D%9E%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-12%20%E4%B8%8B%E5%8D%888.11.32.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>由图可见，K值越大，距离和越小;并且，当K=3时， 存在一个拐点，就像人的肘部一样;当K∈(1,3)时，曲线急速下降;当K&gt;3时，曲线趋于平稳。手肘法认为拐点就是K的最佳值。</p><p>手肘法是一个经验方法，缺点就是不够自动化，Gap Statistic方法的优点是，不再需要肉眼判断，而只需要找到最大的Gap statistic所对应的K即可,因此该方法也适用于批量化作业。在这里继续使用上面的损失函数，当分为K簇时，对应的损失函数为Dk。Gap Statistics定义为<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E7%99%BE%E9%9D%A2%20%E9%9D%9E%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-12%20%E4%B8%8B%E5%8D%888.42.40.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>其中E(logDk)是logDk.的期望，一般通过蒙特卡洛模拟产生。我们在样本所在的区域内按照均匀分布随机地产生和原始样本数一样多的随机样本，并对这个随机样本做K均值，得到一个D;重复多次就可以计算出E(logDk)的近似值。那么Gap(K)物理含义可以视为随机样本的损失与实际样本的损失之差。试想实际样本对应的最佳簇数为K，那么实际样本的损失应该相对较小，随机样本损失与实际样本损失之差也相应地达到最小值，从而Gap(K)取得最大值所对应的K值就是最佳的簇数。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E7%99%BE%E9%9D%A2%20%E9%9D%9E%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-12%20%E4%B8%8B%E5%8D%888.50.02.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>（3）<strong>采用核函数</strong><br>采用核函数是另一种可以尝试的改进方向。传统的欧式距离度量方式，使得K<br>均值算法本质_上假设了各个数据簇的数据具有-样的先验概率，并呈现球形或者高维球形分布，这种分布在实际生活中并不常见。面对非凸的数据分布形状时，可能需要引入核函数来优化，这时算法又称为核K均值算法，是核聚类方法的一种。<strong>核聚类方法的主要思想是通过一个非线性映射，将输入空间中的数据点映射到高位的特征空间中，并在新的特征空间中进行聚类。</strong>非线性映射增加了数据点线性可分的概率，从而在经典的聚类算法失效的情况下，通过引入核函数可以达到更为准确的聚类结果。</p><h2 id="问题3"><a href="#问题3" class="headerlink" title="问题3"></a>问题3</h2><p>问：针对K均值算法的缺点，有哪些改进的模型？</p><p>答：K均值的主要缺点有。<br>（1）需要人工预先确定初始K值，且该值和真实的数据分布未必吻合。<br>（2）K均值只能收敛到局部最优，效果受到初始值很大。<br>（3）易受到噪点的影响。<br>（4）样本点只能被划分到单一的类中。</p><p><strong>K-means++算法</strong><br>K均值的改进算法中，对初始值选择的改进是很重要的一部分。而这类算法<br>中，最具影响力的当属K-means++算法。原始K均值算法最开始随机选取数据集中K个点作为聚类中心，而K-means++ 按照如下的思想选取K个聚类中心。假设已经选取了n个初始聚类中心(0&lt;n&lt;K) ，则在选取第n+1个聚类中心时，距离当前n个聚类中心越远的点会有更高的概率被选为第n+1个聚类中心。在选取第一个聚类中心(n=1) 时同样通过随机的方法。可以说这也符合我们的直觉，聚类中心当然是互相离得越远越好。当选择完初始点后，K-means++ 后续的执行和经典K均值算法相同，这也是对初始值选择进行改进的方法等共同点。</p><p><strong>ISODATA算法</strong><br>当K值的大小不确定时，可以使用<strong>ISODATA</strong>算法。ISODATA的全称是迭代自<br>组织数据分析法。在K均值算法中，聚类个数K的值需要预先人为地确定，并且在整个算法过程中无法更改。而当遇到高维度、海量的数据集时，人们往往很难准确地估计出K的大小。ISODATA算法就是针对这个问题进行了改进，它的思想也很直观。当属于某个类别的样本数过少时，把该类别去除;当属于某个类别的样本数过多、分散程度较大时，把该类别分为两个子类别。ISODATA算法在K均值算法的基础之上增加了两个操作，一是<strong>分裂</strong>操作，对应着增加聚类中心数;二是<strong>合并</strong>操作，对应着减少聚类中心数。ISODATA算法是一个比较常见的算法，其缺点是需要指定的参数比较多，不仅仅需要一个参考的聚类数量K，还需要制定3个阈值。下面介绍ISODATA算法的各个输入参数。<br>（1）预期的聚类中心数目K。在ISODATA运行过程中聚类中心数可以变化，K0是一个用户指定的参考值，该算法的聚类中心数目变动范围也由其决定。具体地，最终输出的聚类中心数目常见范围是从K0的一半，到两倍K0。<br>（2）每个类所要求的最少样本数目Nmin。如果分裂后会导致某个子类别所包   含样本数目小于该阈值,就不会对该类别进行分裂操作。<br>（3）最大方差Sigma。用于控制某个类别中样本的分散程度。当样本的分散程度超过这个阈值时，且分裂后满足(1),进行分裂操作。<br>(4) 两个聚类中心之间所允许最小距离Dmin。如果两个类靠得非常近(即这两个类别对应聚类中心之间的距离非常小)，小于该阈值时，则对这两个类进行合并操作。</p><p>如果希望样本不划分到单一的类中，可以使用模糊C均值或者高斯混合模型。</p><h2 id="问题4"><a href="#问题4" class="headerlink" title="问题4"></a>问题4</h2><p>问：证明K均值算法的收敛性。</p><p>答：首先，我们需要知道K均值聚类的迭代算法实际上是一种最大期望算法(Expectation-Maximization algorithm)，简称EM算法。EM算法解决的是在概率模型中含有无法观测的隐含变量情况下的参数估计问题。即证明EM算法的收敛性。详见zdk的自己的公式推导。</p><h1 id="高斯混合模型"><a href="#高斯混合模型" class="headerlink" title="高斯混合模型"></a>高斯混合模型</h1><p>高斯混合模型( Gaussian Mixed Model,GMM)也是一种常见的聚类算法, 与K均值算法类似,同样使用了EM算法进行迭代计算。高斯混合模型假设每个簇的数据都是符合高斯分布(又叫正态分布)的,当前数据呈现的分布就是各个簇的高斯分布叠加在一起的结果。</p><h2 id="问题1-1"><a href="#问题1-1" class="headerlink" title="问题1"></a>问题1</h2><p>问：高斯混合模型的核心思想是什么?它是如何迭代计算的?</p><p>答：<br>高斯混合模型的核心思想是，假设数据可以看作从多个高斯分布中生成出来<br>的。在该假设下，每个单独的分模型都是标准高斯模型，其均值μ和方差Σ,是待估计的参数。此外，每个分模型都还有一个参数π，可以理解为权重或生成数据的概率。高斯混合模型的公式为<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E7%99%BE%E9%9D%A2%20%E9%9D%9E%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/page120image6336.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>然而，通常我们并不能直接得到高斯混合模型的参数，而是观察到了一系列数据点，给出一个类别的数量K后，希望求得最佳的K个高斯分模型。因此，高斯<br>混合模型的计算，便成了最佳的均值μ,方差Σ、权重π的寻找，这类问题通常通过最大似然估计来求解。遗憾的是，此问题中直接使用最大似然估计,得到的是一个复杂的非凸函数，目标函数是和的对数，难以展开和对其求偏导。</p><p>在这种情况下，可以用上一节已经介绍过的EM算法框架来求解该优化问题。<br>EM算法是在最大化目标函数时，先固定-一个变量使整体函数变为凸优化函数，求导得到最值，然后利用最优参数更新被固定的变量，进入下一一个循环。具体到高斯混合模型的求解，EM算法的迭代过程如下。</p><p>首先，初始随机选择各参数的值。然后，重复下述两步，直到收敛。<br>(1)E步骤。根据当前的参数，计算每个点由某个分模型生成的概率。<br>(2) M步骤。使用E步骤估计出的概率，来改进每个分模型的均值，方差和权重。</p><p>也就是说，我们并不知道最佳的K个高斯分布的各自3个参数，也不知道每个数据点究竟是哪个高斯分布生成的。所以每次循环时，先固定当前的高斯分布不变，获得每个数据点由各个高斯分布生成的概率。然后固定该生成概率不变，根据数据点和生成概率，获得-一个组更佳的高斯分布。循环往复，直到参数不再变化，或者变化非常小时，便得到了比较合理的一组高斯分布。</p><p>高斯混合模型与K均值算法的相同点是，它们都是可用于聚类的算法;都需要指定K值;都是使用EM算法来求解;都往往只能收敛于局部最优。而它相比于K均值算法的优点是，可以给出一个样本属于某类的概率是多少;不仅仅可以用于聚类，还可以用于概率密度的估计;并且可以用于生成新的样本点。</p><h1 id="聚类算法的评估"><a href="#聚类算法的评估" class="headerlink" title="聚类算法的评估"></a>聚类算法的评估</h1><h2 id="问题1-2"><a href="#问题1-2" class="headerlink" title="问题1"></a>问题1</h2><p>问：以聚类问题为例，假设没有外部标签数据，如何评估两个聚类算法的优劣?</p><p>答：非监督学习通常没有标注数据，模型、算法的设计直接影响最终的输出和模型的性能。为了评估不同聚类算法的性能优劣，我们需要了解常见的数据簇的特点。</p><p>●以中心定义的数据簇:这类数据集合倾向于球形分布，通常中心被定义为质心，即此数据簇中所有点的平均值。集合中的数据到中心的距离相比到其他簇中心的距离更近。<br>●以密度定义的数据簇:这类数据集合呈现和周围数据簇明显不同的密度，或稠密或稀疏。当数据簇不规则或互相盘绕，并且有噪声和离群点时，常常使用基于密度的簇定义。<br>● 以连通定义的数据簇:这类数据集合中的数据点和数据点之间有连接关系，整个数据簇表现为图结构。该定义对不规则形状或者缠绕的数据簇有效。<br>● 以概念定义的数据簇:这类数据集合中的所有数据点具有某种共同性质。</p><p>由于数据以及需求的多样性，没有一种算法能够适用于所有的数据类型、数据簇或应用场景，似乎每种情况都可能需要一种不同的评估方法或度量标准。例<br>如，K均值聚类可以用误差平方和来评估,但是基于密度的数据簇可能不是球形，误差平方和则会失效。在许多情况下，判断聚类算法结果的好坏强烈依赖于主观解释。尽管如此，聚类算法的评估还是必需的，它是聚类分析中十分重要的部分之一。</p><p>聚类评估的任务是估计在数据集上进行聚类的可行性，以及聚类方法产生结<br>果的质量。这一过程又分为三个子任务。<br>(1)估计聚类趋势。<br>(2)判定数据簇数<br>(3)测定聚类质量</p><h1 id="自组织映射神经网络"><a href="#自组织映射神经网络" class="headerlink" title="自组织映射神经网络"></a>自组织映射神经网络</h1><p>自组织映射神经网络(Self-Organizing Map，SOM)是无监督学习方法中一类重要方法，可以用作聚类、高维可视化、数据压缩、特征提取等多种用途。在深度神经网络大为流行的今天，谈及自组织映射神经网络依然是一件非常有意义的事情，这主要是由于自组织映射神经网络融入了大量人脑神经元的信号处理机制，有着独特的结构特点。</p><h2 id="问题1-3"><a href="#问题1-3" class="headerlink" title="问题1"></a>问题1</h2><p>问：自组织映射神经网络是如何工作的?它与K均值算法有何区别?</p><p>答：生物学研究表明，在人脑的感知通道上，神经元组织是有序排列的;同时，大脑皮层会对外界特定时空信息的输入在特定区域产生兴奋，而且相类似的外界信息输入产生对应兴奋的大脑皮层区域也连续映像的。例如，生物视网膜中有许多特定的细胞对特定的图形比较敏感，当视网膜中有若千个接收单元同时受特定模式刺激时，就使大脑皮层中的特定神经元开始兴奋，且输入模式接近时与之对应的兴奋神经元也接近;在听觉通道上，神经元在结构排列上与频率的关系十分密切，对于某个频率，特定的神经元具有最大的响应，位置相邻的神经元具有相近的频率特征，而远离的神经元具有的频率特征差别也较大。大脑皮层中神经元的这种响应特点不是先天安排好的，而是通过后天的学习自组织形成的。</p><p>在生物神经系统中，还存在着一种侧抑制现象，即一个神经细胞兴奋后，会对周围其他神经细胞产生抑制作用。这种抑制作用会使神经细胞之间出现竞争，其结果是某些获胜，而另一些则失败。表现形式是获胜神经细胞兴奋，失败神经细胞抑制。自组织神经网络就是对上述生物神经系统功能的一种人工神经网络模拟。</p><p>自组织映射神经网络本质上是一个两层的神经网络，包含输入层和输出层(竞争层)。输入层模拟感知外界输入信息的视网膜,输出层模拟做出响应的大脑皮层。输出层中神经元的个数通常是聚类的个数，代表每一个需要聚成的类。训练时采用“竞争学习”的方式，每个输入的样例在输出层中找到一个和它最匹配的节点，称为激活节点，也叫winning neuron;紧接着用随机梯度下降法更新激活节点的参数;同时，和激活节点临近的点也根据它们距离激活节点的远近而适当地更新参数。这种竞争可以通过神经元之间的横向抑制连接(负反馈路径)来实现。自组织映射神经网络的输出层节点是有拓扑关系的。这个拓扑关系依据需求确定，如果想要-维的模型，那么隐藏节点可以是“-维线阵”;如果想要二维的拓扑关系，那么就行成一个“二维平面阵”，如图5.8所示。也有更高维度的拓扑关系的，比如“三维栅格阵”，但并不常见。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E7%99%BE%E9%9D%A2%20%E9%9D%9E%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-14%20%E4%B8%8B%E5%8D%889.56.05.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>假设输入空间是D维，输入模式为x={xi,i=1.=…D}, 输入单元i和神经元j之间在计算层的连接权重为w= {wi,j= 1,..N,i=1,..,,D},其中N是神经元的总数。自组织映射神经网络的自组织学习过程可以归纳为以下几个子过程。<br>(1)初始化。所有连接权重都用小的随机值进行初始化。<br>(2)竞争。神经元计算每一个输入模式各自的判别函数值，并宣布具有最小判别函数值的特定神经元为胜利者，其中每个神经元j的判别函数为<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E7%99%BE%E9%9D%A2%20%E9%9D%9E%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-14%20%E4%B8%8B%E5%8D%8810.00.29.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>(3)合作。获胜神经元(ω)决定了兴奋神经元拓扑邻域的空间位置。确定激活结点I(x)之后,我们也希望更新和它临近的节点。简单地说,临近的节点距离越远,更新的程度要打更大折扣。<br>(4)适应。适当调整相关兴奋神经元的连接权重,使得获胜的神经元对相似输入模式的后续应用的响应增强。<br>(5)迭代。继续回到步骤(2),直到特征映射趋于稳定。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;EM算法（期望最大）&quot;&gt;&lt;a href=&quot;#EM算法（期望最大）&quot; class=&quot;headerlink&quot; title=&quot;EM算法（期望最大）&quot;&gt;&lt;/a&gt;EM算法（期望最大）&lt;/h1&gt;&lt;p&gt;解决隐变量混合模型的参数估计。&lt;/p&gt;
&lt;p&gt;是软性的聚类，某个数据点可以不
      
    
    </summary>
    
      <category term="知识总结" scheme="https://github.com/zdkswd/categories/%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/"/>
    
    
      <category term="机器学习" scheme="https://github.com/zdkswd/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>聚类</title>
    <link href="https://github.com/zdkswd/2019/03/12/%E8%81%9A%E7%B1%BB/"/>
    <id>https://github.com/zdkswd/2019/03/12/聚类/</id>
    <published>2019-03-12T07:34:12.000Z</published>
    <updated>2019-03-12T07:35:44.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>在“无监督学习”( unsupervised learning)中,训练样本的标记信息是未知的,目标是通过对无标记训练样本的学习来揭示数据的内在性质及规律,为进一步的数据分析提供基础.此类学习任务中研究最多、应用最广的是“聚类”( clustering）</p><p>聚类试图将数据集中的样本划分为若干个通常是不相交的子集,每个子集称为一个“簇”(cluster).通过这样的划分，每个簇可能对应于一些潜在的概念(类别),如“浅色瓜”“深色瓜” ，“有籽瓜”，“无籽瓜”,甚至“本地瓜”，“外地瓜”等;需说明的是,这些概念对聚类算法而言事先是未知的，聚类过程仅能自动形成簇结构,簇所对应的概念语义需由使用者来把握和命名.对聚类算法而言，标记簇亦称为类。</p><p>聚类既能作为一个单独过程,用于找寻数据内在的分布结构,也可作为分类等其他学习任务的前驱过程.例如,在一些商业应用中需对新用户的类型进行判别，但定义“用户类型”对商家来说却可能不太容易，此时往往可先对用户数据进行聚类，根据聚类结果将每个簇定义为一个类,然后再基于这些类训练分类模型,用于判别新用户的类型.</p><h1 id="性能度量"><a href="#性能度量" class="headerlink" title="性能度量"></a>性能度量</h1><p>对聚类结果,我们需通过某种性能度量来评估其好坏;另一方面，若明确了最终将要使用的性能度量,则可直接将其作为聚类过程的优化目标，从而更好地得到符合要求的聚类结果.</p><p>什么样的聚类结果比较好呢?直观上看,我们希望“物以类聚” ’，即同簇的样本尽可能彼此相似,不同簇的样本尽可能不同.换言之,聚类结果的“簇内相似度”(intra-cluster similarity)高且“簇间相似度”(inter-cluster similarity)低.</p><p>聚类性能度量大致有两类一 类是将聚类结果与某个“参考模型”(reference model)进行比较,称为“外部指标”(external index); 另一类是直接考察聚类结果而不利用任何参考模型,称为“内部指标”(internal index).</p><h2 id="外部指标"><a href="#外部指标" class="headerlink" title="外部指标"></a>外部指标</h2><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%81%9A%E7%B1%BB/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-12%20%E4%B8%8A%E5%8D%8810.00.59.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h3 id="Jaccard系数-Jaccard-Coefficient-简称JC"><a href="#Jaccard系数-Jaccard-Coefficient-简称JC" class="headerlink" title="Jaccard系数(Jaccard Coefficient, 简称JC)"></a>Jaccard系数(Jaccard Coefficient, 简称JC)</h3><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%81%9A%E7%B1%BB/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-12%20%E4%B8%8A%E5%8D%8810.02.02.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h3 id="FM指数-Fowlkes-and-Mallows-Index-简称FMI"><a href="#FM指数-Fowlkes-and-Mallows-Index-简称FMI" class="headerlink" title="FM指数(Fowlkes and Mallows Index,简称FMI)"></a>FM指数(Fowlkes and Mallows Index,简称FMI)</h3><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%81%9A%E7%B1%BB/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-12%20%E4%B8%8A%E5%8D%8810.02.28.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h3 id="Rand指数-Rand-Index-简称RI"><a href="#Rand指数-Rand-Index-简称RI" class="headerlink" title="Rand指数(Rand Index,简称RI)"></a>Rand指数(Rand Index,简称RI)</h3><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%81%9A%E7%B1%BB/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-12%20%E4%B8%8A%E5%8D%8810.02.59.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>上述性能度量的结果值均在[0, 1]区间,值越大越好。</p><h2 id="内部指标"><a href="#内部指标" class="headerlink" title="内部指标"></a>内部指标</h2><p>距离越大则样本的相似度越低。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%81%9A%E7%B1%BB/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-12%20%E4%B8%8A%E5%8D%8810.17.20.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><h3 id="DB指数-Davies-Bouldin-Index-简称DBI"><a href="#DB指数-Davies-Bouldin-Index-简称DBI" class="headerlink" title="DB指数(Davies- Bouldin Index,简称DBI)"></a>DB指数(Davies- Bouldin Index,简称DBI)</h3><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%81%9A%E7%B1%BB/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-12%20%E4%B8%8A%E5%8D%8810.18.07.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h3 id="Dunn指数-Dunn-Index-简称DI"><a href="#Dunn指数-Dunn-Index-简称DI" class="headerlink" title="Dunn指数(Dunn Index,简称DI)"></a>Dunn指数(Dunn Index,简称DI)</h3><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%81%9A%E7%B1%BB/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-12%20%E4%B8%8A%E5%8D%8810.18.28.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>DBI的值越小越好，而DI则相反，值越大越好。</p><h2 id="距离计算"><a href="#距离计算" class="headerlink" title="距离计算"></a>距离计算</h2><p>对函数dist(,),若它是一个“距离度量”(distance measure),则需满足一些基本性质:<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%81%9A%E7%B1%BB/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-12%20%E4%B8%8A%E5%8D%8810.36.49.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%81%9A%E7%B1%BB/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-12%20%E4%B8%8A%E5%8D%8810.37.01.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>直递性常被直接称为“三角不等式”。<br>给定样本xi = (xi1;xi2;…;xin) 与xj = (xj1;xj2;..;xjn), 最常用的是“闵可夫斯基距离”(Minkowski distance)<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%81%9A%E7%B1%BB/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-12%20%E4%B8%8A%E5%8D%8810.38.35.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>上式即为xi-xj的Lp范数。<br>当p=2时，闵可夫斯基距离即欧氏距离。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%81%9A%E7%B1%BB/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-12%20%E4%B8%8A%E5%8D%8810.40.34.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>当p=1时，闵可夫斯基距离即曼哈顿距离。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%81%9A%E7%B1%BB/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-12%20%E4%B8%8A%E5%8D%8810.42.10.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><p>闵可夫斯基距离适用于{1,2,3}这样的有序属性，而不适用于{飞机，火车，轮船}这样的无序属性。对无序属性可采用VDM。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%81%9A%E7%B1%BB/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-12%20%E4%B8%8A%E5%8D%8810.46.29.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%81%9A%E7%B1%BB/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-12%20%E4%B8%8A%E5%8D%8810.47.01.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><p>当样本空间中不同属性的重要性不同时,还可使用“加权距离“</p><p>通常我们是基于某种形式的距离来定义“相似度度量”(similarity measure), 距离越大,相似度越小.然而，用于相似度度量的距离未必一定要满足距离度量的所有基本性质,尤其是直递性。</p><h1 id="原型聚类"><a href="#原型聚类" class="headerlink" title="原型聚类"></a>原型聚类</h1><h2 id="k均值算法"><a href="#k均值算法" class="headerlink" title="k均值算法"></a>k均值算法</h2><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%81%9A%E7%B1%BB/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-12%20%E4%B8%8B%E5%8D%8812.30.35.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>最小化式(9.24)并不容易，找到它的最优解需考察样本集D所有可能的簇划分,这是一个NP难问题.因此, k均值算法采用了贪心策略,通过迭代优化来近似求解式(9.24).</p><p>对于以下数据集，为方便叙述，将编号为i的样本称为xi，这是一个包含密度与含糖率两个属性值的二维向量。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%81%9A%E7%B1%BB/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-12%20%E4%B8%8B%E5%8D%8812.38.59.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%81%9A%E7%B1%BB/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-12%20%E4%B8%8B%E5%8D%8812.44.09.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>为避免运行时间过长,通常设置一一个最大运行轮数或最小调整幅度阈值,若达到最大轮数或调整幅度小于阈值,则停止运行。</p><p>假定聚类簇数k= 3,算法开始时随机选取三个样本x6, x12, x27作为初始均值向量,即<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%81%9A%E7%B1%BB/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-12%20%E4%B8%8B%E5%8D%8812.46.12.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>考察样本x1 = (0.697; 0.460),它与当前均值向量μ1, μu2, μ3的距离分别为0.369, 0.506, 0.166, 因此x1将被划入簇C3中.类似的,对数据集中的所有样本<br>考察一遍后, 可得当前簇划分为<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%81%9A%E7%B1%BB/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-12%20%E4%B8%8B%E5%8D%8812.47.13.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>于是，可从C1、C2、C3分别求出新的均值向量<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%81%9A%E7%B1%BB/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-12%20%E4%B8%8B%E5%8D%8812.47.59.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>更新当前均值向量后，不断重复上述过程,如图9.3所示，第五轮迭代产生的结<br>果与第四轮迭代相同，于是算法停止得到最终的簇划分.<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%81%9A%E7%B1%BB/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-12%20%E4%B8%8B%E5%8D%8812.48.47.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><h2 id="学习向量量化"><a href="#学习向量量化" class="headerlink" title="学习向量量化"></a>学习向量量化</h2><p>与k均值算法类似，“ 学习向量量化”(Learning Vector Quantization,简称LVQ)也是试图找到一组原型向量来刻画聚类结构,但与一般聚类算法不同的是,LVQ假设数据样本带有类别标记，学习过程利用样本的这些监督信息来辅助聚类.</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%81%9A%E7%B1%BB/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-12%20%E4%B8%8B%E5%8D%8812.53.04.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%81%9A%E7%B1%BB/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-12%20%E4%B8%8B%E5%8D%8812.57.41.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>在每一轮迭代中，算法随机选取一个有标记的训练样本，找出与其距离最近的原型向量，并根据两者的类别标记是否一致来对原型向量进行响应的更新。若算法的停止条件已满足，例如已达到最大迭代轮数，或原型向量更新很小甚至不再更新，则将当前原型向量作为最终结果返回。</p><p>以西瓜数据集为例演示LVQ的学习过程。假定q=5，即学习目标是找到5个原型向量<strong>p1,p2,p3,p4,p5</strong>,并假定其对应的类别标记分别为c1,c2,c2,c1,c1。</p><p>算法开始时,根据样本的类别标记和簇的预设类别标记对原型向量进行随机初始化，假定初始化为样本<strong>x5, x12, x18, x23, x29</strong>.在第一轮迭代中，假定随机选取的样本为x1,该样本与当前原型向量<strong>p1,p2,p3,p4,p5</strong>的距离分别为0.283, 0.506, 0.434, 0.260, 0.032.由于<strong>p5</strong>与<strong>x1</strong>距离最近且两者具有相同的类别标记c2,假定学习率η= 0.1,则LVQ更新<strong>p5</strong>得到新原型向量<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%81%9A%E7%B1%BB/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-12%20%E4%B8%8B%E5%8D%881.28.14.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%81%9A%E7%B1%BB/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-12%20%E4%B8%8B%E5%8D%881.45.59.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h2 id="高斯混合聚类"><a href="#高斯混合聚类" class="headerlink" title="高斯混合聚类"></a>高斯混合聚类</h2><p>与k均值、LVQ用原型向量来刻画聚类结构不同,高斯混合(Mixture-of-Gaussian)聚类采用概率模型来表达聚类原型.<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%81%9A%E7%B1%BB/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-12%20%E4%B8%8B%E5%8D%881.54.09.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>可定义高斯混合分布<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%81%9A%E7%B1%BB/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-12%20%E4%B8%8B%E5%8D%882.02.00.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%81%9A%E7%B1%BB/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-12%20%E4%B8%8B%E5%8D%882.03.41.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%81%9A%E7%B1%BB/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-12%20%E4%B8%8B%E5%8D%882.03.28.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>按下不表</p><h1 id="密度聚类"><a href="#密度聚类" class="headerlink" title="密度聚类"></a>密度聚类</h1><p>密度聚类亦称“基于密度的聚类”(density-based clustering), 此类算法假设聚类结构能通过样本分布的紧密程度确定.通常情形下，密度聚类算法从样本密度的角度来考察样本之间的可连接性，并基于可连接样本不断扩展聚类簇以获得最终的聚类结果.<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%81%9A%E7%B1%BB/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-12%20%E4%B8%8B%E5%8D%882.10.09.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%81%9A%E7%B1%BB/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-12%20%E4%B8%8B%E5%8D%882.11.07.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>密度直达关系通常不满足对称性，密度可达关系满足直递性,但不满足对称性.密度相连关系满足对称性。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%81%9A%E7%B1%BB/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-12%20%E4%B8%8B%E5%8D%882.22.09.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%81%9A%E7%B1%BB/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-12%20%E4%B8%8B%E5%8D%882.24.24.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>D中不属于任何簇的样本被认为是噪声(noise)或异常(anomaly)样本。</p><p>若x为核心对象,由x密度可达的所有样本组成的集合记为X = {x’∈D |x’由x密度可达},则不难证明X即为满足连接性与最大性的簇.</p><p> DBSCAN算法先任选数据集中的一个核心对象为“种子”(seed),再由此出发确定相应的聚类簇，算法先根据给定的邻域参数找到所有核心对象；然后以任一核心对象为出发点，找到由其密度可达的样本生成聚类簇，直到所有核心对象均被访问过为止。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%81%9A%E7%B1%BB/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-12%20%E4%B8%8B%E5%8D%882.41.24.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>以西瓜数据集为例，假定邻域参数设置为e=0.11，MinPts=5.DBSCAN算法先找出各样本的∈-邻域并确定核心对象集合: S= {x3, x5, x6, x8, x9, x13, x14, x18, x19, x24, x25, x28, x29}.然后，从Ω中随机选取一个核心对象作为种子，找出由它密度可达的所有样本,这就构成了第一个聚类簇.不失一般性,假定核心对象x8被选中作为种子，则DBSCAN生成的第一个聚类簇为<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%81%9A%E7%B1%BB/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-12%20%E4%B8%8B%E5%8D%882.45.11.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>然后, DBSCAN将C1中包含的核心对象从几中去除:Ω=Ω \ C1 ={x3, x5, x9, x13, x14, x24, x25, x28, x2g}.再从更新后的集合8中随机选取一个核心对象作为种子来生成下一一个聚类簇..上述过程不断重复,直至I为空.图9.10显示DBSCAN先后生成聚类簇的情况. C1之后生成的聚类簇为<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%81%9A%E7%B1%BB/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-12%20%E4%B8%8B%E5%8D%883.12.14.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%81%9A%E7%B1%BB/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-12%20%E4%B8%8B%E5%8D%883.12.30.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><h1 id="层次聚类"><a href="#层次聚类" class="headerlink" title="层次聚类"></a>层次聚类</h1><p>层次聚类(hierarchical clustering)试图在不同层次对数据集进行划分，从而形成树形的聚类结构.数据集的划分可采用“自底向上”的聚合策略,也可采用“自顶向下”的分拆策略.</p><p>AGNES是一种采用自底向上聚合策略的层次聚类算法.它先将数据集中的每个样本看作一个初始聚类簇,然后在算法运行的每一步中找出距离最近的两个聚类簇进行合并,该过程不断重复,直至达到预设的聚类簇个数.这里的关键是如何计算聚类簇之间的距离.实际上,每个簇是一个样本集合,因此,只需采用关于集合的某种距离即可。</p><p>对于给定聚类簇Ci，Cj，可通过下面式子来计算距离：<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%81%9A%E7%B1%BB/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-12%20%E4%B8%8B%E5%8D%883.25.52.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>显然,最小距离由两个簇的最近样本决定,最大距离由两个簇的最远样本决定，而平均距离则由两个簇的所有样本共同决定.当聚类簇距离由dmin，dmax或davg计算时，AGNES算法被相应地称为“单链接”(single-linkage)、 “ 全链接”(complete linkage)或“均链接”(average-linkage)算法.</p><p>AGNES算法先对仅含一个样本的初始聚类簇和相应的距离矩阵进行初始化;然后AGNES不断合并距离最近的聚类簇,并对合并得到的聚类簇的距离矩阵进行更新;上述过程不断重复,直至达到预设的聚类簇数.<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%81%9A%E7%B1%BB/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-12%20%E4%B8%8B%E5%8D%883.31.13.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;概述&quot;&gt;&lt;a href=&quot;#概述&quot; class=&quot;headerlink&quot; title=&quot;概述&quot;&gt;&lt;/a&gt;概述&lt;/h1&gt;&lt;p&gt;在“无监督学习”( unsupervised learning)中,训练样本的标记信息是未知的,目标是通过对无标记训练样本的学习来揭示数据的
      
    
    </summary>
    
      <category term="知识总结" scheme="https://github.com/zdkswd/categories/%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/"/>
    
    
      <category term="机器学习" scheme="https://github.com/zdkswd/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
</feed>
