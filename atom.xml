<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>ZDK&#39;s blog</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://github.com/zdkswd/"/>
  <updated>2019-03-19T11:37:23.000Z</updated>
  <id>https://github.com/zdkswd/</id>
  
  <author>
    <name>ZDK</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>CNN 个人总结</title>
    <link href="https://github.com/zdkswd/2019/03/19/CNN%20%E4%B8%AA%E4%BA%BA%E6%80%BB%E7%BB%93/"/>
    <id>https://github.com/zdkswd/2019/03/19/CNN 个人总结/</id>
    <published>2019-03-19T07:45:12.000Z</published>
    <updated>2019-03-19T11:37:23.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="尺寸问题"><a href="#尺寸问题" class="headerlink" title="尺寸问题"></a>尺寸问题</h1><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/CNN%20%E4%B8%AA%E4%BA%BA%E6%80%BB%E7%BB%93/20180404150134375.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>图中，输入层个数为1，深度为3，所以每个filter对应深度也要为。filter数量为2，对应输出层的深度为2，个数为1。</p><p>即输入层的深度与filter深度相等，输出层深度与filter个数相等。</p><h1 id="卷积层的梯度传递"><a href="#卷积层的梯度传递" class="headerlink" title="卷积层的梯度传递"></a>卷积层的梯度传递</h1><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/CNN%20%E4%B8%AA%E4%BA%BA%E6%80%BB%E7%BB%93/2256672-52295dad2641037f.png" alt="图1" title="">                </div>                <div class="image-caption">图1</div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/CNN%20%E4%B8%AA%E4%BA%BA%E6%80%BB%E7%BB%93/2256672-2fb37b0a3ff0e1f9.png" alt="图2" title="">                </div>                <div class="image-caption">图2</div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/CNN%20%E4%B8%AA%E4%BA%BA%E6%80%BB%E7%BB%93/2256672-754f37eb7603e99f.png" alt="图3" title="">                </div>                <div class="image-caption">图3</div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/CNN%20%E4%B8%AA%E4%BA%BA%E6%80%BB%E7%BB%93/2256672-af2da9701a03dc3c.png" alt="图4" title="">                </div>                <div class="image-caption">图4</div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/CNN%20%E4%B8%AA%E4%BA%BA%E6%80%BB%E7%BB%93/WechatIMG601552981169_.pic_hd.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h1 id="卷积层filter权重梯度的计算"><a href="#卷积层filter权重梯度的计算" class="headerlink" title="卷积层filter权重梯度的计算"></a>卷积层filter权重梯度的计算</h1><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/CNN%20%E4%B8%AA%E4%BA%BA%E6%80%BB%E7%BB%93/2256672-afe6d3a863b7cbcc.png" alt="图5" title="">                </div>                <div class="image-caption">图5</div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/CNN%20%E4%B8%AA%E4%BA%BA%E6%80%BB%E7%BB%93/WechatIMG571552981164_.pic.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h1 id="Pooling层的训练"><a href="#Pooling层的训练" class="headerlink" title="Pooling层的训练"></a>Pooling层的训练</h1><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/CNN%20%E4%B8%AA%E4%BA%BA%E6%80%BB%E7%BB%93/2256672-af77e98c09fad84c.png" alt="图6" title="">                </div>                <div class="image-caption">图6</div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/CNN%20%E4%B8%AA%E4%BA%BA%E6%80%BB%E7%BB%93/2256672-c3a6772cb07b416a.png" alt="图7" title="">                </div>                <div class="image-caption">图7</div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/CNN%20%E4%B8%AA%E4%BA%BA%E6%80%BB%E7%BB%93/WechatIMG591552981167_.pic_hd.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h1 id="CNN的反向传播"><a href="#CNN的反向传播" class="headerlink" title="CNN的反向传播"></a>CNN的反向传播</h1><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/CNN%20%E4%B8%AA%E4%BA%BA%E6%80%BB%E7%BB%93/WechatIMG581552981167_.pic.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;尺寸问题&quot;&gt;&lt;a href=&quot;#尺寸问题&quot; class=&quot;headerlink&quot; title=&quot;尺寸问题&quot;&gt;&lt;/a&gt;尺寸问题&lt;/h1&gt;&lt;figure class=&quot;image-bubble&quot;&gt;
                &lt;div class=&quot;img-lig
      
    
    </summary>
    
      <category term="知识总结" scheme="https://github.com/zdkswd/categories/%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/"/>
    
    
      <category term="机器学习" scheme="https://github.com/zdkswd/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>EM GMM 个人总结</title>
    <link href="https://github.com/zdkswd/2019/03/18/EM%20GMM%20%E4%B8%AA%E4%BA%BA%E6%80%BB%E7%BB%93/"/>
    <id>https://github.com/zdkswd/2019/03/18/EM GMM 个人总结/</id>
    <published>2019-03-18T08:40:47.000Z</published>
    <updated>2019-03-18T08:40:20.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="EM-GMM-个人总结"><a href="#EM-GMM-个人总结" class="headerlink" title="EM GMM 个人总结"></a>EM GMM 个人总结</h1><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/EM%20GMM%20%E4%B8%AA%E4%BA%BA%E6%80%BB%E7%BB%93/WechatIMG541552897534_.pic_hd.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/EM%20GMM%20%E4%B8%AA%E4%BA%BA%E6%80%BB%E7%BB%93/WechatIMG551552897535_.pic_hd.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/EM%20GMM%20%E4%B8%AA%E4%BA%BA%E6%80%BB%E7%BB%93/WechatIMG531552897534_.pic_hd.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/EM%20GMM%20%E4%B8%AA%E4%BA%BA%E6%80%BB%E7%BB%93/WechatIMG501552897530_.pic.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/EM%20GMM%20%E4%B8%AA%E4%BA%BA%E6%80%BB%E7%BB%93/WechatIMG511552897532_.pic.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/EM%20GMM%20%E4%B8%AA%E4%BA%BA%E6%80%BB%E7%BB%93/WechatIMG521552897533_.pic.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;EM-GMM-个人总结&quot;&gt;&lt;a href=&quot;#EM-GMM-个人总结&quot; class=&quot;headerlink&quot; title=&quot;EM GMM 个人总结&quot;&gt;&lt;/a&gt;EM GMM 个人总结&lt;/h1&gt;&lt;figure class=&quot;image-bubble&quot;&gt;
       
      
    
    </summary>
    
      <category term="知识总结" scheme="https://github.com/zdkswd/categories/%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/"/>
    
    
      <category term="机器学习" scheme="https://github.com/zdkswd/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>百面 非监督学习</title>
    <link href="https://github.com/zdkswd/2019/03/15/%E7%99%BE%E9%9D%A2%20%E9%9D%9E%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    <id>https://github.com/zdkswd/2019/03/15/百面 非监督学习/</id>
    <published>2019-03-15T06:48:47.000Z</published>
    <updated>2019-03-15T06:49:37.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="EM算法（期望最大）"><a href="#EM算法（期望最大）" class="headerlink" title="EM算法（期望最大）"></a>EM算法（期望最大）</h1><p>解决隐变量混合模型的参数估计。</p><p>是软性的聚类，某个数据点可以不同强弱程度地同时属于不同的聚类。</p><p>极大似然估计，只是一种概率论在统计学的应用，它是参数估计的方法之一。说的是已知某个随机样本满足某种概率分布，但是其中具体的参数不清楚，通过若干次试验，观察其结果，利用结果推出参数的大概值。</p><p>如何感性地理解EM算法<br><a href="https://www.jianshu.com/p/1121509ac1dc" target="_blank" rel="noopener">https://www.jianshu.com/p/1121509ac1dc</a></p><h1 id="K均值聚类"><a href="#K均值聚类" class="headerlink" title="K均值聚类"></a>K均值聚类</h1><p>它的基本思想是，通过迭代方式寻找K个簇(Cluster) 的一种划分方案，使得聚类结果对应的代价函数最小。特别地，代价函数可以定义为各个样本距离所属簇中心点的误差平方和<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E7%99%BE%E9%9D%A2%20%E9%9D%9E%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/page110image5024.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>其中xi代表第i个样本，ci是xi所属于的簇，μci代表簇对应的中心点，M是样本总数。</p><h2 id="问题1"><a href="#问题1" class="headerlink" title="问题1"></a>问题1</h2><p>问：简述K均值算法的具体步骤。</p><p>答：K均值聚类的核心目标是将给定的数据集划分成K个簇，并给出每个数据对应的簇中心点。<br>（1）数据预处理，如归一化、离群点处理等。<br>（2）随机选取K个簇中心，记为<figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E7%99%BE%E9%9D%A2%20%E9%9D%9E%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/page111image2648.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>（3）定义代价函数:<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E7%99%BE%E9%9D%A2%20%E9%9D%9E%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/page111image3160.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>（4）令t=0,1,2,…为迭代步数，重复下面过程直到J收敛:<br>●对于每一个样本xi,将其分配到距离最近的簇<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E7%99%BE%E9%9D%A2%20%E9%9D%9E%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-12%20%E4%B8%8B%E5%8D%886.53.47.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>● 对于每一个类簇k，重新计算该类簇的中心<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E7%99%BE%E9%9D%A2%20%E9%9D%9E%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-12%20%E4%B8%8B%E5%8D%886.54.23.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><p>K均值算法在迭代时,假设当前J没有达到最小值,那么首先固定簇中心{μx},调整每个样例xi所属的类别ci来让J函数减少;然后固定{ci},调整簇中心{μk}使J减小。这两个过程交替循环,J单调递减:当J递减到最小值时,{μk}和{ci}也同时收敛。<figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E7%99%BE%E9%9D%A2%20%E9%9D%9E%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/page112image848.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>如图所示，首先初始化中心点，叉子代表中心点，根据中心点的位置计算每个样本所属的簇，用不同颜色表示，然后根据每个簇中的所有点平均值计算新的中心点位置。</p><h2 id="问题2"><a href="#问题2" class="headerlink" title="问题2"></a>问题2</h2><p>问：K均值算法的优缺点是什么?如何对其进行调优?</p><p>答：K均值算法有一些缺点，例如受初值和离群点的影响每次的结果不稳定、结果通常不是全局最优而是局部最优解、无法很好地解决数据簇分布差别比较大的情况(比如一类是另一类样本数量的100倍)、不太适用于离散分类等。但是瑕不掩瑜，K均值聚类的优点也是很明显和突出的，主要体现在:对于大数据集，K均值聚类算法相对是可伸缩和高效的，它的计算复杂度是O(NKt)接近于线性，其中N是数据对象的数目，<strong>K是聚类的簇数</strong>，堤迭代的轮数。尽管算法经常以局部最优结束，但一般情况下达到的局部最优已经可以满足聚类的需求。</p><p>K均值算法的调优一般可以从以下几个角度出发。<br>（1）<strong>数据归一化和离群点处理</strong>。<br>K均值聚类本质上是—种基于欧式距离度量的数据划分方法，均值和方差大的<br>维度将对数据的聚类结果产生决定性的影响，所以未做归一化处理和统一单位的数据是无法直接参与运算和比较的。同时，离群点或者少量的噪声数据就会对均值产生较大的影响，导致中心偏移，因此使用K均值聚类算法之前通常需要对数据做预处理。<br>（2）<strong>合理选择K值</strong><br>K值的选择是K均值聚类最大的问题之一，这也是K均值聚类算法的主要缺点。实际上，我们希望能够找到一些可行的办法来弥补这一缺点，或者说找到K值的合理估计方法。但是，K值的选择一般基于经验和多次实验结果。例如采用手肘法，我们可以尝试不同的K值，并将不同K值所对应的损失函数画成折线，横轴为K的取值，纵轴为误差平方和所定义的损失函数。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E7%99%BE%E9%9D%A2%20%E9%9D%9E%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-12%20%E4%B8%8B%E5%8D%888.11.32.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>由图可见，K值越大，距离和越小;并且，当K=3时， 存在一个拐点，就像人的肘部一样;当K∈(1,3)时，曲线急速下降;当K&gt;3时，曲线趋于平稳。手肘法认为拐点就是K的最佳值。</p><p>手肘法是一个经验方法，缺点就是不够自动化，Gap Statistic方法的优点是，不再需要肉眼判断，而只需要找到最大的Gap statistic所对应的K即可,因此该方法也适用于批量化作业。在这里继续使用上面的损失函数，当分为K簇时，对应的损失函数为Dk。Gap Statistics定义为<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E7%99%BE%E9%9D%A2%20%E9%9D%9E%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-12%20%E4%B8%8B%E5%8D%888.42.40.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>其中E(logDk)是logDk.的期望，一般通过蒙特卡洛模拟产生。我们在样本所在的区域内按照均匀分布随机地产生和原始样本数一样多的随机样本，并对这个随机样本做K均值，得到一个D;重复多次就可以计算出E(logDk)的近似值。那么Gap(K)物理含义可以视为随机样本的损失与实际样本的损失之差。试想实际样本对应的最佳簇数为K，那么实际样本的损失应该相对较小，随机样本损失与实际样本损失之差也相应地达到最小值，从而Gap(K)取得最大值所对应的K值就是最佳的簇数。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E7%99%BE%E9%9D%A2%20%E9%9D%9E%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-12%20%E4%B8%8B%E5%8D%888.50.02.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>（3）<strong>采用核函数</strong><br>采用核函数是另一种可以尝试的改进方向。传统的欧式距离度量方式，使得K<br>均值算法本质_上假设了各个数据簇的数据具有-样的先验概率，并呈现球形或者高维球形分布，这种分布在实际生活中并不常见。面对非凸的数据分布形状时，可能需要引入核函数来优化，这时算法又称为核K均值算法，是核聚类方法的一种。<strong>核聚类方法的主要思想是通过一个非线性映射，将输入空间中的数据点映射到高位的特征空间中，并在新的特征空间中进行聚类。</strong>非线性映射增加了数据点线性可分的概率，从而在经典的聚类算法失效的情况下，通过引入核函数可以达到更为准确的聚类结果。</p><h2 id="问题3"><a href="#问题3" class="headerlink" title="问题3"></a>问题3</h2><p>问：针对K均值算法的缺点，有哪些改进的模型？</p><p>答：K均值的主要缺点有。<br>（1）需要人工预先确定初始K值，且该值和真实的数据分布未必吻合。<br>（2）K均值只能收敛到局部最优，效果受到初始值很大。<br>（3）易受到噪点的影响。<br>（4）样本点只能被划分到单一的类中。</p><p><strong>K-means++算法</strong><br>K均值的改进算法中，对初始值选择的改进是很重要的一部分。而这类算法<br>中，最具影响力的当属K-means++算法。原始K均值算法最开始随机选取数据集中K个点作为聚类中心，而K-means++ 按照如下的思想选取K个聚类中心。假设已经选取了n个初始聚类中心(0&lt;n&lt;K) ，则在选取第n+1个聚类中心时，距离当前n个聚类中心越远的点会有更高的概率被选为第n+1个聚类中心。在选取第一个聚类中心(n=1) 时同样通过随机的方法。可以说这也符合我们的直觉，聚类中心当然是互相离得越远越好。当选择完初始点后，K-means++ 后续的执行和经典K均值算法相同，这也是对初始值选择进行改进的方法等共同点。</p><p><strong>ISODATA算法</strong><br>当K值的大小不确定时，可以使用<strong>ISODATA</strong>算法。ISODATA的全称是迭代自<br>组织数据分析法。在K均值算法中，聚类个数K的值需要预先人为地确定，并且在整个算法过程中无法更改。而当遇到高维度、海量的数据集时，人们往往很难准确地估计出K的大小。ISODATA算法就是针对这个问题进行了改进，它的思想也很直观。当属于某个类别的样本数过少时，把该类别去除;当属于某个类别的样本数过多、分散程度较大时，把该类别分为两个子类别。ISODATA算法在K均值算法的基础之上增加了两个操作，一是<strong>分裂</strong>操作，对应着增加聚类中心数;二是<strong>合并</strong>操作，对应着减少聚类中心数。ISODATA算法是一个比较常见的算法，其缺点是需要指定的参数比较多，不仅仅需要一个参考的聚类数量K，还需要制定3个阈值。下面介绍ISODATA算法的各个输入参数。<br>（1）预期的聚类中心数目K。在ISODATA运行过程中聚类中心数可以变化，K0是一个用户指定的参考值，该算法的聚类中心数目变动范围也由其决定。具体地，最终输出的聚类中心数目常见范围是从K0的一半，到两倍K0。<br>（2）每个类所要求的最少样本数目Nmin。如果分裂后会导致某个子类别所包   含样本数目小于该阈值,就不会对该类别进行分裂操作。<br>（3）最大方差Sigma。用于控制某个类别中样本的分散程度。当样本的分散程度超过这个阈值时，且分裂后满足(1),进行分裂操作。<br>(4) 两个聚类中心之间所允许最小距离Dmin。如果两个类靠得非常近(即这两个类别对应聚类中心之间的距离非常小)，小于该阈值时，则对这两个类进行合并操作。</p><p>如果希望样本不划分到单一的类中，可以使用模糊C均值或者高斯混合模型。</p><h2 id="问题4"><a href="#问题4" class="headerlink" title="问题4"></a>问题4</h2><p>问：证明K均值算法的收敛性。</p><p>答：首先，我们需要知道K均值聚类的迭代算法实际上是一种最大期望算法(Expectation-Maximization algorithm)，简称EM算法。EM算法解决的是在概率模型中含有无法观测的隐含变量情况下的参数估计问题。即证明EM算法的收敛性。详见zdk的自己的公式推导。</p><h1 id="高斯混合模型"><a href="#高斯混合模型" class="headerlink" title="高斯混合模型"></a>高斯混合模型</h1><p>高斯混合模型( Gaussian Mixed Model,GMM)也是一种常见的聚类算法, 与K均值算法类似,同样使用了EM算法进行迭代计算。高斯混合模型假设每个簇的数据都是符合高斯分布(又叫正态分布)的,当前数据呈现的分布就是各个簇的高斯分布叠加在一起的结果。</p><h2 id="问题1-1"><a href="#问题1-1" class="headerlink" title="问题1"></a>问题1</h2><p>问：高斯混合模型的核心思想是什么?它是如何迭代计算的?</p><p>答：<br>高斯混合模型的核心思想是，假设数据可以看作从多个高斯分布中生成出来<br>的。在该假设下，每个单独的分模型都是标准高斯模型，其均值μ和方差Σ,是待估计的参数。此外，每个分模型都还有一个参数π，可以理解为权重或生成数据的概率。高斯混合模型的公式为<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E7%99%BE%E9%9D%A2%20%E9%9D%9E%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/page120image6336.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>然而，通常我们并不能直接得到高斯混合模型的参数，而是观察到了一系列数据点，给出一个类别的数量K后，希望求得最佳的K个高斯分模型。因此，高斯<br>混合模型的计算，便成了最佳的均值μ,方差Σ、权重π的寻找，这类问题通常通过最大似然估计来求解。遗憾的是，此问题中直接使用最大似然估计,得到的是一个复杂的非凸函数，目标函数是和的对数，难以展开和对其求偏导。</p><p>在这种情况下，可以用上一节已经介绍过的EM算法框架来求解该优化问题。<br>EM算法是在最大化目标函数时，先固定-一个变量使整体函数变为凸优化函数，求导得到最值，然后利用最优参数更新被固定的变量，进入下一一个循环。具体到高斯混合模型的求解，EM算法的迭代过程如下。</p><p>首先，初始随机选择各参数的值。然后，重复下述两步，直到收敛。<br>(1)E步骤。根据当前的参数，计算每个点由某个分模型生成的概率。<br>(2) M步骤。使用E步骤估计出的概率，来改进每个分模型的均值，方差和权重。</p><p>也就是说，我们并不知道最佳的K个高斯分布的各自3个参数，也不知道每个数据点究竟是哪个高斯分布生成的。所以每次循环时，先固定当前的高斯分布不变，获得每个数据点由各个高斯分布生成的概率。然后固定该生成概率不变，根据数据点和生成概率，获得-一个组更佳的高斯分布。循环往复，直到参数不再变化，或者变化非常小时，便得到了比较合理的一组高斯分布。</p><p>高斯混合模型与K均值算法的相同点是，它们都是可用于聚类的算法;都需要指定K值;都是使用EM算法来求解;都往往只能收敛于局部最优。而它相比于K均值算法的优点是，可以给出一个样本属于某类的概率是多少;不仅仅可以用于聚类，还可以用于概率密度的估计;并且可以用于生成新的样本点。</p><h1 id="聚类算法的评估"><a href="#聚类算法的评估" class="headerlink" title="聚类算法的评估"></a>聚类算法的评估</h1><h2 id="问题1-2"><a href="#问题1-2" class="headerlink" title="问题1"></a>问题1</h2><p>问：以聚类问题为例，假设没有外部标签数据，如何评估两个聚类算法的优劣?</p><p>答：非监督学习通常没有标注数据，模型、算法的设计直接影响最终的输出和模型的性能。为了评估不同聚类算法的性能优劣，我们需要了解常见的数据簇的特点。</p><p>●以中心定义的数据簇:这类数据集合倾向于球形分布，通常中心被定义为质心，即此数据簇中所有点的平均值。集合中的数据到中心的距离相比到其他簇中心的距离更近。<br>●以密度定义的数据簇:这类数据集合呈现和周围数据簇明显不同的密度，或稠密或稀疏。当数据簇不规则或互相盘绕，并且有噪声和离群点时，常常使用基于密度的簇定义。<br>● 以连通定义的数据簇:这类数据集合中的数据点和数据点之间有连接关系，整个数据簇表现为图结构。该定义对不规则形状或者缠绕的数据簇有效。<br>● 以概念定义的数据簇:这类数据集合中的所有数据点具有某种共同性质。</p><p>由于数据以及需求的多样性，没有一种算法能够适用于所有的数据类型、数据簇或应用场景，似乎每种情况都可能需要一种不同的评估方法或度量标准。例<br>如，K均值聚类可以用误差平方和来评估,但是基于密度的数据簇可能不是球形，误差平方和则会失效。在许多情况下，判断聚类算法结果的好坏强烈依赖于主观解释。尽管如此，聚类算法的评估还是必需的，它是聚类分析中十分重要的部分之一。</p><p>聚类评估的任务是估计在数据集上进行聚类的可行性，以及聚类方法产生结<br>果的质量。这一过程又分为三个子任务。<br>(1)估计聚类趋势。<br>(2)判定数据簇数<br>(3)测定聚类质量</p><h1 id="自组织映射神经网络"><a href="#自组织映射神经网络" class="headerlink" title="自组织映射神经网络"></a>自组织映射神经网络</h1><p>自组织映射神经网络(Self-Organizing Map，SOM)是无监督学习方法中一类重要方法，可以用作聚类、高维可视化、数据压缩、特征提取等多种用途。在深度神经网络大为流行的今天，谈及自组织映射神经网络依然是一件非常有意义的事情，这主要是由于自组织映射神经网络融入了大量人脑神经元的信号处理机制，有着独特的结构特点。</p><h2 id="问题1-3"><a href="#问题1-3" class="headerlink" title="问题1"></a>问题1</h2><p>问：自组织映射神经网络是如何工作的?它与K均值算法有何区别?</p><p>答：生物学研究表明，在人脑的感知通道上，神经元组织是有序排列的;同时，大脑皮层会对外界特定时空信息的输入在特定区域产生兴奋，而且相类似的外界信息输入产生对应兴奋的大脑皮层区域也连续映像的。例如，生物视网膜中有许多特定的细胞对特定的图形比较敏感，当视网膜中有若千个接收单元同时受特定模式刺激时，就使大脑皮层中的特定神经元开始兴奋，且输入模式接近时与之对应的兴奋神经元也接近;在听觉通道上，神经元在结构排列上与频率的关系十分密切，对于某个频率，特定的神经元具有最大的响应，位置相邻的神经元具有相近的频率特征，而远离的神经元具有的频率特征差别也较大。大脑皮层中神经元的这种响应特点不是先天安排好的，而是通过后天的学习自组织形成的。</p><p>在生物神经系统中，还存在着一种侧抑制现象，即一个神经细胞兴奋后，会对周围其他神经细胞产生抑制作用。这种抑制作用会使神经细胞之间出现竞争，其结果是某些获胜，而另一些则失败。表现形式是获胜神经细胞兴奋，失败神经细胞抑制。自组织神经网络就是对上述生物神经系统功能的一种人工神经网络模拟。</p><p>自组织映射神经网络本质上是一个两层的神经网络，包含输入层和输出层(竞争层)。输入层模拟感知外界输入信息的视网膜,输出层模拟做出响应的大脑皮层。输出层中神经元的个数通常是聚类的个数，代表每一个需要聚成的类。训练时采用“竞争学习”的方式，每个输入的样例在输出层中找到一个和它最匹配的节点，称为激活节点，也叫winning neuron;紧接着用随机梯度下降法更新激活节点的参数;同时，和激活节点临近的点也根据它们距离激活节点的远近而适当地更新参数。这种竞争可以通过神经元之间的横向抑制连接(负反馈路径)来实现。自组织映射神经网络的输出层节点是有拓扑关系的。这个拓扑关系依据需求确定，如果想要-维的模型，那么隐藏节点可以是“-维线阵”;如果想要二维的拓扑关系，那么就行成一个“二维平面阵”，如图5.8所示。也有更高维度的拓扑关系的，比如“三维栅格阵”，但并不常见。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E7%99%BE%E9%9D%A2%20%E9%9D%9E%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-14%20%E4%B8%8B%E5%8D%889.56.05.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>假设输入空间是D维，输入模式为x={xi,i=1.=…D}, 输入单元i和神经元j之间在计算层的连接权重为w= {wi,j= 1,..N,i=1,..,,D},其中N是神经元的总数。自组织映射神经网络的自组织学习过程可以归纳为以下几个子过程。<br>(1)初始化。所有连接权重都用小的随机值进行初始化。<br>(2)竞争。神经元计算每一个输入模式各自的判别函数值，并宣布具有最小判别函数值的特定神经元为胜利者，其中每个神经元j的判别函数为<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E7%99%BE%E9%9D%A2%20%E9%9D%9E%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-14%20%E4%B8%8B%E5%8D%8810.00.29.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>(3)合作。获胜神经元(ω)决定了兴奋神经元拓扑邻域的空间位置。确定激活结点I(x)之后,我们也希望更新和它临近的节点。简单地说,临近的节点距离越远,更新的程度要打更大折扣。<br>(4)适应。适当调整相关兴奋神经元的连接权重,使得获胜的神经元对相似输入模式的后续应用的响应增强。<br>(5)迭代。继续回到步骤(2),直到特征映射趋于稳定。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;EM算法（期望最大）&quot;&gt;&lt;a href=&quot;#EM算法（期望最大）&quot; class=&quot;headerlink&quot; title=&quot;EM算法（期望最大）&quot;&gt;&lt;/a&gt;EM算法（期望最大）&lt;/h1&gt;&lt;p&gt;解决隐变量混合模型的参数估计。&lt;/p&gt;
&lt;p&gt;是软性的聚类，某个数据点可以不
      
    
    </summary>
    
      <category term="知识总结" scheme="https://github.com/zdkswd/categories/%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/"/>
    
    
      <category term="机器学习" scheme="https://github.com/zdkswd/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>聚类</title>
    <link href="https://github.com/zdkswd/2019/03/12/%E8%81%9A%E7%B1%BB/"/>
    <id>https://github.com/zdkswd/2019/03/12/聚类/</id>
    <published>2019-03-12T07:34:12.000Z</published>
    <updated>2019-03-12T07:35:44.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>在“无监督学习”( unsupervised learning)中,训练样本的标记信息是未知的,目标是通过对无标记训练样本的学习来揭示数据的内在性质及规律,为进一步的数据分析提供基础.此类学习任务中研究最多、应用最广的是“聚类”( clustering）</p><p>聚类试图将数据集中的样本划分为若干个通常是不相交的子集,每个子集称为一个“簇”(cluster).通过这样的划分，每个簇可能对应于一些潜在的概念(类别),如“浅色瓜”“深色瓜” ，“有籽瓜”，“无籽瓜”,甚至“本地瓜”，“外地瓜”等;需说明的是,这些概念对聚类算法而言事先是未知的，聚类过程仅能自动形成簇结构,簇所对应的概念语义需由使用者来把握和命名.对聚类算法而言，标记簇亦称为类。</p><p>聚类既能作为一个单独过程,用于找寻数据内在的分布结构,也可作为分类等其他学习任务的前驱过程.例如,在一些商业应用中需对新用户的类型进行判别，但定义“用户类型”对商家来说却可能不太容易，此时往往可先对用户数据进行聚类，根据聚类结果将每个簇定义为一个类,然后再基于这些类训练分类模型,用于判别新用户的类型.</p><h1 id="性能度量"><a href="#性能度量" class="headerlink" title="性能度量"></a>性能度量</h1><p>对聚类结果,我们需通过某种性能度量来评估其好坏;另一方面，若明确了最终将要使用的性能度量,则可直接将其作为聚类过程的优化目标，从而更好地得到符合要求的聚类结果.</p><p>什么样的聚类结果比较好呢?直观上看,我们希望“物以类聚” ’，即同簇的样本尽可能彼此相似,不同簇的样本尽可能不同.换言之,聚类结果的“簇内相似度”(intra-cluster similarity)高且“簇间相似度”(inter-cluster similarity)低.</p><p>聚类性能度量大致有两类一 类是将聚类结果与某个“参考模型”(reference model)进行比较,称为“外部指标”(external index); 另一类是直接考察聚类结果而不利用任何参考模型,称为“内部指标”(internal index).</p><h2 id="外部指标"><a href="#外部指标" class="headerlink" title="外部指标"></a>外部指标</h2><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%81%9A%E7%B1%BB/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-12%20%E4%B8%8A%E5%8D%8810.00.59.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h3 id="Jaccard系数-Jaccard-Coefficient-简称JC"><a href="#Jaccard系数-Jaccard-Coefficient-简称JC" class="headerlink" title="Jaccard系数(Jaccard Coefficient, 简称JC)"></a>Jaccard系数(Jaccard Coefficient, 简称JC)</h3><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%81%9A%E7%B1%BB/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-12%20%E4%B8%8A%E5%8D%8810.02.02.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h3 id="FM指数-Fowlkes-and-Mallows-Index-简称FMI"><a href="#FM指数-Fowlkes-and-Mallows-Index-简称FMI" class="headerlink" title="FM指数(Fowlkes and Mallows Index,简称FMI)"></a>FM指数(Fowlkes and Mallows Index,简称FMI)</h3><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%81%9A%E7%B1%BB/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-12%20%E4%B8%8A%E5%8D%8810.02.28.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h3 id="Rand指数-Rand-Index-简称RI"><a href="#Rand指数-Rand-Index-简称RI" class="headerlink" title="Rand指数(Rand Index,简称RI)"></a>Rand指数(Rand Index,简称RI)</h3><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%81%9A%E7%B1%BB/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-12%20%E4%B8%8A%E5%8D%8810.02.59.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>上述性能度量的结果值均在[0, 1]区间,值越大越好。</p><h2 id="内部指标"><a href="#内部指标" class="headerlink" title="内部指标"></a>内部指标</h2><p>距离越大则样本的相似度越低。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%81%9A%E7%B1%BB/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-12%20%E4%B8%8A%E5%8D%8810.17.20.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><h3 id="DB指数-Davies-Bouldin-Index-简称DBI"><a href="#DB指数-Davies-Bouldin-Index-简称DBI" class="headerlink" title="DB指数(Davies- Bouldin Index,简称DBI)"></a>DB指数(Davies- Bouldin Index,简称DBI)</h3><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%81%9A%E7%B1%BB/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-12%20%E4%B8%8A%E5%8D%8810.18.07.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h3 id="Dunn指数-Dunn-Index-简称DI"><a href="#Dunn指数-Dunn-Index-简称DI" class="headerlink" title="Dunn指数(Dunn Index,简称DI)"></a>Dunn指数(Dunn Index,简称DI)</h3><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%81%9A%E7%B1%BB/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-12%20%E4%B8%8A%E5%8D%8810.18.28.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>DBI的值越小越好，而DI则相反，值越大越好。</p><h2 id="距离计算"><a href="#距离计算" class="headerlink" title="距离计算"></a>距离计算</h2><p>对函数dist(,),若它是一个“距离度量”(distance measure),则需满足一些基本性质:<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%81%9A%E7%B1%BB/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-12%20%E4%B8%8A%E5%8D%8810.36.49.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%81%9A%E7%B1%BB/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-12%20%E4%B8%8A%E5%8D%8810.37.01.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>直递性常被直接称为“三角不等式”。<br>给定样本xi = (xi1;xi2;…;xin) 与xj = (xj1;xj2;..;xjn), 最常用的是“闵可夫斯基距离”(Minkowski distance)<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%81%9A%E7%B1%BB/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-12%20%E4%B8%8A%E5%8D%8810.38.35.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>上式即为xi-xj的Lp范数。<br>当p=2时，闵可夫斯基距离即欧氏距离。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%81%9A%E7%B1%BB/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-12%20%E4%B8%8A%E5%8D%8810.40.34.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>当p=1时，闵可夫斯基距离即曼哈顿距离。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%81%9A%E7%B1%BB/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-12%20%E4%B8%8A%E5%8D%8810.42.10.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><p>闵可夫斯基距离适用于{1,2,3}这样的有序属性，而不适用于{飞机，火车，轮船}这样的无序属性。对无序属性可采用VDM。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%81%9A%E7%B1%BB/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-12%20%E4%B8%8A%E5%8D%8810.46.29.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%81%9A%E7%B1%BB/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-12%20%E4%B8%8A%E5%8D%8810.47.01.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><p>当样本空间中不同属性的重要性不同时,还可使用“加权距离“</p><p>通常我们是基于某种形式的距离来定义“相似度度量”(similarity measure), 距离越大,相似度越小.然而，用于相似度度量的距离未必一定要满足距离度量的所有基本性质,尤其是直递性。</p><h1 id="原型聚类"><a href="#原型聚类" class="headerlink" title="原型聚类"></a>原型聚类</h1><h2 id="k均值算法"><a href="#k均值算法" class="headerlink" title="k均值算法"></a>k均值算法</h2><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%81%9A%E7%B1%BB/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-12%20%E4%B8%8B%E5%8D%8812.30.35.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>最小化式(9.24)并不容易，找到它的最优解需考察样本集D所有可能的簇划分,这是一个NP难问题.因此, k均值算法采用了贪心策略,通过迭代优化来近似求解式(9.24).</p><p>对于以下数据集，为方便叙述，将编号为i的样本称为xi，这是一个包含密度与含糖率两个属性值的二维向量。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%81%9A%E7%B1%BB/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-12%20%E4%B8%8B%E5%8D%8812.38.59.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%81%9A%E7%B1%BB/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-12%20%E4%B8%8B%E5%8D%8812.44.09.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>为避免运行时间过长,通常设置一一个最大运行轮数或最小调整幅度阈值,若达到最大轮数或调整幅度小于阈值,则停止运行。</p><p>假定聚类簇数k= 3,算法开始时随机选取三个样本x6, x12, x27作为初始均值向量,即<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%81%9A%E7%B1%BB/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-12%20%E4%B8%8B%E5%8D%8812.46.12.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>考察样本x1 = (0.697; 0.460),它与当前均值向量μ1, μu2, μ3的距离分别为0.369, 0.506, 0.166, 因此x1将被划入簇C3中.类似的,对数据集中的所有样本<br>考察一遍后, 可得当前簇划分为<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%81%9A%E7%B1%BB/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-12%20%E4%B8%8B%E5%8D%8812.47.13.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>于是，可从C1、C2、C3分别求出新的均值向量<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%81%9A%E7%B1%BB/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-12%20%E4%B8%8B%E5%8D%8812.47.59.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>更新当前均值向量后，不断重复上述过程,如图9.3所示，第五轮迭代产生的结<br>果与第四轮迭代相同，于是算法停止得到最终的簇划分.<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%81%9A%E7%B1%BB/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-12%20%E4%B8%8B%E5%8D%8812.48.47.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><h2 id="学习向量量化"><a href="#学习向量量化" class="headerlink" title="学习向量量化"></a>学习向量量化</h2><p>与k均值算法类似，“ 学习向量量化”(Learning Vector Quantization,简称LVQ)也是试图找到一组原型向量来刻画聚类结构,但与一般聚类算法不同的是,LVQ假设数据样本带有类别标记，学习过程利用样本的这些监督信息来辅助聚类.</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%81%9A%E7%B1%BB/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-12%20%E4%B8%8B%E5%8D%8812.53.04.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%81%9A%E7%B1%BB/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-12%20%E4%B8%8B%E5%8D%8812.57.41.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>在每一轮迭代中，算法随机选取一个有标记的训练样本，找出与其距离最近的原型向量，并根据两者的类别标记是否一致来对原型向量进行响应的更新。若算法的停止条件已满足，例如已达到最大迭代轮数，或原型向量更新很小甚至不再更新，则将当前原型向量作为最终结果返回。</p><p>以西瓜数据集为例演示LVQ的学习过程。假定q=5，即学习目标是找到5个原型向量<strong>p1,p2,p3,p4,p5</strong>,并假定其对应的类别标记分别为c1,c2,c2,c1,c1。</p><p>算法开始时,根据样本的类别标记和簇的预设类别标记对原型向量进行随机初始化，假定初始化为样本<strong>x5, x12, x18, x23, x29</strong>.在第一轮迭代中，假定随机选取的样本为x1,该样本与当前原型向量<strong>p1,p2,p3,p4,p5</strong>的距离分别为0.283, 0.506, 0.434, 0.260, 0.032.由于<strong>p5</strong>与<strong>x1</strong>距离最近且两者具有相同的类别标记c2,假定学习率η= 0.1,则LVQ更新<strong>p5</strong>得到新原型向量<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%81%9A%E7%B1%BB/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-12%20%E4%B8%8B%E5%8D%881.28.14.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%81%9A%E7%B1%BB/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-12%20%E4%B8%8B%E5%8D%881.45.59.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h2 id="高斯混合聚类"><a href="#高斯混合聚类" class="headerlink" title="高斯混合聚类"></a>高斯混合聚类</h2><p>与k均值、LVQ用原型向量来刻画聚类结构不同,高斯混合(Mixture-of-Gaussian)聚类采用概率模型来表达聚类原型.<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%81%9A%E7%B1%BB/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-12%20%E4%B8%8B%E5%8D%881.54.09.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>可定义高斯混合分布<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%81%9A%E7%B1%BB/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-12%20%E4%B8%8B%E5%8D%882.02.00.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%81%9A%E7%B1%BB/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-12%20%E4%B8%8B%E5%8D%882.03.41.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%81%9A%E7%B1%BB/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-12%20%E4%B8%8B%E5%8D%882.03.28.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>按下不表</p><h1 id="密度聚类"><a href="#密度聚类" class="headerlink" title="密度聚类"></a>密度聚类</h1><p>密度聚类亦称“基于密度的聚类”(density-based clustering), 此类算法假设聚类结构能通过样本分布的紧密程度确定.通常情形下，密度聚类算法从样本密度的角度来考察样本之间的可连接性，并基于可连接样本不断扩展聚类簇以获得最终的聚类结果.<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%81%9A%E7%B1%BB/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-12%20%E4%B8%8B%E5%8D%882.10.09.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%81%9A%E7%B1%BB/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-12%20%E4%B8%8B%E5%8D%882.11.07.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>密度直达关系通常不满足对称性，密度可达关系满足直递性,但不满足对称性.密度相连关系满足对称性。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%81%9A%E7%B1%BB/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-12%20%E4%B8%8B%E5%8D%882.22.09.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%81%9A%E7%B1%BB/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-12%20%E4%B8%8B%E5%8D%882.24.24.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>D中不属于任何簇的样本被认为是噪声(noise)或异常(anomaly)样本。</p><p>若x为核心对象,由x密度可达的所有样本组成的集合记为X = {x’∈D |x’由x密度可达},则不难证明X即为满足连接性与最大性的簇.</p><p> DBSCAN算法先任选数据集中的一个核心对象为“种子”(seed),再由此出发确定相应的聚类簇，算法先根据给定的邻域参数找到所有核心对象；然后以任一核心对象为出发点，找到由其密度可达的样本生成聚类簇，直到所有核心对象均被访问过为止。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%81%9A%E7%B1%BB/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-12%20%E4%B8%8B%E5%8D%882.41.24.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>以西瓜数据集为例，假定邻域参数设置为e=0.11，MinPts=5.DBSCAN算法先找出各样本的∈-邻域并确定核心对象集合: S= {x3, x5, x6, x8, x9, x13, x14, x18, x19, x24, x25, x28, x29}.然后，从Ω中随机选取一个核心对象作为种子，找出由它密度可达的所有样本,这就构成了第一个聚类簇.不失一般性,假定核心对象x8被选中作为种子，则DBSCAN生成的第一个聚类簇为<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%81%9A%E7%B1%BB/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-12%20%E4%B8%8B%E5%8D%882.45.11.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>然后, DBSCAN将C1中包含的核心对象从几中去除:Ω=Ω \ C1 ={x3, x5, x9, x13, x14, x24, x25, x28, x2g}.再从更新后的集合8中随机选取一个核心对象作为种子来生成下一一个聚类簇..上述过程不断重复,直至I为空.图9.10显示DBSCAN先后生成聚类簇的情况. C1之后生成的聚类簇为<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%81%9A%E7%B1%BB/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-12%20%E4%B8%8B%E5%8D%883.12.14.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%81%9A%E7%B1%BB/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-12%20%E4%B8%8B%E5%8D%883.12.30.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><h1 id="层次聚类"><a href="#层次聚类" class="headerlink" title="层次聚类"></a>层次聚类</h1><p>层次聚类(hierarchical clustering)试图在不同层次对数据集进行划分，从而形成树形的聚类结构.数据集的划分可采用“自底向上”的聚合策略,也可采用“自顶向下”的分拆策略.</p><p>AGNES是一种采用自底向上聚合策略的层次聚类算法.它先将数据集中的每个样本看作一个初始聚类簇,然后在算法运行的每一步中找出距离最近的两个聚类簇进行合并,该过程不断重复,直至达到预设的聚类簇个数.这里的关键是如何计算聚类簇之间的距离.实际上,每个簇是一个样本集合,因此,只需采用关于集合的某种距离即可。</p><p>对于给定聚类簇Ci，Cj，可通过下面式子来计算距离：<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%81%9A%E7%B1%BB/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-12%20%E4%B8%8B%E5%8D%883.25.52.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>显然,最小距离由两个簇的最近样本决定,最大距离由两个簇的最远样本决定，而平均距离则由两个簇的所有样本共同决定.当聚类簇距离由dmin，dmax或davg计算时，AGNES算法被相应地称为“单链接”(single-linkage)、 “ 全链接”(complete linkage)或“均链接”(average-linkage)算法.</p><p>AGNES算法先对仅含一个样本的初始聚类簇和相应的距离矩阵进行初始化;然后AGNES不断合并距离最近的聚类簇,并对合并得到的聚类簇的距离矩阵进行更新;上述过程不断重复,直至达到预设的聚类簇数.<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%81%9A%E7%B1%BB/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-12%20%E4%B8%8B%E5%8D%883.31.13.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;概述&quot;&gt;&lt;a href=&quot;#概述&quot; class=&quot;headerlink&quot; title=&quot;概述&quot;&gt;&lt;/a&gt;概述&lt;/h1&gt;&lt;p&gt;在“无监督学习”( unsupervised learning)中,训练样本的标记信息是未知的,目标是通过对无标记训练样本的学习来揭示数据的
      
    
    </summary>
    
      <category term="知识总结" scheme="https://github.com/zdkswd/categories/%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/"/>
    
    
      <category term="机器学习" scheme="https://github.com/zdkswd/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>决策树 个人总结</title>
    <link href="https://github.com/zdkswd/2019/03/07/%E5%86%B3%E7%AD%96%E6%A0%91%20%E4%B8%AA%E4%BA%BA%E6%80%BB%E7%BB%93/"/>
    <id>https://github.com/zdkswd/2019/03/07/决策树 个人总结/</id>
    <published>2019-03-07T12:22:47.000Z</published>
    <updated>2019-03-07T12:22:05.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="决策树-个人总结"><a href="#决策树-个人总结" class="headerlink" title="决策树 个人总结"></a>决策树 个人总结</h1><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E5%86%B3%E7%AD%96%E6%A0%91%20%E4%B8%AA%E4%BA%BA%E6%80%BB%E7%BB%93/22.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E5%86%B3%E7%AD%96%E6%A0%91%20%E4%B8%AA%E4%BA%BA%E6%80%BB%E7%BB%93/23.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;决策树-个人总结&quot;&gt;&lt;a href=&quot;#决策树-个人总结&quot; class=&quot;headerlink&quot; title=&quot;决策树 个人总结&quot;&gt;&lt;/a&gt;决策树 个人总结&lt;/h1&gt;&lt;figure class=&quot;image-bubble&quot;&gt;
                &lt;di
      
    
    </summary>
    
      <category term="知识总结" scheme="https://github.com/zdkswd/categories/%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/"/>
    
    
      <category term="机器学习" scheme="https://github.com/zdkswd/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>循环神经网络</title>
    <link href="https://github.com/zdkswd/2019/03/05/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <id>https://github.com/zdkswd/2019/03/05/循环神经网络/</id>
    <published>2019-03-05T01:21:47.000Z</published>
    <updated>2019-03-05T01:23:03.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="博客"><a href="#博客" class="headerlink" title="博客"></a>博客</h1><p>参考<a href="https://cuijiahua.com/blog/2018/12/dl-11.html" target="_blank" rel="noopener">深度学习实战教程(五)：循环神经网络</a></p><p>RNN是在自然语言处理领域中最先被用起来的，使用RNN之前，语言模型主要是采用N-Gram。N可以是一个自然数，比如2或者3。它的含义是，假设一个词出现的概率只与前面N个词相关。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-03%20%E4%B8%8B%E5%8D%887.26.20.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>如果用2-Gram进行建模，那么电脑在预测的时候，只会看到前面的『了』，然后，电脑会在语料库中，搜索『了』后面最可能的一个词。不管最后电脑选的是不是『我』，我们都知道这个模型是不靠谱的，因为『了』前面说了那么一大堆实际上是没有用到的。如果是3-Gram模型呢，会搜索『批评了』后面最可能的词，感觉上比2-Gram靠谱了不少，但还是远远不够的。因为这句话最关键的信息『我』，远在9个词之前！</p><p>读者可能会想，可以提升继续提升N的值呀，比如4-Gram、5-Gram…….。实际上，这个想法是没有实用性的。因为我们想处理任意长度的句子，N设为多少都不合适；另外，模型的大小和N的关系是指数级的，4-Gram模型就会占用海量的存储空间。</p><p>所以，该轮到RNN出场了，RNN理论上可以往前看(往后看)任意多个词。</p><h2 id="基本循环神经网络"><a href="#基本循环神经网络" class="headerlink" title="基本循环神经网络"></a>基本循环神经网络</h2><p>下图是一个简单的循环神经网络如，它由输入层、一个隐藏层和一个输出层组成：<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/dl-11-1.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>循环神经网络实在是太难画出来了，网上所有大神们都不得不用了这种抽象艺术手法。如果把上面有W的那个带箭头的圈去掉，它就变成了最普通的全连接神经网络。x是一个向量，它表示输入层的值（这里面没有画出来表示神经元节点的圆圈）；s是一个向量，它表示隐藏层的值（这里隐藏层面画了一个节点，你也可以想象这一层其实是多个节点，节点数与向量s的维度相同）；U是输入层到隐藏层的权重矩阵。o也是一个向量，它表示输出层的值；V是隐藏层到输出层的权重矩阵。循环神经网络的隐藏层的值s不仅仅取决于当前这次的输入x，还取决于上一次隐藏层的值s。权重矩阵 W就是隐藏层上一次的值作为这一次的输入的权重。</p><p>把上面的图展开，循环神经网络也可以画成下面这个样子：<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/dl-11-2.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>现在看上去就比较清楚了，这个网络在t时刻接收到输入x𝑡之后，隐藏层的值是s𝑡，输出值是o𝑡。关键一点是，s𝑡的值不仅仅取决于x𝑡，还取决于s𝑡−1。我们可以用下面的公式来表示循环神经网络的计算方法：<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/dl-11-3.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>式1是输出层的计算公式，输出层是一个全连接层，也就是它的每个节点都和隐藏层的每个节点相连。V是输出层的权重矩阵，g是激活函数。式2是隐藏层的计算公式，它是循环层。U是输入x的权重矩阵，W是上一次的值s𝑡−1作为这一次的输入的权重矩阵，f是激活函数。</p><p>如果反复把式2带入到式1，我们将得到：<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/dl-11-4.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>从上面可以看出，循环神经网络的输出值𝑜𝑡，是受前面历次输入值x𝑡、x𝑡−1、x𝑡−2、x𝑡−3…影响的，这就是为什么循环神经网络可以往前看任意多个输入值的原因。</p><h2 id="双向循环神经网络"><a href="#双向循环神经网络" class="headerlink" title="双向循环神经网络"></a>双向循环神经网络</h2><p>对于语言模型来说，很多时候光看前面的词是不够的，比如下面这句话：<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-03%20%E4%B8%8B%E5%8D%887.49.40.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>我们需要双向循环神经网络，如下图所示：<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/dl-11-5-1024x358.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>从上图可以看出，双向卷积神经网络的隐藏层要保存两个值，一个A参与正向计算，另一个值A’参与反向计算。最终的输出值y2取决于𝐴2和𝐴′2。其计算方法为：<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/dl-11-6.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>𝐴2 和𝐴′2则分别计算：<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/dl-11-7.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>现在，我们已经可以看出一般的规律：正向计算时，隐藏层的值s𝑡与s𝑡−1有关；反向计算时，隐藏层的值s′𝑡与s′𝑡+1有关；最终的输出取决于正向和反向计算的加和。现在，我们仿照式1和式2，写出双向循环神经网络的计算方法：<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/dl-11-8.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>从上面三个公式我们可以看到，正向计算和反向计算不共享权重，也就是说U和U’、W和W’、V和V’都是不同的权重矩阵。</p><h2 id="深度循环神经网络"><a href="#深度循环神经网络" class="headerlink" title="深度循环神经网络"></a>深度循环神经网络</h2><p>前面我们介绍的循环神经网络只有一个隐藏层，我们当然也可以堆叠两个以上的隐藏层，这样就得到了深度循环神经网络。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/dl-11-9.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>我们把第i个隐藏层的值表示为s(𝑖)𝑡和s′(𝑖)𝑡，则深度循环神经网络的计算方式可以表示为：<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/dl-11-10.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><h2 id="循环神经网络的训练"><a href="#循环神经网络的训练" class="headerlink" title="循环神经网络的训练"></a>循环神经网络的训练</h2><p>循环神经网络的训练算法：BPTT<br>BPTT算法是针对循环层的训练算法，它的基本原理和BP算法是一样的，也包含同样的三个步骤：</p><ol><li>前向计算每个神经元的输出值</li><li>反向计算每个神经元的误差项𝛿𝑗值，它是误差函数E对神经元j的加权输入𝑛𝑒𝑡𝑗的偏导数；</li><li>计算每个权重的梯度。</li></ol><p>最后再用随机梯度下降算法更新权重。</p><p>循环层如下图所示：<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/dl-11-11.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br><strong>前向计算：</strong><br>使用前面的式2对循环层进行前向计算：<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/dl-11-12.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>注意，上面的s𝑡、x𝑡、s𝑡−1都是向量，用黑体字母表示；而U、V是矩阵，用大写字母表示。向量的下标表示时刻，例如，s𝑡表示在t时刻向量s的值。</p><p>我们假设输入向量x的维度是m，输出向量s的维度是n，则矩阵U的维度是n <em> m，矩阵W的维度是n </em> m。下面是上式展开成矩阵的样子，看起来更直观一些：<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/dl-11-13.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>在这里我们用手写体字母表示向量的一个元素，它的下标表示它是这个向量的第几个元素，它的上标表示第几个时刻。例如，𝑠𝑡𝑗表示向量s的第j个元素在t时刻的值。𝑢𝑗𝑖表示输入层第i个神经元到循环层第j个神经元的权重。𝑤𝑗𝑖表示循环层第t-1时刻的第i个神经元到循环层第t个时刻的第j个神经元的权重。</p><p><strong>误差项的计算</strong><br>BTPP算法将第l层t时刻的误差项𝛿𝑙𝑡值沿两个方向传播，一个方向是其传递到上一层网络，得到𝛿𝑙−1𝑡，这部分只和权重矩阵U有关；另一个是方向是将其沿时间线传递到初始𝑡1时刻，得到𝛿𝑙1，这部分只和权重矩阵W有关。</p><p>我们用向量net𝑡表示神经元在t时刻的加权输入，因为：<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/dl-11-14.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>因此：<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/dl-11-15.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>我们用a表示列向量，用a𝑇表示行向量。上式的第一项是向量函数对向量求导，其结果为Jacobian矩阵：<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/dl-11-16.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>同理，上式第二项也是一个Jacobian矩阵：<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/dl-11-17.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>其中，diag[a]表示根据向量a创建一个对角矩阵，即<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/dl-11-18.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>最后，将两项合在一起，可得：<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/dl-11-19.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>上式描述了将𝛿沿时间往前传递一个时刻的规律，有了这个规律，我们就可以求得任意时刻k的误差项𝛿𝑘：<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/dl-11-20.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>式3就是将误差项沿时间反向传播的算法。循环层将误差项反向传递到上一层网络，与普通的全连接层是完全一样的。<br>循环层的加权输入net𝑙与上一层的加权输入net𝑙−1关系如下：<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/dl-11-21.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>上式中net𝑙𝑡是第l层神经元的加权输入(假设第l层是循环层)；net𝑙−1𝑡是第l-1层神经元的加权输入；a𝑙−1𝑡是第l-1层神经元的输出；𝑓𝑙−1是第l-1层的激活函数。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/dl-11-23.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>所以，<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/dl-11-24.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>式4就是将误差项传递到上一层算法。</p><p><strong>权重梯度的计算</strong><br>现在，我们终于来到了BPTT算法的最后一步：计算每个权重的梯度。</p><p>首先，我们计算误差函数E对权重矩阵W的梯度∂𝐸 / ∂𝑊。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/dl-11-25.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/dl-11-28.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>具体推导参考<br><a href="https://cuijiahua.com/blog/2018/12/dl-11.html" target="_blank" rel="noopener">https://cuijiahua.com/blog/2018/12/dl-11.html</a></p><h2 id="RNN的梯度爆炸和消失问题"><a href="#RNN的梯度爆炸和消失问题" class="headerlink" title="RNN的梯度爆炸和消失问题"></a>RNN的梯度爆炸和消失问题</h2><p>不幸的是，实践中前面介绍的几种RNNs并不能很好的处理较长的序列。一个主要的原因是，RNN在训练中很容易发生梯度爆炸和梯度消失，这导致训练时梯度不能在较长序列中一直传递下去，从而使RNN无法捕捉到长距离的影响。</p><p>为什么RNN会产生梯度爆炸和消失问题呢？我们接下来将详细分析一下原因。我们根据式3可得：<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/dl-11-40.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>上式的𝛽定义为矩阵的模的上界。因为上式是一个指数函数，如果t-k很大的话（也就是向前看很远的时候），会导致对应的误差项的值增长或缩小的非常快，这样就会导致相应的梯度爆炸和梯度消失问题（取决于𝛽大于1还是小于1）。</p><p>通常来说，梯度爆炸更容易处理一些。因为梯度爆炸的时候，我们的程序会收到NaN错误。我们也可以设置一个梯度阈值，当梯度超过这个阈值的时候可以直接截取。</p><p>梯度消失更难检测，而且也更难处理一些。总的来说，我们有三种方法应对梯度消失问题：</p><ol><li>合理的初始化权重值。初始化权重，使每个神经元尽可能不要取极大或极小值，以躲开梯度消失的区域。</li><li>使用relu代替sigmoid和tanh作为激活函数。</li><li>使用其他结构的RNNs，比如长短时记忆网络（LTSM）和Gated Recurrent Unit（GRU），这是最流行的做法。</li></ol><h2 id="向量化"><a href="#向量化" class="headerlink" title="向量化"></a>向量化</h2><p>我们知道，神经网络的输入和输出都是向量，为了让语言模型能够被神经网络处理，我们必须把词表达为向量的形式，这样神经网络才能处理它。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/dl-11-43.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>使用这种向量化方法，我们就得到了一个高维、稀疏的向量（稀疏是指绝大部分元素的值都是0）。处理这样的向量会导致我们的神经网络有很多的参数，带来庞大的计算量。因此，往往会需要使用一些降维方法，将高维的稀疏向量转变为低维的稠密向量。</p><p>语言模型要求的输出是下一个最可能的词，我们可以让循环神经网络计算计算词典中每个词是下一个词的概率，这样，概率最大的词就是下一个最可能的词。因此，神经网络的输出向量也是一个N维向量，向量中的每个元素对应着词典中相应的词是下一个词的概率。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/dl-11-44.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><h2 id="Softmax层"><a href="#Softmax层" class="headerlink" title="Softmax层"></a>Softmax层</h2><p>语言模型是对下一个词出现的概率进行建模。那么，怎样让神经网络输出概率呢？方法就是用softmax层作为神经网络的输出层。</p><p>softmax函数的定义：<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/dl-11-45.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>举一个例子。Softmax层如下图所示：<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/dl-11-46.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>计算过程为：<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/dl-11-47.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>输出向量y的特征：</p><ol><li>每一项为取值为0-1之间的正数；</li><li>所有项的总和是1。</li></ol><p>这些特征和概率的特征是一样的，因此我们可以把它们看做是概率。对于语言模型来说，我们可以认为模型预测下一个词是词典中第一个词的概率是0.03，是词典中第二个词的概率是0.09，以此类推。</p><p><strong>交叉熵误差</strong><br>一般来说，当神经网络的输出层是softmax层时，对应的误差函数E通常选择交叉熵误差函数，其定义如下：<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/dl-11-49.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>在上式中，N是训练样本的个数，向量𝑦𝑛是样本的标记，向量𝑜𝑛是网络的输出。标记𝑦𝑛是一个one-hot向量，例如𝑦1=[1,0,0,0]，如果网络的输出𝑜=[0.03,0.09,0.24,0.64]，那么，交叉熵误差是（假设只有一个训练样本，即N=1）：<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/dl-11-50.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>我们当然可以选择其他函数作为我们的误差函数，比如最小平方误差函数(MSE)。不过对概率进行建模时，选择交叉熵误差函数更make sense。具体原因，<br><a href="https://jamesmccaffrey.wordpress.com/2011/12/17/neural-network-classification-categorical-data-softmax-activation-and-cross-entropy-error/" target="_blank" rel="noopener">https://jamesmccaffrey.wordpress.com/2011/12/17/neural-network-classification-categorical-data-softmax-activation-and-cross-entropy-error/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;博客&quot;&gt;&lt;a href=&quot;#博客&quot; class=&quot;headerlink&quot; title=&quot;博客&quot;&gt;&lt;/a&gt;博客&lt;/h1&gt;&lt;p&gt;参考&lt;a href=&quot;https://cuijiahua.com/blog/2018/12/dl-11.html&quot; target=&quot;_bla
      
    
    </summary>
    
      <category term="知识总结" scheme="https://github.com/zdkswd/categories/%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/"/>
    
    
      <category term="机器学习" scheme="https://github.com/zdkswd/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>XGBoost 陈天奇论文 ppt 个人总结</title>
    <link href="https://github.com/zdkswd/2019/03/04/XGBoost%20%E4%B8%AA%E4%BA%BA%E6%80%BB%E7%BB%93/"/>
    <id>https://github.com/zdkswd/2019/03/04/XGBoost 个人总结/</id>
    <published>2019-03-04T08:49:32.000Z</published>
    <updated>2019-03-07T12:15:41.000Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://arxiv.org/pdf/1603.02754.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1603.02754.pdf</a></p><h1 id="阅读目标"><a href="#阅读目标" class="headerlink" title="阅读目标"></a>阅读目标</h1><p>了解作者群<br>1 论文解决了什么问题<br>2 论文核心贡献是什么<br>3 详细研究论文的具体方法</p><h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>文中提到，XGBoost，这个系统的效果已经被大量的机器学习和数据挖掘竞赛所验证。以Kaggle为例，2015年29组优胜方案中17组使用了XGBoost。这其中，8组只是使用了XGBoost去训练模型，其他大部分都是将XGBoost和神经网络做集成。作为对比，第二受欢迎的工具，深度神经网络，有11组采用。在KDDCup 2015中，top-10 每一组都使用了XGBoost。</p><p>在以下问题上提出了优胜的解决方案，包括：商店销售预测，高能物理事件分类，网站文本分类，顾客行为预测，动作检测，广告点击通过率预测，风险预测，大规模的在线课程辍学率预测。</p><p>XGBoost在所有场景中的可扩展性也十分的重要，XGBoost的可扩展性归功于一些重要的系统和算法优化。这些创新包括：一种处理稀疏数据的新奇的树的学习算法；并行和分布式计算让学习进行的更快能够进行更快的模型探索。提出了具有合理理论支撑的分布分位调整框架。</p><p>论文的主要贡献有：</p><ol><li>构建了高可扩展的端到端的boosting系统。</li><li>提出了具有合理理论支撑的分布分位调整框架。</li><li>介绍了一个新奇的并行适应稀疏处理树学习算法。</li><li>提出了基于缓存快的结构便于外存树的学习。</li></ol><p>已经有人做了并行树、基于外存的计算、缓存的计算、稀疏特征的学习等一些列工作，这篇文章最重要的是能够把很多特征结合到一个系统中。</p><p>文章的组织结构：boosting树的正则化（防止过拟合）、树的split方法（decision Tree 使用Gini划分）、系统设计、实验。</p><p>陈天奇XGBoost ppt（内容非常详细）链接:<a href="https://pan.baidu.com/s/10NWfRM9qimswGxPsF9VlDw" target="_blank" rel="noopener">https://pan.baidu.com/s/10NWfRM9qimswGxPsF9VlDw</a>  密码:v3y6</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/add/24.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/add/25.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/add/26.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1603.02754.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://arxiv.org/pdf/1603.02754.pdf&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;阅读目标&quot;&gt;
      
    
    </summary>
    
      <category term="论文" scheme="https://github.com/zdkswd/categories/%E8%AE%BA%E6%96%87/"/>
    
    
      <category term="机器学习" scheme="https://github.com/zdkswd/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>卷积神经网络</title>
    <link href="https://github.com/zdkswd/2019/03/01/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <id>https://github.com/zdkswd/2019/03/01/卷积神经网络/</id>
    <published>2019-03-01T07:01:47.000Z</published>
    <updated>2019-03-03T12:44:18.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="博客"><a href="#博客" class="headerlink" title="博客"></a>博客</h1><p>参考<a href="https://cuijiahua.com/blog/2018/12/dl-10.html" target="_blank" rel="noopener">深度学习实战教程(四)：卷积神经网络</a><br>卷积神经网络常见架构，也就是N个卷积层叠加，然后(可选)叠加一个Pooling层，重复这个结构M次，最后叠加K个全连接层。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/dl-10-3.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><p>卷积神经网络的层结构和全连接神经网络的层结构有很大不同。全连接神经网络每层的神经元是按照一维排列的，也就是排成一条线的样子；而卷积神经网络每层的神经元是按照三维排列的，也就是排成一个长方体的样子，有宽度、高度和深度。</p><p>对于上图展示的神经网络，输入层的宽度和高度对应于输入图像的宽度和高度，而它的深度为1。接着，第一个卷积层对这幅图像进行了卷积操作，得到了三个Feature Map。因为图中看出这个卷积层包含三个Filter，也就是三套参数，每个Filter都可以把原始输入图像卷积得到一个Feature Map，三个Filter就可以得到三个Feature Map。至于一个卷积层可以有多少个Filter，那是可以自由设定的。也就是说，卷积层的Filter个数也是一个<strong>超参数</strong>。我们可以把Feature Map可以看做是通过卷积变换提取到的图像特征，三个Filter就对原始图像提取出三组不同的特征，也就是得到了三个Feature Map，也称做<strong>三个通道</strong>(channel)。</p><p>在第一个卷积层之后，Pooling层对三个Feature Map做了下采样，得到了三个更小的Feature Map。接着，是第二个卷积层，它有5个Filter。每个Fitler都把前面下采样之后的3个Feature Map卷积在一起，得到一个新的Feature Map。这样，5个Filter就得到了5个Feature Map。接着，是第二个Pooling，继续对5个Feature Map进行下采样，得到了5个更小的Feature Map。</p><p>上图最后两层是全连接层，第一个全连接层的每个神经元，和上一层5个Feature Map中的每个神经元相连，第二个全连接层(也就是输出层)的每个神经元，则和第一个全连接层的每个神经元相连，这样得到了整个网络的输出。</p><p>对于卷积操作<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/dl-10-4.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>用X𝑖,𝑗表示图像的第i行第j列元素；对filter的每个权重进行编号，用W𝑚,𝑛表示第m行第n列权重，用𝑤𝑏表示filter的偏置项；对Feature Map的每个元素进行编号，用𝑎𝑖,𝑗表示Feature Map的第i行第j列元素；用f表示激活函数(这个例子选择relu函数作为激活函数)。然后，使用下列公式计算卷积：<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/dl-10-1-2.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>在卷积过程中，可以设置步幅strid。</p><p>图像大小、步幅和卷积后的Feature Map大小有关。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/dl-10-1-5.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>在上面两个公式中，𝑊2是卷积后Feature Map的宽度；𝑊1是卷积前图像的宽度；𝐹是filter的宽度；𝑃是Zero Padding数量，Zero Padding是指在原始图像周围补几圈0，如果𝑃的值是1，那么就补1圈0；𝑆是步幅；𝐻2是卷积后Feature Map的高度；𝐻1是卷积前图像的宽度。式2和式3本质上是一样的。</p><p>如果卷积前的图像深度为D，那么相应的filter的深度也必须为D。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/dl-10-1-7.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/20171209171829551.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>我们前面还曾提到，每个卷积层可以有多个filter。每个filter和原始图像进行卷积后，都可以得到一个Feature Map。因此，卷积后Feature Map的个数和卷积层的filter个数是相同的。</p><p>这里面体现了局部连接和权值共享：每层神经元只和上一层部分神经元相连(卷积计算规则)，且filter的权值对于上一层所有神经元都是一样的。对于包含两个3* 3 *3的fitler的卷积层来说，其参数数量仅有(3 * 3 * 3+1) * 2=56个，且参数数量与上一层神经元个数无关。与全连接神经网络相比，其参数数量大大减少了。</p><p>池化层的操作<br>Pooling层主要的作用是下采样，通过去掉Feature Map中不重要的样本，进一步减少参数数量。Max Pooing之外，常用的还有Mean Pooling——取各样本的平均值。对于深度为D的Feature Map，各层独立做Pooling，因此Pooling后的深度仍然为D。</p><h2 id="卷积神经网络的训练"><a href="#卷积神经网络的训练" class="headerlink" title="卷积神经网络的训练"></a>卷积神经网络的训练</h2><p>和全连接神经网络相比，卷积神经网络的训练要复杂一些。但训练的原理是一样的：利用链式求导计算损失函数对每个权重的偏导数（梯度），然后根据梯度下降公式更新权重。训练算法依然是反向传播算法。</p><p>对于卷积神经网络，由于涉及到局部连接、下采样的等操作，影响到了第二步误差项𝛿的具体计算方法，而权值共享影响了第三步权重𝑤的梯度的计算方法。</p><h3 id="卷积层的训练"><a href="#卷积层的训练" class="headerlink" title="卷积层的训练"></a>卷积层的训练</h3><p>获得了所有的梯度之后，就是根据梯度下降算法来更新每个权重。</p><h3 id="Pooling层的训练"><a href="#Pooling层的训练" class="headerlink" title="Pooling层的训练"></a>Pooling层的训练</h3><p>无论max pooling还是mean pooling，都没有需要学习的参数。因此，在卷积神经网络的训练中，Pooling层需要做的仅仅是将误差项传递到上一层，而没有梯度的计算。<br>详见<a href="https://cuijiahua.com/blog/2018/12/dl-10.html" target="_blank" rel="noopener">https://cuijiahua.com/blog/2018/12/dl-10.html</a></p><h1 id="美团-卷积神经网络"><a href="#美团-卷积神经网络" class="headerlink" title="美团 卷积神经网络"></a>美团 卷积神经网络</h1><p>卷积神经网络中最重要的两个概念是卷积和池化。</p><p>卷积操作在信号处理中可以看作滤波过程，过滤或提取出需要的频段信息。相应的，图像处理中也可实现不同功能的卷积操作，如边缘轮廓，锐化，模糊化等。简言之，卷积核沿着输入矩阵从左到右，从上到下遍历，每到一个网格，其输出是输入矩阵对应位置元素相乘并求和。如下图所示的卷积操作过程可以看出，如果卷积核设计得当（可以通过网络学习出来），不同层次的局部重要信息就能提取出来，另一方面卷积核共享参数能显著降低参数量。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-28%20%E4%B8%8B%E5%8D%888.53.11.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><p>另外一个重要的概念是池化，下图为最大池化的操作。池化有两个作用：一个是降维，另一个就是保持局部不变性，提取抽象信息。深灰色区域在池化后的输出值为6，而且即使该区域的像素发生扰动（像素值不超过6），输出值也不会变，因此具备一定健壮性。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-28%20%E4%B8%8B%E5%8D%888.53.31.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>卷积神经网络在计算机视觉应用非常广，包括图像分类，检测，识别等，从2012年的AlexNet，到2014年的VGG，2015年的GoogleNet，2016年的ResNet，都是卷积神经网络不断演化并在ImageNet比赛上不断刷新成绩。卷积神经网络在自然语言处理方向也有很好的应用，比如情感分类，文本匹配等。</p><h1 id="百面-深度卷积神经网络"><a href="#百面-深度卷积神经网络" class="headerlink" title="百面 深度卷积神经网络"></a>百面 深度卷积神经网络</h1><p>卷积神经网络是一种前馈神经网络，其特点是每层的神经元节点只响应前一层局部区域范围内的神经元（全连接网络中每个神经元节点响应前一层的全部节点）。一个深度卷积神经网络模型通常由若干卷积层叠加若干全连接层组成，中间也包括各种非线性操作以及池化操作。卷积神经网络同样可以使用反向传播算法进行训练，相较于其他网络模型，卷积神经网络的<strong>参数共享</strong>特性使得需要优化的参数数目大大缩减，提高了模型的训练效率以及可扩展性。由于卷积运算主要用于处理类网格结构的数据，因此对于时间序列以及图像数据的分析与识别具有显著优势。</p><h2 id="问题1"><a href="#问题1" class="headerlink" title="问题1"></a>问题1</h2><p>问：卷积操作的本质特性包括系数交互和参数共享，具体解释这两种特性及其作用。</p><p>答：稀疏交互（sparse interaction）<br>在传统神经网络中，网络层之间输入与输出的连接关系可以由一个权值参数矩阵来表示，其中每个单独的参数值都表示了前后层某两个神经元节点之间的交互。对于全连接网络，任意一对输入与输出神经元之间都产生交互，形成稠密的连接结构。</p><p>而在卷积神经网络中，卷积核尺度远小于输入的维度，这样每个输出神经元仅与前一层特定局部区域内的神经元存在连接权重（即产生交互），我们称这种特性为稀疏交互。具体来说，假设网络中相邻两层分别有m个输入和n个输出，全连接网络中权值参数矩阵将包含m * n个参数。对于稀疏交互的卷积神经网络，如果限定每个输出与前一层神经元的连接数为k，那么该层的参数总量为k * n。在实际应用中，一般k值远小于m就可以取得较为可观的效果；而此时优化过程的时间复杂度将会减小几个数量级，过拟合的情况也得到较好的改善。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-28%20%E4%B8%8B%E5%8D%889.57.16.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>稀疏交互的物理意义是，通常图像，文本，语音等现实世界中的数据都具有局部的特征结构，我们可以先学习局部的特征，再将局部的特征组合起来形成更复杂和抽象的特征。</p><p>参数共享（Parameter Sharing）<br>参数共享是指在同一个模型的不同模块中使用相同的参数，它是卷积运算的固有属性。全连接网络中，计算每层的输出时，权值参数矩阵中的每个元素只作用于某个输入元素一次；而在卷积神经网络中，卷积核中的每一个元素将作用于每一次局部输入的特定位置上。根据参数共享的思想，我们只需要学习一组参数集合，而不需要针对每个位置的每个参数都进行优化，从而大大降低了模型的存储需求。</p><p>参数共享的物理意义是使得卷积层具有平移等特性。假如图像中有一只猫，那么无论它出现在图像中的任何位置，我们都应该将它识别为猫，也就是说神经网络的输出对于平移变换来说应当是等变的。特别地，当函数f（x）与g（x）满足f（g（x））=g（f（x））时我们称f（x）关于变换g具有等变性。也就是说，在猫的图片上先进行卷积，再向右平移l像素的输出，与先将图片向右平移l像素再进行卷积操作的输出结果是相等的。</p><h2 id="问题3"><a href="#问题3" class="headerlink" title="问题3"></a>问题3</h2><p>问：卷积神经网络如何用于文本分类任务？</p><p>答：卷积神经网络的核心思想是捕捉局部特征，最初在图像领域取得了巨大的成功，后来在文本领域也得到了广泛的应用。对于文本来说，局部特征就是由若干单词组成的滑动窗口，类似于N-gram特征进行组合和筛选，获得不同抽象层次的语义信息。由于在每次卷积中采用了共享权重的机制，因此它的训练速度相对较快，在实际的文本分类任务中取得了非常不错的效果。</p><p>下图是一个用卷积神经网络模型进行文本表示，并最终用于文本分类的网络结构。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-01%20%E4%B8%8B%E5%8D%881.35.52.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br> 输入层是一个N * K的矩阵，其中N为文章所对应的单词总数，K是每个词对应的表示向量的维度。每个词的K维向量可以是预先在其他语料库中训练好的，也可以作为未知的参数有网络训练得到。这两种方法各有优势，一方面，预先训练的词嵌入可以利用其他语料库得到更多的先验知识；另一方面，由当前网络训练的词向量能够更好地抓住与当前任务相关联的特征。因此，图中的输入层实际采用了两个通道的形式，即有两个N * K的输入矩阵，其中一个用预先训练好的词嵌入表达，并且在训练过程中不再发生变化；另外一个也由同样的方式初始化，但是会作为参数，随着网络的训练发生改变。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;博客&quot;&gt;&lt;a href=&quot;#博客&quot; class=&quot;headerlink&quot; title=&quot;博客&quot;&gt;&lt;/a&gt;博客&lt;/h1&gt;&lt;p&gt;参考&lt;a href=&quot;https://cuijiahua.com/blog/2018/12/dl-10.html&quot; target=&quot;_bla
      
    
    </summary>
    
      <category term="知识总结" scheme="https://github.com/zdkswd/categories/%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/"/>
    
    
      <category term="机器学习" scheme="https://github.com/zdkswd/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>百面机器学习 多层感知机的反向传播算法</title>
    <link href="https://github.com/zdkswd/2019/02/28/%E7%99%BE%E9%9D%A2%20%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%9A%84%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95/"/>
    <id>https://github.com/zdkswd/2019/02/28/百面 多层感知机的反向传播算法/</id>
    <published>2019-02-28T12:01:47.000Z</published>
    <updated>2019-03-18T08:38:44.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="百面-多层感知机的反向传播算法"><a href="#百面-多层感知机的反向传播算法" class="headerlink" title="百面 多层感知机的反向传播算法"></a>百面 多层感知机的反向传播算法</h1><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E7%99%BE%E9%9D%A2%20%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%9A%84%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-28%20%E4%B8%8B%E5%8D%884.35.49.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>在网络训练中，前向传播最终产生一个标量损失函数，反向传播算法则将损失函数的信息沿网络层向后传播用以计算梯度，达到优化网络参数的目的。</p><h2 id="问题1"><a href="#问题1" class="headerlink" title="问题1"></a>问题1</h2><p>问：写出多层感知机的平方误差和交叉熵损失函数。</p><p>答：图片内容说明，图中有平方误差函数以及交叉熵损失函数在二分类和多分类的情况下的表达式。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E7%99%BE%E9%9D%A2%20%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%9A%84%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95/321551343834_.pic_hd.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><h2 id="问题2"><a href="#问题2" class="headerlink" title="问题2"></a>问题2</h2><p>问：根据问题1中定义的损失函数，推导各层参数更新的梯度计算公式。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E7%99%BE%E9%9D%A2%20%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%9A%84%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95/341551353140_.pic.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E7%99%BE%E9%9D%A2%20%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%9A%84%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95/351551353141_.pic.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E7%99%BE%E9%9D%A2%20%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%9A%84%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95/331551353138_.pic.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h2 id="问题3"><a href="#问题3" class="headerlink" title="问题3"></a>问题3</h2><p>问：平方误差损失函数和交叉熵损失函数分别适合什么场景？</p><p>答：一般来说，平方损失函数更适合输出为连续，并且最后一层不含Sigmoid或Softmax激活函数的神经网络；交叉熵损失则更适合二分类或多分类的场景。</p><p>平方损失函数不适合最后一层含有Sigmoid或Softmax激活函数的神经网路的原因是。由（9.23）得，右式最后一项是激活函数的导数，当激活函数为Sigmoid函数时，如果z的绝对值较大，函数的梯度会趋于饱和，即右式最后一项导数的绝对值非常小，导致等式的左边取值非常小，使得基于梯度的学习速度非常缓慢。</p><p>当使用交叉熵损失时，激活函数为Softmax，由（9.27），导数是线性的，因此不会存在学习速度过慢的问题。</p><h2 id="神经元"><a href="#神经元" class="headerlink" title="神经元"></a>神经元</h2><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/add/27.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;百面-多层感知机的反向传播算法&quot;&gt;&lt;a href=&quot;#百面-多层感知机的反向传播算法&quot; class=&quot;headerlink&quot; title=&quot;百面 多层感知机的反向传播算法&quot;&gt;&lt;/a&gt;百面 多层感知机的反向传播算法&lt;/h1&gt;&lt;figure class=&quot;image-
      
    
    </summary>
    
      <category term="知识总结" scheme="https://github.com/zdkswd/categories/%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/"/>
    
    
      <category term="机器学习" scheme="https://github.com/zdkswd/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>激活函数 训练技巧</title>
    <link href="https://github.com/zdkswd/2019/02/28/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%20%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/"/>
    <id>https://github.com/zdkswd/2019/02/28/激活函数 训练技巧/</id>
    <published>2019-02-28T08:00:47.000Z</published>
    <updated>2019-03-10T07:47:38.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="百面-深度神经网络中的激活函数"><a href="#百面-深度神经网络中的激活函数" class="headerlink" title="百面 深度神经网络中的激活函数"></a>百面 深度神经网络中的激活函数</h1><p>线性模型是机器学习领域中最基本也是最重要的工具，以逻辑回归和线性回归为例，无论是通过闭解形式还是使用凸优化，它们都能高效且可靠地拟合数据。然而真实情况中，我们往往会遇到线性不可分的问题（如XOR异或函数），需要非线性变换对数据的分布进行重新映射。对于深度神经网络，我们在每一层线性变换后叠加一个非线性激活函数，以避免多层网络等效于单层线性函数，从而获得更强大的学习与拟合能力。</p><h2 id="问题1"><a href="#问题1" class="headerlink" title="问题1"></a>问题1</h2><p>问：写出常用的激活函数及其导数。</p><p>答：<br><strong>Sigmoid</strong>激活函数的形式为<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%20%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/WechatIMG121551316508_.pic_hd.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>对应的导函数为<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%20%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/WechatIMG161551316509_.pic_hd.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><p><strong>Tanh</strong>激活函数的形式为<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%20%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/WechatIMG141551316509_.pic_hd.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>对应的导函数为<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%20%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/WechatIMG131551316509_.pic_hd.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><p><strong>ReLU</strong>激活函数的形式为<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%20%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/WechatIMG111551316508_.pic_hd.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>对应的导函数为<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%20%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/WechatIMG151551316509_.pic_hd.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><h2 id="问题2"><a href="#问题2" class="headerlink" title="问题2"></a>问题2</h2><p>问：为什么Sigmoid和Tanh激活函数会导致梯度消失的现象？</p><p>答：<strong>Sigmoid</strong>激活函数的曲线如下，它将输入z映射到区间（0，1），当z很大时，f（z）趋近于1；当z很小时，f（z）趋近于0。其导数在z很大或很小时都会趋近于0，造成梯度消失的现象。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%20%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/191551317388_.pic_hd.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><p><strong>Tanh</strong>激活函数的曲线如下。当z很大时，f（z）趋近于1；当z很小时，f（z）趋近于-1.。其导数在z很大或很小时都会趋近于0，同样会出现“梯度消失”。实际上，<strong>Tanh</strong>激活函数相当于<strong>Sigmoid</strong>的平移：<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%20%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/171551317387_.pic_hd.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%20%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/181551317388_.pic_hd.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><h2 id="问题3"><a href="#问题3" class="headerlink" title="问题3"></a>问题3</h2><p>问：ReLU系列的激活函数相对于Sigmoid和Tanh激活函数的优点是什么？它们有什么局限性以及如何改进？</p><p>答：优点：</p><ol><li>从计算的角度，Sigmoid和Tanh激活函数均需要计算指数，复杂度高，而ReLU只需要一个阈值即可得到激活值。</li><li>ReLU的非饱和性可以有效地解决梯度消失的问题，提供相对宽的激活边界。</li><li>ReLU的单侧抑制提供了网络的稀疏表达能力。</li></ol><p>关于稀疏性的解释：通过对大脑的研究发现，大脑在工作的时候只有大约5%的神经元是激活的，而采用sigmoid激活函数的人工神经网络，其激活率大约是50%。有论文声称人工神经网络在15%-30%的激活率时是比较理想的。因为relu函数在输入小于0时是完全不激活的，因此可以获得一个更低的激活率。</p><p>局限性：<br>ReLU的局限性在于其训练过程中会导致神经元的死亡问题。这是由于函数f（z）=max（0，z）导致负梯度在经过该ReLU单元时被置为0，且在之后也不被任何数据激活，即流经该神经元的梯度永远为0，不对任何数据产生响应。在实际训练中，如果学习率设置较大，会导致超过一定比例的神经元不可逆死亡，进而参数梯度无法更新，整个训练过程失败。</p><p>为解决这一问题，人们设计了ReLU的变种Leaky ReLU（LReLU），其形式表示为<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%20%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/201551318136_.pic_hd.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>ReLU和LReLU的函数曲线对比如下图，一般a是一个很小的正常数，这样即实现了单侧抑制，又保留了部分负梯度信息以致不完全丢失，另一方面，a值的选择增加了问题难度，需要较强的人工先验或多次重复训练以确定合适的参数值。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%20%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/211551318136_.pic_hd.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>基于比，参数化的PReLU应运而生。它与LReLU的主要区别是将负轴部分斜率a作为网络中一个可学习的参数，进行反向传播训练，与其他含参数网络层联合优化。而另一个LReLU的变种增加了随机化机制，具体的，在训练过程中，斜率a作为一个满足某种分布的随机采样，测试时再固定下来。Random ReLU在一定程度上能起到正则化的作用。</p><h1 id="百面-神经网络训练技巧"><a href="#百面-神经网络训练技巧" class="headerlink" title="百面 神经网络训练技巧"></a>百面 神经网络训练技巧</h1><p>在大规模神经网络的训练过程中，我们经常会面临过拟合问题，即当参数数目过于庞大而相应的训练数据短缺时，模型在训练集上损失值很小，但在测试集上损失较大，泛化能力很差。解决过拟合的方法有很多，包括数据集增强，参数范数惩罚，正则化，模型集成等。其中Dropout是模型集成方法中最高效与常用的技巧。同时，深度神经网络的训练中涉及诸多手调参数，如学习率，权重衰减系数，Dropout比例等，这些参数的选择会显著影响模型最终的训练效果，批量归一化（Batch Normalization，BN）方法有效规避了这些复杂参数对网络训练产生的影响，在加速训练收敛的同时也提升了网络的泛化能力。</p><h2 id="问题1-1"><a href="#问题1-1" class="headerlink" title="问题1"></a>问题1</h2><p>问：神经网络训练时是否可以将全部参数初始化为0？</p><p>答：考虑全连接的深度神经网络，同一层中的任意神经元都是同构的，它们拥有相同的输入和输出，如果再将参数全部初始化为同样的值，那么无论是前向传播还是反向传播的取值都是完全相同的。学习过程将永远无法打破这种对称性，最终同一网络层中各个参数任然是相同的。</p><p>因此，我们需要随机地初始化神经网络参数的值，以打破这种对称性。简单来说，我们可以初始化参数为取值范围<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%20%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/221551322121_.pic.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>的均匀分布，其中d是一个神经元接受的输入维度。偏置可以被简单地设为0，并不会导致参数对称的问题。</p><h2 id="问题2-1"><a href="#问题2-1" class="headerlink" title="问题2"></a>问题2</h2><p>问：为什么Dropout可以抑制过拟合？它的工作原理和实现？</p><p>答：Dropout是指在深度网络的训练中，以一定的概率随机地“临时丢弃”一部分神经元节点，具体来讲，Dropout作用于每份小批量训练数据，由于其随机丢弃部分神经元的机制，相当于每次迭代都在训练不同结构的神经网络。类比于Bagging方法，Dropout可被认为是一种使用的大规模深度神经网络的模型集成算法。这是由于传统意义上的Bagging涉及多个模型的同时训练与测试评估，当网络与参数规模庞大时，这种集成方式需要消耗大量的运算时间与空间。Dropout在小批量级别上的操作，提供了一种轻量级的Bagging集成近似，能够实现指数级数量神经网络的训练与评测。</p><p>Dropout的具体实现中，要求某个神经元节点激活值以一定的概率p被丢弃，即该神经元暂时停止工作，如下图所示<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%20%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/241551332898_.pic_hd.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>因此，对于包含N个神经元节点的网络，在Dropout的作用下可看作为2^N 个模型的集成。这2^N 个模型可认为是原始网络的子网络，它们共享部分权值，并且具有相同的网络层数，而模型整体的参数数目不变，这就大大简化了运算。对于任意神经元，每次训练中都与一组随机挑选的不同神经元集合共同进行优化，这个过程会减弱全体神经元之间的联合适应性，减小过拟合风险，增强泛化能力。</p><p>在神经网络中应用Dropout包括训练和预测两个阶段。在训练阶段中，每个神经节点需要增加一个概率系数，如下图所示。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%20%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/251551333501_.pic_hd.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>训练阶段又分为前向传播和反向传播两个步骤，原始网络对应的前向传播公式为：<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%20%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/231551332897_.pic_hd.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>应用Dropout之后，前向传播公式变为：<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%20%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/271551334463_.pic_hd.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%20%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/261551334462_.pic_hd.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>上面的Bernoulli函数的作用是以概率系数p随机生成一个取值为0或1的向量，代表每个神经元是否需要被丢弃。如果取值为0，则该神经元将不会计算梯度或参与后面的误差传播。</p><p>测试阶段是前向传播过程。在前向传播的计算时，每个神经元的参数要预先乘以概率系数p，以恢复在训练该神经元只有p的概率被用于整个神经网络的前向传播计算。</p><h2 id="问题3-1"><a href="#问题3-1" class="headerlink" title="问题3"></a>问题3</h2><p>问：批量归一化的基本动机与原理是什么？在卷积神经网络中如何使用？</p><p>答：神经网络训练的本质是学习数据分布，如果训练数据与测试数据的分布不同将大大降低网络的泛化能力，因此我们需要在训练开始前对所有输入数据进行归一化处理。</p><p>然而随着网络训练的进行，每个隐层的参数变化使得后一层的输入发生变化，从而每一批训练数据的分布也随之改变，致使网络在每次迭代中都需要拟合不同的数据分布，增大训练的复杂度以及过拟合的风险。</p><p>批量归一化方法是针对每一批数据，在网络的每一层输入之前增加归一化处理（均值Wie0，标准差为1），将所有批数据强制在统一地数据分布下，即对该层的任意一个神经元（假设为第k维）采用如下公式：<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%20%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/281551335543_.pic.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>x^k 是该层第k个神经元的原始输入数据，E是这一批输入数据在第k个神经元的均值，分母是这一批数据在第k个神经元的标准差。</p><p>批量归一化可以看作在每一层输入和上一层输出之间加入了一个新的计算层，对数据的分布进行额外的约束，从而增强模型的泛化能力。但是批量归一化同时也降低了模型的拟合能力，归一化之后的输入分布被强制为0均值和1标准差。以Sigmoid激活函数为例，批量归一化之后数据整体处于函数的非饱和区域，只包含线性变换，破坏了之前学习到的特征分布。为了恢复原始数据分布，具体实现中引入了变换重构以及可学习参数γ和β：<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%20%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/291551336805_.pic.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>其中γ和β分别是输入数据分布的方差和偏差。对于一般的网络，不采用批量归一化操作时，这两个参数高度依赖前面网络学习到的连接权重（对应复杂的非线性）。而在批量归一化操作中，γ和β变成了该层的学习参数，仅用两个参数就可以恢复最优的输入数据分布，与之前网络层的参数解耦，从而更有利于优化的过程，提高模型的泛化能力。</p><p>完整的批量归一化网络层的前向传导过程公式如下：<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%20%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/301551338705_.pic.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>批量归一化在卷积神经网络中应用时，需要注意卷积神经网络的参数共享机制。每一个卷积核的参数在不同位置的神经元当中是共享的，因此也应该被一起归一化。具体实现中，假设网络训练中每一批包含b个样本，由一个卷积核生成的特征图的宽高分别是w和h，则每个特征图所对应的全部神经元个数为b <em> w </em> h；利用这些神经元对应的所有输入数据，我们根据一组待学习的参数γ和β对每个输入数据进行批量归一化操作。如果有f个卷积核，就对应f个特征图和f组不同的β和γ参数。</p><p>批标准化的使用方法：<br>批标准化一般用在非线性映射（激活函数）之前，对y= Wx + b进行规范化，是结果(输出信号的各个维度)的均值都为0,方差为1,让每一层的输入有一个稳定的分布会有利于网络的训练。<br>在神经网络收敛过慢或者梯度爆炸时的那个无法训练的情况下都可以尝试。<br>批准标化指的是批数据, 把数据分成小批小批进行随机梯度下降. 而且在每批数据进行前向传递的时候, 对每一层都进行标化的处理。</p><h1 id="美团-激活函数"><a href="#美团-激活函数" class="headerlink" title="美团 激活函数"></a>美团 激活函数</h1><p>在深度前馈神经网络中，有个关键的问题需要考虑：非线性变换有什么用，如何进行选择？</p><p>理论上，包含非线性变换的深度前馈神经网络能拟合任何连续函数。非线性变换一方面可以解决线性感知机无法解决的问题（比如XOR问题），真正发挥深层网络的优势。</p><p>Sigmoid有双边饱和效应，当信号超过一定门限后会被抑制，反向梯度容易消失，回传不会多远。而且取值范围（0，1），取值不是以0为中心对称，这样容易产生梯度同正同负的线性，降低收敛速度，适合在最外层用来做概率预测。</p><p>Tanh取值虽然是以0为中心对称，但依然存在饱和问题。</p><p>在深层网络中，ReLU使用最广泛，它以0为中心，右边是线性函数可以保证梯度回传很远，左边直接对信号进行抑制。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;百面-深度神经网络中的激活函数&quot;&gt;&lt;a href=&quot;#百面-深度神经网络中的激活函数&quot; class=&quot;headerlink&quot; title=&quot;百面 深度神经网络中的激活函数&quot;&gt;&lt;/a&gt;百面 深度神经网络中的激活函数&lt;/h1&gt;&lt;p&gt;线性模型是机器学习领域中最基本也是最
      
    
    </summary>
    
      <category term="知识总结" scheme="https://github.com/zdkswd/categories/%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/"/>
    
    
      <category term="机器学习" scheme="https://github.com/zdkswd/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>感知机</title>
    <link href="https://github.com/zdkswd/2019/02/27/%E6%84%9F%E7%9F%A5%E6%9C%BA/"/>
    <id>https://github.com/zdkswd/2019/02/27/感知机/</id>
    <published>2019-02-27T11:28:47.000Z</published>
    <updated>2019-02-28T09:00:09.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="百面机器学习"><a href="#百面机器学习" class="headerlink" title="百面机器学习"></a>百面机器学习</h1><p>深度前馈网络是一类网络模型的统称，常见的多层感知机，自编码器，限制玻尔兹曼机，以及卷积神经网络等，都是其中的成员。<br>在人工神经网络领域中，感知机也被认为是单层的人工神经网络。</p><h2 id="多层感知机与布尔函数"><a href="#多层感知机与布尔函数" class="headerlink" title="多层感知机与布尔函数"></a>多层感知机与布尔函数</h2><p>神经网络概念的诞生很大程度上受到了神经科学的启发。生物学研究表明，大脑皮层的感知与计算功能是分层实现的，例如视觉图像，首先光信号进入大脑皮层的V1区，即初级视皮层，之后依次通过V2层和V4层，即纹外皮层，进入下叶参与物体识别。深度神经网络，除了模拟人脑功能的多层结构，最大的优势在于能够以紧凑简洁的方式来表达比浅层网络更复杂的函数集合（这里的简洁定义为隐层单元的数目与输入单元的数目呈多项式关系）。</p><h3 id="问题1"><a href="#问题1" class="headerlink" title="问题1"></a>问题1</h3><p>问：多层感知机表示异或逻辑时最少需要几个隐含层（仅考虑二元输入）？</p><p>答：回顾逻辑回归的公式Z=sigmoid(AX+BY+C)，发现如果用表示XY的异或关系，采用逻辑回归（即不带隐藏层的感知机）无法精确学习出一个输出为异或的模型表示。</p><p>考虑有一个隐藏层的情况，通用近似定理表示，一个前馈神经网络如果具有线性输出层和至少一层具有任何一种“挤压”性质的激活函数的隐藏层，当给予网络足够数量的隐藏单元时，可以以任意精度近似任何从一个有限维空间到另一个有限维空间的波莱尔可测函数。可以简单的认为我们常用的激活函数和目标函数是通用近似定理适用的一个子集，因此多层感知机的表达能力是非常强的，关键在于我们是否能够学习到对应此表达的模型参数。</p><p>第一个隐藏单元在X和Y均为1时激活，第二个隐藏单元在X和Y均为0时激活，最后再将两个隐藏单元的输出做一个线性变换即可事先异或操作。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E6%84%9F%E7%9F%A5%E6%9C%BA/WechatIMG5.jpeg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><h3 id="问题2"><a href="#问题2" class="headerlink" title="问题2"></a>问题2</h3><p>问：如果只使用一个隐层，需要多少隐节点能够实现包含n元输入的任意布尔函数？</p><p>答：包含n元输入的任意布尔函数可以唯一表示为析取范式，当n=5时的简单范例：<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E6%84%9F%E7%9F%A5%E6%9C%BA/WechatIMG6.jpeg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>最终的输出Y可以表示成由6个合取范式所组成的析取范式。该函数可由包含6个隐节点的3层感知机实现。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E6%84%9F%E7%9F%A5%E6%9C%BA/WechatIMG7.jpeg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>首先证明单个隐节点可以表示任意合取范式。考虑任意布尔变量假设Xi，若它在合取范式中出现的形式为正（Xi），则设权重为1；若出现的形式为非，则设置权重为-1；若没有在合取范式中出现。设置权重为0；并且偏置设为合取范式中变量的总数取负加1。可以看出，当采用ReLU激活函数之后，当且仅当所有出现的布尔变量均满足条件时，该隐藏单元才会被激活（输出1），否则输出0，这与合取范式的定义的相符。然后，令所有隐藏单元到输出层的参数为1，并设输出单元的偏置为0.这样，当且仅当所有的隐藏单元到输出层的参数为1，并设输出单元的偏置为0.这样，当且仅当所有的隐藏单元都未被激活时才会输出0，否则都将输出一个正数，起到了析取的作用。</p><p>可以使用卡诺图表示析取式，即用网格表示真值表，当输入的合取式值为1时，则填充相应的网格。卡诺图中相邻的填色区域可以进行规约，以达到化简布尔函数的目的。该函数可由包含3个隐节点的3层感知机实现：<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E6%84%9F%E7%9F%A5%E6%9C%BA/WechatIMG9.jpeg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>回顾初始的问题：在最差的情况下，需要多少个隐藏结点来表示包含n元输入的布尔函数呢？现在问题可以转化为：寻找“最大不可规约的”n元析取范式，也等价于最大不可规约的卡诺图。直观上，我们只需间隔填充网格即可实现，其表示的布尔函数恰为n元输入的异或操作，如下图，容易看出，在间隔填充的网格上反转任意网格的取值都会引起一次规约，因此，n元布尔函数的析取范式最多包含2^(n-1)个不可规约的合取范式，对于单隐层的感知机，需要2^(n-1)个隐节点来实现。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E6%84%9F%E7%9F%A5%E6%9C%BA/WechatIMG8.jpeg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><h3 id="问题3"><a href="#问题3" class="headerlink" title="问题3"></a>问题3</h3><p>问：考虑多隐层的情况，实现包含n元输入的任意布尔函数最少需要多少个网络节点和网络层？</p><p>答：参考问题1的解答，考虑二元输入的情况，需要3个节点可以完成一次异或操作，其中隐藏层由两个节点构成，输出层需要一个结点，用来输出异或的结果并作为下一个结点的输入。对于四元输入，包含三次异或操作，需要3 * 3=9个节点即可完成。如下图<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E6%84%9F%E7%9F%A5%E6%9C%BA/WechatIMG10.jpeg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>输入W，X，Y，Z四个布尔变量，首先用3个结点计算W异或X；然后再加入3个结点，将W，X的输出与Y进行异或，得到W异或X异或Y，最后与Z进行异或，整个网络总共需要9个结点。以此类推，n元异或函数需要包括3（n-1）个节点（包括最终输出节点）。多隐层结构可以将隐节点的数目从指数级O（2^(n-1)）直接减少至线性级O（3（n-1））。</p><p>层数还可以进一步减小，如果在同一层中计算W异或X和Y异或Z，再将二者的输出进行异或就能将层数从6变成4，根据二分思想，每层节点两两分组进行异或运算，需要的最少网络层数为2log2N（向上取整）。</p><h1 id="美团-深度学习技术发展历程"><a href="#美团-深度学习技术发展历程" class="headerlink" title="美团 深度学习技术发展历程"></a>美团 深度学习技术发展历程</h1><h2 id="第一阶段-1943到1986年"><a href="#第一阶段-1943到1986年" class="headerlink" title="第一阶段 1943到1986年"></a>第一阶段 1943到1986年</h2><p>1943年，人工神经元模型MCP（作者名字的缩写）的发明可以看做是人工神经网络的起点。MCP模型由多个输入加权求和，二值激活函数组成，通过网络来模拟神经元的过程。</p><p>1957年，感知机算法Perceptron的发明，使用MCP模型对多维输入做二分类，梯度下降法学习权重。感知机算法的思想很简单，即通过样本正确与否调整分类面，使得分类面对样本的分类误差最小。该算法是神经网络和支持向量机的基础，随后被证明能够收敛，其理论和实践效果引发了第一次神经网络浪潮。</p><p>1969年，美国数学家和人工智能先驱Minsky证明了感知机是一种线性模型，它只能处理线性分类问题，比如简单的异或问题（XOR）就解决不了。从而神经网络陷入了第一次近20年的停止期。</p><h2 id="第二阶段-1986年到2006年"><a href="#第二阶段-1986年到2006年" class="headerlink" title="第二阶段 1986年到2006年"></a>第二阶段 1986年到2006年</h2><p>1986年，Hinton发明了优化多层感知机（MLP）的反向传播算法（BP）算法，从而解决了神经网络只能解决线性分类的问题，引发了第二轮研究热潮。三年后，就证明了MLP的万能逼近原理，也就是包含非线性隐层的MLP能逼近任意的联系函数，极大鼓舞了神经网络研究热情。</p><p>1990年，20世纪90年代，支持向量机如火如荼发展起来，理论上有很好的解释，且效果非常好。而神经网络开始走下坡路，一方面BP在深层网络中存在梯度消失和爆炸问题，另一方面其理论解释没那么完善，就这样神经网络一直低迷到2006年。</p><h2 id="第三阶段-2006年至今"><a href="#第三阶段-2006年至今" class="headerlink" title="第三阶段 2006年至今"></a>第三阶段 2006年至今</h2><p>2006年。Hinton有了重大发明，提出的无监督分层初始化方法结合深度玻尔兹曼机解决了深层神经网络梯度消失等难题。这项发明发表在science期刊上，从而开启了深度学习第三次研究热潮。</p><p>2012年，ImageNet比赛夺冠AlexNet，引爆深度学习研究热潮。</p><p>2015年至今，各大公司纷纷开源其深度学习框架和模型，极大推进了各领域应用深度学习的进程。</p><h1 id="统计学习方法-感知机"><a href="#统计学习方法-感知机" class="headerlink" title="统计学习方法  感知机"></a>统计学习方法  感知机</h1><p>感知机(perceptron)是二类分类的线性分类模型,其输入为实例的特征向量, 输出为实例的类别,取+1和-1二值.感知机对应于输入空间(特征空间)中将实例划分为正负两类的分离超平面,属于判别模型.感知机学习旨在求出将训练数据进行线性划分的分离超平面,为此,导入基于误分类的损失函数,利用梯度下降法对损失函数进行极小化,求得感知机模型.感知机学习算法具有简单而易于实现的优点,分为原始形式和对偶形式.感知机预测是用学习得到的感知机模型对新的输入实例进行分类.感知机1957年由 Rosenblat提出,是神经网络与支持向量机的基础。</p><h2 id="感知机模型"><a href="#感知机模型" class="headerlink" title="感知机模型"></a>感知机模型</h2><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E6%84%9F%E7%9F%A5%E6%9C%BA/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-27%20%E4%B8%8B%E5%8D%887.14.25.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>感知机是一种线性分类模型，属于判别模型。感知机模型的假设空间是定义在特征空间中的所有线性分类模型或线性分类器。</p><p>感知机的几何解释，线性方程wx+b=0，对于特征空间R中的一个超平面S，其中w是超平面的法向量，b是超平面的截距。这个超平面将特征空间划分为两个部分。位于两部分的点（特征向量）分别为正负两类，因此，超平面S被称为分离超平面。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E6%84%9F%E7%9F%A5%E6%9C%BA/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-27%20%E4%B8%8B%E5%8D%887.19.26.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><p>感知机学习的策略是极小化损失函数：<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E6%84%9F%E7%9F%A5%E6%9C%BA/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-27%20%E4%B8%8B%E5%8D%887.22.22.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>损失函数对应于误分类点到分离超平面的总距离。</p><p>感知机学习算法是基于随机梯度下降法的对损失函数的最优化算法，有原始形式和对偶形式.算法简单且易于实现.原始形式中，首先任意选取一个超平面，然后用梯度下降法不断极小化目标函数.在这个过程中一次随机选取一个误分类点使其梯度下降。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;百面机器学习&quot;&gt;&lt;a href=&quot;#百面机器学习&quot; class=&quot;headerlink&quot; title=&quot;百面机器学习&quot;&gt;&lt;/a&gt;百面机器学习&lt;/h1&gt;&lt;p&gt;深度前馈网络是一类网络模型的统称，常见的多层感知机，自编码器，限制玻尔兹曼机，以及卷积神经网络等，都是其中的
      
    
    </summary>
    
      <category term="知识总结" scheme="https://github.com/zdkswd/categories/%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/"/>
    
    
      <category term="机器学习" scheme="https://github.com/zdkswd/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>FM FFM</title>
    <link href="https://github.com/zdkswd/2019/02/26/FM%20FFM/"/>
    <id>https://github.com/zdkswd/2019/02/26/FM FFM/</id>
    <published>2019-02-26T10:27:56.000Z</published>
    <updated>2019-02-26T10:28:43.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="FM-FFM"><a href="#FM-FFM" class="headerlink" title="FM FFM"></a>FM FFM</h1><h1 id="深入浅出Factorization-Machines系列"><a href="#深入浅出Factorization-Machines系列" class="headerlink" title="深入浅出Factorization Machines系列"></a>深入浅出Factorization Machines系列</h1><p><a href="http://kubicode.me/2018/02/23/Deep%20Learning/Deep-in-out-Factorization-Machines-Series/" target="_blank" rel="noopener">深入浅出Factorization Machines系列 | Kubi Code’Blog</a></p><h1 id="FM"><a href="#FM" class="headerlink" title="FM"></a>FM</h1><p>Factorization Machine(FM)由Steffen Rendle在2010年提出，旨在解决系数数据下的特征组合的问题，目前该系列模型在搜索推荐领域被广泛使用。</p><h2 id="一个例子"><a href="#一个例子" class="headerlink" title="一个例子"></a>一个例子</h2><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/FM%20FFM/fm_case.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>问题就是需要对电影进行评分(y项)，而x都是特征,其中:</p><ol><li>第一部分蓝色的为当前评分的用户</li><li>第二部分红色的为被评分的电影</li><li>第三部分黄色的为该用户曾经对其他电影的评分情况</li><li>第四部分绿色的为该用户当前评分的月数</li><li>第五部分棕色为该用户最新一次评分的电影</li></ol><p>这是一个经典的回归问题，最简单粗暴的方法就先上一个线性回归，其中对于绿色特征处理成binary，这样计算公式就是为<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/FM%20FFM/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-26%20%E4%B8%8A%E5%8D%889.45.36.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>这样可能会过于简单粗暴，按照算法（特征）工程师的套路会对某些特征进行组合，这样为了方便，咱们就给他来一个全组合:<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/FM%20FFM/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-26%20%E4%B8%8A%E5%8D%889.53.57.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>看似问题解决了，但是这样会存在这么几个问题:</p><ol><li>参数空间过大,这里为O(n2)，在处理互联网数据时，特征量级别可能是亿级别的。</li><li>需要人工经验，这里一般会选择某些特征来组合，此时人工/专家经验就会很重要。</li><li>样本量稀疏，实际上那这种方式拿到的特征会是很稀疏的，对于在训练样本中未出现过的组合该模型无能为力。</li></ol><h2 id="FM解法"><a href="#FM解法" class="headerlink" title="FM解法"></a>FM解法</h2><p>定理：对于一个正定矩阵W，始终存在一个矩阵使得<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/FM%20FFM/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-26%20%E4%B8%8A%E5%8D%8810.00.38.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>成立（需要V的维数k足够大）<br>但是在巨大稀疏矩阵的情况下，当k并不是很大时也可以很接近W，因此可以用<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/FM%20FFM/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-26%20%E4%B8%8A%E5%8D%8810.01.54.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>其中这里v为长度k的一个向量,⟨vi,vj⟩表示两个向量的点积，在FM中也称为<strong>隐向量</strong>,这样就有了FM的式子:<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/FM%20FFM/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-26%20%E4%B8%8A%E5%8D%8810.02.27.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>其中&lt;&gt;表示两个向量的点积<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/FM%20FFM/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-26%20%E4%B8%8A%E5%8D%8810.05.42.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>直观上看，FM的复杂度是 O(kn2)。但是，通过下列等式，FM的二次项可以化简，其复杂度可以优化到 O(kn)。由此可见，FM可以在线性时间对新样本作出预测。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/FM%20FFM/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-26%20%E4%B8%8A%E5%8D%8810.08.06.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>以下是详细证明过程。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/FM%20FFM/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-26%20%E4%B8%8A%E5%8D%8810.09.13.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>之后采用随机梯度下降SGD（Stochastic Gradient Descent）训练模型参数。那么，模型各个参数的梯度如下:<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/FM%20FFM/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-26%20%E4%B8%8A%E5%8D%8810.11.53.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/FM%20FFM/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-26%20%E4%B8%8A%E5%8D%8810.19.33.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><h2 id="FM总结"><a href="#FM总结" class="headerlink" title="FM总结"></a>FM总结</h2><p>首先是为什么使用向量的点积可以解决以上问题呢？</p><ol><li>参数的数量大幅度缩减，从n×(n−1)/2降低到nk</li><li>隐向量的点积可以表示原本两个毫无相关的参数之间的关系</li><li>可以解决稀疏向量问题，因为每个特征都有一个隐向量，就算是稀疏向量在训练样本没有出现过的组合在预测时也可以进行计算。</li></ol><h2 id="扩展"><a href="#扩展" class="headerlink" title="扩展"></a>扩展</h2><p>FM与矩阵分解MF与SVM有什么差别呢？</p><ol><li>FM是一种比较灵活的模型，通过合适的特征变换方式，FM可以模拟二阶多项式核的SVM模型、MF模型、SVD++模型等。</li><li>相比SVM的二阶多项式核而言，FM在样本稀疏的情况下是有优势的；而且，FM的训练/预测复杂度是线性的，而二项多项式核SVM需要计算核矩阵，核矩阵复杂度就是N平方。</li><li>相比MF而言，我们把MF中每一项的rating分改写为rui∼βu+γi+xTuyi，从此公式中可以看出，这相当于只有两类特征 β 和 γ 的FM模型。对于FM而言，我们可以加任意多的特征，比如user的历史购买平均值，item的历史购买平均值等，但是MF只能局限在两类特征。SVD++与MF类似，在特征的扩展性上都不如FM。</li></ol><h1 id="FFM"><a href="#FFM" class="headerlink" title="FFM"></a>FFM</h1><p>Field-aware Factorization Machine(FFM) 场感知分解机。</p><p>场感知说白了可以理解为分类。通过引入field的概念，FFM把相同性质的特征归于同一个field。比如， “MovieClass = romantic”、“MovieClass = action”这2个特征值都是代表电影分类的，可以放到同一个field中。简单来说，同一个类别的特征经过One-Hot编码生成的数值特征都可以放到同一个field。</p><p>在FFM中，每一维特征 xi，针对其它特征的每一种field fj，都会学习一个隐向量<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/FM%20FFM/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-26%20%E4%B8%8B%E5%8D%881.23.02.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>因此，隐向量不仅与特征相关，也与field相关。也就是说，“MovieClass”这个特征与“UserRate”特征和“PlayTimes”特征进行关联的时候使用不同的隐向量，也是FFM中“field-aware”的由来。<br>通过修改FM的公式，我们可以得出：<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/FM%20FFM/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-26%20%E4%B8%8B%E5%8D%881.26.49.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/FM%20FFM/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-26%20%E4%B8%8B%E5%8D%881.28.19.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><h2 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h2><p>FFM其实是在FM的基础上做了一些更加细致化的工作:作者Yuchin认为相同性质的特征归于同一field，而当前特征在不同field上的表现应该是不同的.</p><p>比如在广告领域中性别对于广告商(Advertiser)和投放地(Publisher)的作用就是不一样的，比如:<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/FM%20FFM/ffm_case.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>这里的特征被分为了三类，有投放地(Publisher)，广告商(Advertiser)和性别(Gender),如果使用<strong>FM</strong>来预估这个点击率则是:<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/FM%20FFM/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-26%20%E4%B8%8B%E5%8D%881.46.37.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>这里可以看出FM中隐向量对于不同类别的特征进行组合时都是使用同一个向量，而基于Field-aware的FFM就是对这点进行修改，认为当前向量对于每一个类别都有一个不同的隐向量，比如性别和投放地进行组合的时候使用的隐向量为vMale,G,这样推广开来之后这个问题中FFM的二阶项就可以表述为:<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/FM%20FFM/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-26%20%E4%B8%8B%E5%8D%881.47.35.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>这样,FFM使用通用化的学习公式表达了之后为:<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/FM%20FFM/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-26%20%E4%B8%8B%E5%8D%881.48.53.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>因为FFM的参数空间为nfk,其计算复杂度为O(nk),但是FFM都是在特定的field的中来学习训练隐向量的，所以一般来说:<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/FM%20FFM/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-26%20%E4%B8%8B%E5%8D%881.49.50.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>FFM的改进看上去还是有挺有道理的，但是其实最终实验做出来和FM的效果不相上下。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/FM%20FFM/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-26%20%E4%B8%8B%E5%8D%881.50.29.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><h1 id="美团机器学习实践"><a href="#美团机器学习实践" class="headerlink" title="美团机器学习实践"></a>美团机器学习实践</h1><p>逻辑回归无法学习到特征间的组合关系，而特征组合关系在推荐和CTR预估中却是比较常见的。在进行点击率预估时，特征通常来自于用户，广告和上下文环境，如果没有对这些特征进行组合，模型就无法学习到所有有用的信息。例如，同一个用户在不同时间或者地点感兴趣的广告是不同的，同一件商品在不同地区的受欢迎程度也是不同的。但是人工对特征组合需要做大量的特征工程工作，对特征做暴力组合模型又太复杂，参数太多。模型训练迭代无论是内存开销还是时间开销都让人很难接受，迭代效果往往也比较差。所以可以用因子分解机和场感知因子分解机来进行自动做特征组合，并且算法效率比较高。 </p><p>利用模型来做特征组合，很容易想到使用支持向量机的核函数来实现特征之间的交叉。但是多项式核函数的问题就是二次参数过多。设特征维数为n，则二次项的参数数目为n(n+1) / 2，特别是某些广告ID，用户ID类特征，其特征维数可能达到几百万维，这导致只有极少数的二阶组合模式才能找到，所以这些特征组合后得到的特征矩阵就是十分稀疏。而在训练样本不足的时候，特征矩阵的稀疏性很容易导致相关参数准确性较低，导致模型效果不好。而我们可以通过对二次项参数施加某种限制来减少参数的自由度。</p><p>因子分解机施加的限制就是要求二次项参数矩阵是低秩的，能够分解为低秩矩阵的乘积。所有二次项参数矩阵W就可以分解为<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/FM%20FFM/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-26%20%E4%B8%8A%E5%8D%8810.00.38%202.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br><strong>V</strong>的第j列便是第j维特征向量。<br>因子分解机的模型方程：<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/FM%20FFM/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-26%20%E4%B8%8B%E5%8D%881.26.49%202.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>在多项式模型中，Whi和Wij是相互独立的，但参数因子化使得XhXi和XiXj的系数分别为&lt;Vh,Vi&gt;和&lt;Vi,Vj&gt;，它们有了共同项Vi。也就是说<strong>所有包含Xj的非零组合特征的样本都可以用来学习隐向量Vi</strong>，这在<strong>很大程度上避免了数据稀疏性造成的影响</strong>。</p><p>FM可以看做是FFM的特殊情况，是把所有特征都归属到一个场时的场感知因子分解机模型。</p><h2 id="FFM的应用"><a href="#FFM的应用" class="headerlink" title="FFM的应用"></a>FFM的应用</h2><p>FFM可以自动做特征组合和处理高维稀疏特征，因而它在处理大量离散特征的问题上往往有比较好的效果。使用场感知因子分解机时要注意对连续特征做归一化或离散化。</p><p>FM，FFM与其他模型的对比关系。</p><h3 id="FM与FFM"><a href="#FM与FFM" class="headerlink" title="FM与FFM"></a>FM与FFM</h3><p>场感知因子分解机对因子分解机模型引入场的概念，增加了模型复杂度和模型表达能力。可以将因子分解机理解为场感知因子分解机的特殊简化模式，即所有特征都属于同一个场。</p><h3 id="FM与神经网络"><a href="#FM与神经网络" class="headerlink" title="FM与神经网络"></a>FM与神经网络</h3><p>神经网络难以直接处理高维稀疏的离散特征，因为这导致神经元的连接参数太多。而因子分解机可以看做对高维稀疏的离散特征做嵌入（Embedding）。</p><h3 id="FM和梯度提升树"><a href="#FM和梯度提升树" class="headerlink" title="FM和梯度提升树"></a>FM和梯度提升树</h3><p>因子分解机与梯度提升树都可以做特征组合，Facebook就基于梯度提升树学习过特征的组合，梯度提升树可以方便对特征做高阶组合。当数据不是高度稀疏时，梯度提升树可以有效地学习到比较复杂的特征组合；但是在高度稀疏的数据中，特征二阶组合的数量就足以让绝大多数模式找不到样本，因而梯度提升树无法学习到这种高阶组合。</p><h3 id="因子分解机与其他模型"><a href="#因子分解机与其他模型" class="headerlink" title="因子分解机与其他模型"></a>因子分解机与其他模型</h3><p>因子分解机是一种比较灵活的模型，通过合适的特征变换方式，因子分解机可以模拟二阶多项式核的支持向量机模型、MF模型、SVD++模型等。但SVD++与MF在特征的扩展性上都不如因子分解机，而支持向量机核函数计算复杂度较高。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;FM-FFM&quot;&gt;&lt;a href=&quot;#FM-FFM&quot; class=&quot;headerlink&quot; title=&quot;FM FFM&quot;&gt;&lt;/a&gt;FM FFM&lt;/h1&gt;&lt;h1 id=&quot;深入浅出Factorization-Machines系列&quot;&gt;&lt;a href=&quot;#深入浅出Facto
      
    
    </summary>
    
      <category term="知识总结" scheme="https://github.com/zdkswd/categories/%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/"/>
    
    
      <category term="机器学习" scheme="https://github.com/zdkswd/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>百面机器学习 美团 逻辑回归</title>
    <link href="https://github.com/zdkswd/2019/02/25/%E7%99%BE%E9%9D%A2%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%20%20%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
    <id>https://github.com/zdkswd/2019/02/25/百面机器学习  逻辑回归/</id>
    <published>2019-02-25T08:31:47.000Z</published>
    <updated>2019-02-26T13:51:38.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="百面机器学习-逻辑回归"><a href="#百面机器学习-逻辑回归" class="headerlink" title="百面机器学习  逻辑回归"></a>百面机器学习  逻辑回归</h1><p>逻辑回归可以说是机器学习领域最基础也是最常用的模型，逻辑回归的原理推导以及扩展应用几乎是算法工程师的必备技能。</p><h2 id="问题1"><a href="#问题1" class="headerlink" title="问题1"></a>问题1</h2><p>问：逻辑回归相比于线性回归，有何异同？</p><p>答：逻辑回归，乍一听名字和数学中的线性回归异派同源，但本质却是大相径庭。</p><p>首先，逻辑回归处理的是分类问题，线性回归处理的是回归问题，这是这两者最本质的区别。逻辑回归中，因变量取值是一个二元分布，模型学习得出的是E[y|x;θ]，即给定自变量和超参数后，得到因变量的期望，并基于此期望来处理预测分类问题。</p><p>分类和回归是如今机器学习中两个不同的任务，而属于分类算法的逻辑回归，其命名有一定的历史原因。</p><p>将逻辑回归的公式进行整理，我们可以得到<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E7%99%BE%E9%9D%A2%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%20%20%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/WechatIMG4.jpeg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>其中p=P(y=1|x)，也就是将给定输入x预测为正样本的概率。如果把一个事件的几率定义为该事件发生的概率与该事件不发生的概率的比值 p / 1-p，那么逻辑回归可以看做是对于“y=1|x”这一事件的对数几率的线性回归，于是“逻辑回归”这一称谓也就延续下来了。</p><p>在关于逻辑回归的讨论中，我们均认为y是因变量，而非p / 1-p ，这便引出逻辑回归与线性回归<strong>最大的区别</strong>。即逻辑回归中的因变量为离散的，而线性回归中的因变量是连续的。并且在自变量x与超参数θ确定的情况下，逻辑回归可以看作广义线性模型在因变量y服从二元分布时的一个特殊情况；而使用最小二乘法求解线性回归时，我们认为因变量y服从正态分布。</p><p><strong>相同之处</strong>：首先二者都使用了极大似然估计来对训练样本进行建模。线性回归使用最小二乘法，实际上就是在自变量x与超参数θ确定，因变量y服从正态分布的假设下，使用极大似然估计的一个化简；而逻辑回归中通过对似然函数的学习，得到最佳参数θ。另外，二者在求解超参数过程中，都可以使用梯度下降的方法，这也是监督学习中一个常见的相似之处。</p><h2 id="问题2"><a href="#问题2" class="headerlink" title="问题2"></a>问题2</h2><p>问：当使用逻辑回归处理多标签的分类问题时，有哪些常见做法，分别应用于哪些场景，它们之间又有着怎样的关系？</p><p>答：使用哪一种方法来处理多分类的问题取决于具体问题的定义。首先，如果一个样本只对应于一个标签，我们可以假设每个样本属于不同标签的概率服从于几何分布，使用多项逻辑回归（Softmax Regression）来进行分类。多项逻辑回归实际上是二分类逻辑回归在多标签分类下的一种扩展。</p><p>当存在样本可能属于多个标签的情况时，我们可以训练k个二分类的逻辑回归分类器。第i个分类器用以区分每个样本是否可以归为第i类，训练该分类器时，需要把标签重新整理为“第i类标签”与“非第i类标签”两类。通过这样的办法，我们就解决了每个样本可能拥有多个标签的情况。</p><h1 id="美团机器学习实践"><a href="#美团机器学习实践" class="headerlink" title="美团机器学习实践"></a>美团机器学习实践</h1><p>逻辑回归是一种广义线性模型，它与线性回归模型包含的线性函数十分相似。但逻辑回归通过对数概率函数将线性函数的结果进行映射，目标函数的取值空间从（-∞，+∞）映射到（0，1）。从而可以处理分类问题。逻辑回归虽然有“回归”二字，却是统计学习中经典分类方法。</p><h2 id="逻辑回归原理"><a href="#逻辑回归原理" class="headerlink" title="逻辑回归原理"></a>逻辑回归原理</h2><p>将线性回归与逻辑回归进行对比，可以发现线性回归模型在训练时在整个实数域上对异常点的敏感性是一致的，因而在处理分类问题时线性回归模型效果较差，线性回归模型不适合处理分类问题。对于二分类任务，逻辑回归输出标记y∈{0,1}，而线性回归模型产生的预测值是实数，所以需要一个映射函数将实值转换为0 / 1值。</p><p>最理想的映射函数是单位阶跃函数，即预测值大于零就判为正例，预测值小于零则判为负例，预测值为临界值则可任意判别。虽然单位阶跃函数看似完美解决了这个问题，但是单位阶跃函数不连续并且不充分光滑，因而无法进行求解。</p><p>所以我们希望找到一个近似函数来替代单位阶跃函数，并希望它单调可微。对数概率函数正是这样一个替代的函数，对数概率函数将θ^Tx（<strong>θTx是多项式</strong>）的值转化为接近0或1的值，并且其输出=0处变化很陡，将对数概率函数代入，就能得到逻辑回归的表达式。</p><p><strong>在进行分类的过程实际上是使用多项式来进行“围数据”，逻辑回归相对于在多项式上加了一层函数，输出0，1。</strong></p><p>为了提高算法收敛速度和节省内存，实际应用在迭代求解时往往会使用高效的优化算法，如LBFGS，信赖域算法，但这些求解方法是基于批量处理的，批处理算法无法高效处理超大规模的数据集，也无法对线上模型进行快速实时更新。</p><p>随机梯度下降是相对于批处理的另外一种优化方式，它每次只用一个样本来更新模型的权重，这样就可以更快地进行模型迭代。对于广告和新闻推荐这种数据和样本更新比较频繁场景，快速的模型更新能够更早捕捉到新数据的规律进而提升业务指标。谷歌FTRL就是基于随机梯度下降的一种逻辑回归优化算法。是一种在线学习算法。</p><h2 id="逻辑回归应用"><a href="#逻辑回归应用" class="headerlink" title="逻辑回归应用"></a>逻辑回归应用</h2><p>逻辑回归常用于疾病自动诊断，经济预测，点击率预测等领域。由于其处理速度快且容易并行，逻辑回归适合用来学习需要大规模训练的样本和特征，对于广告十亿量级的特征和亿量级的特征来说，逻辑回归有着天然的优势，因而逻辑回归在工业界获得了广泛的应用。而逻辑回归的缺点是需要大量的特征组合和离散工作来增加特征的表达性，模型表达能力弱，比较容易欠拟合。</p><p>业界对逻辑回归的研究热点主要集中在稀疏性，准确性和大规模计算上。实际应用逻辑回归前，经常会对特征进行独热（One Hot）编码，比如广告点击率应用中的用户ID，广告ID。为了实现计算效率和性能优化，逻辑回归求解有很多种优化方法，比如BFGS，LBFGS，共轭梯度法，信赖域法，其中前两个是牛顿法的变种，LBFGS算法是BFGS算法在受限内存下的近似优化。针对逻辑回归在线学习中遇到的稀疏性和准确性问题，谷歌和伯克利分校提出来稀疏性比较好的FOBOS算法，微软提出了RDA算法。谷歌综合了精度比较好的RDA和稀疏性比较好的FTRL，但在L1范数或者非光滑的正则项下，FTRL的效果会更好。</p><p>在实际应用中，逻辑回归也需要注意正则化的问题。L1正则（也称LASSO）假设模型参数取值满足拉普拉斯分布，L2正则（也称RIDGE）假设模型参数取值满足高斯分布。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;百面机器学习-逻辑回归&quot;&gt;&lt;a href=&quot;#百面机器学习-逻辑回归&quot; class=&quot;headerlink&quot; title=&quot;百面机器学习  逻辑回归&quot;&gt;&lt;/a&gt;百面机器学习  逻辑回归&lt;/h1&gt;&lt;p&gt;逻辑回归可以说是机器学习领域最基础也是最常用的模型，逻辑回归的原
      
    
    </summary>
    
      <category term="知识总结" scheme="https://github.com/zdkswd/categories/%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/"/>
    
    
      <category term="机器学习" scheme="https://github.com/zdkswd/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>XGBoost</title>
    <link href="https://github.com/zdkswd/2019/02/25/XGBoost/"/>
    <id>https://github.com/zdkswd/2019/02/25/XGBoost/</id>
    <published>2019-02-25T05:56:47.000Z</published>
    <updated>2019-03-08T00:58:40.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="XGBoost"><a href="#XGBoost" class="headerlink" title="XGBoost"></a>XGBoost</h1><p>参考<a href="https://mp.weixin.qq.com/s/AnENu0i3i5CdUQkZscMKgQ" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/AnENu0i3i5CdUQkZscMKgQ</a><br>XGBoost是boosting算法的其中一种。Boosting算法的思想是将许多弱分类器集成在一起形成一个强分类器。因为XGBoost是一种提升树模型，所以它是将许多树模型集成在一起，形成一个很强的分类器。而所用到的树模型则是CART回归树模型。讲解其原理前，先讲解一下CART回归树。</p><h2 id="CART回归树"><a href="#CART回归树" class="headerlink" title="CART回归树"></a>CART回归树</h2><p>CART回归树是假设树为二叉树，通过不断将特征进行分裂。比如当前树结点是基于第j个特征值进行分裂的，设该特征值小于s的样本划分为左子树，大于s的样本划分为右子树。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/XGBoost/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-25%20%E4%B8%8A%E5%8D%8810.23.19.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>而CART回归树实质上就是在该特征维度对样本空间进行划分，而这种空间划分的优化是一种NP难问题，因此，在决策树模型中是使用启发式方法解决。典型CART回归树产生的目标函数为：<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/XGBoost/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-25%20%E4%B8%8A%E5%8D%8810.23.46.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>因此，当我们为了求解最优的切分特征j和最优的切分点s，就转化为求解这么一个目标函数：<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/XGBoost/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-25%20%E4%B8%8A%E5%8D%8810.28.25.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>所以我们只要遍历所有特征的的所有切分点，就能找到最优的切分特征和切分点。最终得到一棵回归树。</p><h2 id="XGBoost算法思想"><a href="#XGBoost算法思想" class="headerlink" title="XGBoost算法思想"></a>XGBoost算法思想</h2><p>该算法思想就是不断地添加树，不断地进行特征分裂来生长一棵树，每次添加一个树，其实是学习一个新函数，去拟合上次预测的残差。当我们训练完成得到k棵树，我们要预测一个样本的分数，其实就是根据这个样本的特征，在每棵树中会落到对应的一个叶子节点，每个叶子节点就对应一个分数，最后只需要将每棵树对应的分数加起来就是该样本的预测值。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/XGBoost/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-25%20%E4%B8%8A%E5%8D%8810.29.31.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>注：w_q(x)为叶子节点q的分数，f(x)为其中一棵回归树</p><p>如下图例子，训练出了2棵决策树，小孩的预测分数就是两棵树中小孩所落到的结点的分数相加。爷爷的预测分数同理。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/XGBoost/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-25%20%E4%B8%8A%E5%8D%8810.30.02.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><h2 id="XGBoost原理"><a href="#XGBoost原理" class="headerlink" title="XGBoost原理"></a>XGBoost原理</h2><p>XGBoost目标函数定义为：<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/XGBoost/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-25%20%E4%B8%8A%E5%8D%8810.31.54.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>目标函数由两部分构成，第一部分用来衡量预测分数和真实分数的差距，另一部分则是正则化项。正则化项同样包含两部分，T表示叶子结点的个数，w表示叶子节点的分数。γ可以控制叶子结点的个数，λ可以控制叶子节点的分数不会过大，防止过拟合。yi即为真实值，yi hat为预测值。</p><p>正如上文所说，新生成的树是要拟合上次预测的残差的，即当生成t棵树后，预测分数可以写成：<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/XGBoost/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-25%20%E4%B8%8A%E5%8D%8810.36.35.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>同时，可以将目标函数改写成：<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/XGBoost/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-25%20%E4%B8%8A%E5%8D%8810.39.13.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>很明显，我们接下来就是要去找到一个f_t能够最小化目标函数。XGBoost的想法是利用其在f_t=0处的泰勒二阶展开近似它。所以，目标函数近似为：<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/XGBoost/20160421102838944.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/XGBoost/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-25%20%E4%B8%8A%E5%8D%8810.49.32.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>其中g_i为一阶导数，h_i为二阶导数：<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/XGBoost/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-25%20%E4%B8%8A%E5%8D%8810.54.27.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>由于前t-1棵树的预测分数与y的残差对目标函数优化不影响，可以直接去掉。简化目标函数为：<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/XGBoost/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-25%20%E4%B8%8B%E5%8D%8812.15.48.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>上式是将每个样本的损失函数值加起来，我们知道，每个样本都最终会落到一个叶子结点中，所以我们可以将所以同一个叶子结点的样本重组起来，过程如下图：<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/XGBoost/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-25%20%E4%B8%8B%E5%8D%8812.21.37.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>因此通过上式的改写，我们可以将目标函数改写成关于叶子结点分数w的一个一元二次函数，求解最优的w和目标函数值就变得很简单了，直接使用顶点公式即可。因此，最优的w和目标函数公式为<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/XGBoost/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-25%20%E4%B8%8B%E5%8D%8812.28.52.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><h2 id="分裂结点算法"><a href="#分裂结点算法" class="headerlink" title="分裂结点算法"></a>分裂结点算法</h2><p>在上面的推导中，我们知道了如果我们一棵树的结构确定了，如何求得每个叶子结点的分数。但我们还没介绍如何确定树结构，即每次特征分裂怎么寻找最佳特征，怎么寻找最佳分裂点。</p><p>正如上文说到，基于空间切分去构造一颗决策树是一个NP难问题，我们不可能去遍历所有树结构，因此，XGBoost使用了和CART回归树一样的想法，利用贪婪算法，遍历所有特征的所有特征划分点，不同的是使用上式目标函数值作为评价函数。具体做法就是对比分裂后的目标函数值与单子叶子节点的目标函数增益，同时为了限制树生长过深，还加了个阈值，只有当增益大于该阈值才进行分裂。</p><p>同时可以设置树的最大深度、当样本权重和小于设定阈值时停止生长去防止过拟合。</p><h2 id="Shrinkage-and-Column-Subsampling"><a href="#Shrinkage-and-Column-Subsampling" class="headerlink" title="Shrinkage and Column Subsampling"></a>Shrinkage and Column Subsampling</h2><p>XGBoost还提出了两种防止过拟合的方法：Shrinkage and Column Subsampling。</p><p>Shrinkage方法就是在每次迭代中对树的每个叶子结点的分数乘上一个缩减权重η，这可以使得每一棵树的影响力不会太大，留下更大的空间给后面生成的树去优化模型。Column Subsampling类似于随机森林中的选取部分特征进行建树。其可分为两种，一种是按层随机采样，在对同一层内每个结点分裂之前，先随机选择一部分特征，然后只需要遍历这部分的特征，来确定最优的分割点。另一种是随机选择特征，建树前随机选择一部分特征然后分裂就只遍历这些特征。一般情况下前者效果更好。</p><h2 id="近似算法"><a href="#近似算法" class="headerlink" title="近似算法"></a>近似算法</h2><p>对于连续型特征值，当样本数量非常大，该特征取值过多时，遍历所有取值会花费很多时间，且容易过拟合。因此XGBoost思想是对特征进行分桶，即找到l个划分点，将位于相邻分位点之间的样本分在一个桶中。在遍历该特征的时候，只需要遍历各个分位点，从而计算最优划分。从算法伪代码中该流程还可以分为两种，全局的近似是在新生成一棵树之前就对各个特征计算分位点并划分样本，之后在每次分裂过程中都采用近似划分，而局部近似就是在具体的某一次分裂节点的过程中采用近似算法。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/XGBoost/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-25%20%E4%B8%8B%E5%8D%881.01.02.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><h2 id="针对稀疏矩阵的算法（缺失值处理）"><a href="#针对稀疏矩阵的算法（缺失值处理）" class="headerlink" title="针对稀疏矩阵的算法（缺失值处理）"></a>针对稀疏矩阵的算法（缺失值处理）</h2><p>当样本的第i个特征值缺失时，无法利用该特征进行划分时，XGBoost的想法是将该样本分别划分到左结点和右结点，然后计算其增益，哪个大就划分到哪边。</p><h2 id="XGBoost的优点"><a href="#XGBoost的优点" class="headerlink" title="XGBoost的优点"></a>XGBoost的优点</h2><p>之所以XGBoost可以成为机器学习的大杀器，广泛用于数据科学竞赛和工业界，是因为它有许多优点：</p><ol><li>使用许多策略去防止过拟合，如：正则化项、Shrinkage and Column Subsampling等。</li><li>目标函数优化利用了损失函数关于待求函数的二阶导数</li><li>支持并行化，这是XGBoost的闪光点，虽然树与树之间是串行关系，但是同层级节点可并行。具体的对于某个节点，节点内选择最佳分裂点，候选分裂点计算增益用多线程并行。训练速度快。</li><li>添加了对稀疏数据的处理。</li><li>交叉验证，early stop，当预测结果已经很好的时候可以提前停止建树，加快训练速度。</li><li>支持设置样本权重，该权重体现在一阶导数g和二阶导数h，通过调整权重可以去更加关注一些样本。</li></ol><h1 id="百面机器学习的补充-XGBoost的构建过程"><a href="#百面机器学习的补充-XGBoost的构建过程" class="headerlink" title="百面机器学习的补充  XGBoost的构建过程"></a>百面机器学习的补充  XGBoost的构建过程</h1><p>假设决策树的结构已知，通过令损失函数相对于wj的导数为0可以求出在最小化损失函数的情况下各个叶子节点上的预测值<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/XGBoost/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-25%20%E4%B8%8B%E5%8D%881.15.25.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>然而从所有的树结构中寻找最优树结构是一个NP-hard问题，因此在实际中往往采用贪心法来构建一个次优的树结构，基本思想是从根结点开始，每次对一个叶子节点进行分裂，针对每一种可能的分裂，根据特定的准则选取最优的分裂。XGBoost有特定的准则来选取最优分裂。</p><p>将预测值带入到损失函数中可求得损失函数的最小值。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/XGBoost/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-25%20%E4%B8%8B%E5%8D%881.27.16.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>容易计算出分裂前后损失函数的差值。XGBoost采用最大化这个差值作为准则来进行决策树的构建，通过遍历所有特征的所有取值，寻找使得损失函数前后相差最大时对应的分裂方式。此外，由于损失函数前后存在差值一定为正的限制，此时y起到了一定的预剪枝效果。</p><h1 id="美团机器学习实践"><a href="#美团机器学习实践" class="headerlink" title="美团机器学习实践"></a>美团机器学习实践</h1><p>梯度提升树算法与其他算法的对比如下：</p><h2 id="梯度提升树算法与线性模型"><a href="#梯度提升树算法与线性模型" class="headerlink" title="梯度提升树算法与线性模型"></a>梯度提升树算法与线性模型</h2><p>梯度提升树可以更好的处理缺失特征和不在同一个区间的特征（年龄特征取值范围为【0，150】，星期几特征取值范围为【0，6】，而转化率特征取值范围为【0，1】），梯度提升树算法也可以更好地应对异常点，更好处理特征间的相关性，处理非线性决策边界的问题。</p><h2 id="梯度提升树算法与随机森林"><a href="#梯度提升树算法与随机森林" class="headerlink" title="梯度提升树算法与随机森林"></a>梯度提升树算法与随机森林</h2><p>随机森林可以并行训练，较不容易过拟合。梯度提升树可以学习更复杂的决策边界，效果往往会更好。</p><h2 id="梯度提升树算法与神经网络"><a href="#梯度提升树算法与神经网络" class="headerlink" title="梯度提升树算法与神经网络"></a>梯度提升树算法与神经网络</h2><p>神经网络作为单模型时，参数较多时VC维数也较高，训练较为困难。在中小数据集上，XGBoost可以取得很不错的效果。但是在数据集较大，且选取合适神经网络结构时，神经网络得到的结果往往是更完美的。</p><h1 id="XGBoost-VS-深度学习"><a href="#XGBoost-VS-深度学习" class="headerlink" title="XGBoost VS 深度学习"></a>XGBoost VS 深度学习</h1><p>观点1：XGBoost要比深度学习更重要。2016年Kaggle大赛29个获奖方案中，17个用了XGBoost。因为它好用，在很多情况下都更为可靠、灵活，而且准确；在绝大多数的回归和分类问题上，XGBoost的实际表现都是顶尖的.<br>观点2：针对非常要求准确度的那些问题，XGBoost确实很有优势，同时它的计算特性也很不错。然而，相对于支持向量机、随机森林或深度学习，XGBoost的优势倒也没到那种夸张的程度。特别是当你拥有足够的训练数据，并能找到合适的深度神经网络时，深度学习的效果就明显能好上一大截。<br>观点3：深度学习和XGBoost并不截然对立（XGBoost发起人-陈天奇博士）。两种方法在其各自擅长领域的性能表现都非常好：<br>XGBoost专注于模型的可解释性，而基于人工神经网络的深度学习，则更关注模型的准确度。<br>XGBoost更适用于变量数较少的表格数据，而深度学习则更适用于图像或其他拥有海量变量的数据。</p><p>不同的机器学习模型适用于不同类型的任务：<br>深度神经网络通过对时空位置建模，能够很好地捕获图像、语音、文本等高维数据。<br>基于树模型的XGBoost则能很好地处理表格数据，同时还拥有一些深度神经网络所没有的特性（如：模型的可解释性、输入数据的不变性、更易于调参等）。<br>这两类模型都很重要，并广泛用于数据科学竞赛和工业界。我们需要全面理解每一种模型，并能选出最适合你当前任务的那个。XGBoost、深度神经网络与其他经常要用的机器学习算法（如因子分解机、logistic回归分析等），值得机器学习行业的每一位从业者关注。这里没有一药能解百病的说法。</p><h1 id="XGBoost为什么要展到二次项"><a href="#XGBoost为什么要展到二次项" class="headerlink" title="XGBoost为什么要展到二次项"></a>XGBoost为什么要展到二次项</h1><p>为了可以设置任何可以二阶求导的损失函数，只要该损失函数二阶可导，都可以用泰勒展开式进行近似替代，实现形式上的”统一”</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;XGBoost&quot;&gt;&lt;a href=&quot;#XGBoost&quot; class=&quot;headerlink&quot; title=&quot;XGBoost&quot;&gt;&lt;/a&gt;XGBoost&lt;/h1&gt;&lt;p&gt;参考&lt;a href=&quot;https://mp.weixin.qq.com/s/AnENu0i3i5Cd
      
    
    </summary>
    
      <category term="知识总结" scheme="https://github.com/zdkswd/categories/%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/"/>
    
    
      <category term="机器学习" scheme="https://github.com/zdkswd/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>百面机器学习 集成学习</title>
    <link href="https://github.com/zdkswd/2019/02/23/%E7%99%BE%E9%9D%A2%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%20%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/"/>
    <id>https://github.com/zdkswd/2019/02/23/百面机器学习 集成学习/</id>
    <published>2019-02-23T12:46:47.000Z</published>
    <updated>2019-03-08T01:28:48.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="集成学习有几种？有何异同？"><a href="#集成学习有几种？有何异同？" class="headerlink" title="集成学习有几种？有何异同？"></a>集成学习有几种？有何异同？</h1><h2 id="Boosting"><a href="#Boosting" class="headerlink" title="Boosting"></a>Boosting</h2><p>Boosting方法训练基分类器时采用串行的方式，各个基分类器之间有依赖。</p><p>它的基本思想是将基分类器层层叠加，在每一层训练时，对前一层基分类器分错的样本给予更高的权重，测试时，根据各层分类器的结果的加权得到最终结果。</p><p>Boosting的过程很类似于人类学习的过程，我们学习新知识的过程往往是迭代的，第一遍学习时，我们会记住一部分知识，但往往也会犯一些错误，对于这些错误，我们的印象会很深。第二遍学习时，就会针对犯过错误的知识加强学习，以减少类似的错误发生。不断循环往复，直到犯错误的次数减少到很低的程度。</p><p>Boosting的主要思想：迭代式学习。</p><p>AdaBoost，GBDT，XGBoost都属于Boosting思想。</p><h2 id="Bagging"><a href="#Bagging" class="headerlink" title="Bagging"></a>Bagging</h2><p>Bagging与Boosting的串行训练方式不同，Bagging方法在训练过程中，各基分类器之间无强依赖，可以进行并行训练。其中很著名的算法之一就是基于决策树基分类器的随机森林。为了让基分类器之间相互独立，将训练集分为若干子集（当训练样本数量较少时，子集之间可能有交叠）。Bagging方法更像是一个集体决策的过程，每个个体都进行单独学习，学习的内容可以相同，也可以不同，也可以部分重叠。但由于个体之间存在差异性，最终做出的判断不会完全一致。在最终做决策时，每个个体单独做出判断，再通过投票的方式做出最后的集体决策。</p><p>Bagging主要思想：集体投票决策</p><p>从消除基分类器的偏差和方差的角度来理解Boosting和Bagging方法的差异。基分类器又称为弱分类器，基分类器的错误率要大于集成分类器，是偏差与方差两种错误之和。偏差主要是由于分类器的表达能力有限导致的系统性错误，表现在训练误差不收敛。方差是由于分类器对于样本分布过于敏感，导致在训练样本数较少时，产生过拟合。</p><p>用训练集的子集训练出来的决策边界很曲折，有过拟合的趋向。集成之后的模型的决策边界就比各个独立的模型平滑了，这是由于集成的加权投票方法，减小了方差。</p><h1 id="集成学习的步骤和例子"><a href="#集成学习的步骤和例子" class="headerlink" title="集成学习的步骤和例子"></a>集成学习的步骤和例子</h1><p>虽然集成学习的具体算法和策略各不相同，但都共享同样的基本步骤。</p><h2 id="问题1"><a href="#问题1" class="headerlink" title="问题1"></a>问题1</h2><p>问：集成学习有哪些基本步骤？请举几个集成学习的例子</p><p>答：集成学习一般分为3个步骤。</p><ol><li>找到误差相互独立的基分类器。</li><li>训练基分类器</li><li>合并基分类器的结果</li></ol><p>合并基分类器的方法有voting和stacking两种。前者是用投票的方式，将获得最多选票的结果作为最终的结果。后者是用串行的方式，把前一个基分类器的结果输出到下一个分类器，将所有基分类器的输出结果相加（或者使用更复杂的算法融合，比如把各基分类器的输出作为特征，使用逻辑回归作为融合模型进行最后的结果预测）作为最终的输出。</p><p>以AdaBoost为例看到Boosting思想，对分类正确的样本降低了权重，对分类错误的样本升高或者保持权重不变。在最后进行模型融合的过程中，也根据错误率对基分类器进行加权融合。错误率低的分类器拥有更大的话语权。</p><p>另一个非常流行的模型是梯度提升决策树（GBDT），其核心思想是，每一颗树学的是之前所有树结论和的残差，这个残差就是一个加预测值后能得真实值的累加量。</p><p>以视频网站的用户画像为例，为了将广告定向投放给指定年龄的用户，视频网站需要对每个用户的年龄做出预测。在这个问题中，每个样本是一个已知性别年龄的用户，而特征则包括这个人访问的时长，时段，观看的视频的类型等。</p><p>例如用户A的真实年龄是25岁，但第一棵决策树的预测年龄是22岁，差了3岁，即残差是3，那么第二颗树我们把A的年龄设为3岁去学习，如果第二颗树能把A分到3岁的叶子节点，那两颗树的结果相加就可以得到A的真实年龄，如果第二棵树的结论是5岁，则A仍然存在-2岁的残差，第三棵树里A的年龄就变成-2岁，继续学。这里使用残差继续学习。</p><h1 id="基分类器"><a href="#基分类器" class="headerlink" title="基分类器"></a>基分类器</h1><p>基分类器的选择是集成学习主要步骤中的第一步，也是非常重要的一步，到底选择什么样的基分类器，为什么很多集成学习模型都选择决策树作为基分类器，这些都是需要明确的问题，做到知其然知其所以然。</p><h2 id="问题1-1"><a href="#问题1-1" class="headerlink" title="问题1"></a>问题1</h2><p>问：常用的基分类器是什么？</p><p>答：最常用的基分类器是决策树。有三个原因</p><ol><li>决策树可以较为方便地将样本的权重整合到训练过程中，而不需要使用过采样的方法来调整样本权重。</li><li>决策树的表达能力和泛化能力，可以通过调节树的层数来做折中。</li><li>数据样本的扰动对于决策树的影响较大，因此不同子样本集合生成的决策树基分类器随机性较大，这样的不稳定学习器更适合作为基分类器。此外，在决策树节点分裂时，随机选择一个特征子集，从中找出最优分裂属性，很好地引入了随机性。</li></ol><p>除了决策树外，神经网络模型也适合作为基分类器，主要由于神经网络模型也比较不稳定，而且还可以通过调整神经元数量，连接方式，网络层数，初始权值等方式引入随机性。</p><h2 id="问题2"><a href="#问题2" class="headerlink" title="问题2"></a>问题2</h2><p>问：可否将随机森林中的基分类器，由决策树替换为线性分类器或K-近邻？请解释为什么？</p><p>答：随机森林属于Bagging类的集成学习。Bagging的主要好处是集成后的分类器的方差比基分类器的方差小。Bagging所采用的基分类器，最好是本身对样本分布较为敏感的（即所谓不稳定的分类器），这样Bagging才能有用武之地。线性分类器或K-近邻都是较为稳定的分类器，本身方差就不大，所以以它们为基分类器使用Bagging并不能在原有基分类器的基础上获得更好的表现，甚至可能因为Bagging的采样，而导致它们在训练中更难收敛，从而增大了集成分类器的偏差。</p><h1 id="偏差与方差"><a href="#偏差与方差" class="headerlink" title="偏差与方差"></a>偏差与方差</h1><p>我们经常用过拟合，欠拟合来定性的描述模型是否很好地解决了特定的问题。从定量的角度来说，可以用模型的偏差（Bias）与方差（Variance）来描述模型的性能。集成学习往往能够“神奇”地提升弱分类器的性能。可以从偏差与方差的角度去解释着背后的机理。</p><h2 id="问题1-2"><a href="#问题1-2" class="headerlink" title="问题1"></a>问题1</h2><p>问：什么是偏差和方差？</p><p>答：在有监督学习中，模型泛化误差来源于两个方面-偏差和方差。</p><p>偏差指的是由所有采样得到的大小为m的训练数据集训练出的所有模型的输出的平均值和真实模型输出之间的偏差。<strong>偏差通常是由于我们对学习算法做了错误的假设所导致的</strong>，比如真实模型是某个二次函数，但我们假设模型是一次函数。由偏差带来的误差通常在训练误差上就能体现出来。</p><p>方差指的是由所有采样得到的大小为m的训练数据集训练出的所有模型输出的方差。<strong>方差通常是由于模型的复杂度相对于训练样本数m过高导致的</strong>，比如一共有100个训练样本，而我们假设模型是阶数不大于200的多项式函数。由方差带来的误差通常体现在测试误差相对于训练误差的增量上。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E7%99%BE%E9%9D%A2%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%20%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/20180401130516184.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><h2 id="问题2-1"><a href="#问题2-1" class="headerlink" title="问题2"></a>问题2</h2><p>问：如何从减小方差和偏差的角度解释Boosting和Bagging的原理？</p><p>答：简单回答就是，Bagging能够提高弱分类器性能的原因是降低了方差，Boosting能够提升弱分类器性能的原因是降低了偏差。</p><p>首先，Bagging是Bootstrap Aggregating的简称，意思是再抽样，然后在每个样本上训练出来的模型取平均。假设有n个随机变量，方差记为ρ方，在随机变量完全独立的情况下，n个随机变量的方差为ρ方 / n，也就是方差减小到了原来的1 / n。记得在数理统计中提到过这么回事。</p><p>再从模型的角度理解这个问题，对n个独立不相关的模型的预测结果取平均，方差是原来单个模型的1/ n。这个描述不甚严谨，但原理已经讲得很清楚了。当然，模型之间不可能完全独立。为了追求模型的独立性，诸多Bagging的方法做了不同的改进。比如在随机森林算法中，每次选取节点分裂属性时，会随机抽取一个属性子集，而不是从所有属性中选取最优属性，这就是为了避免弱分类器之间过强的相关性。通过训练集的重采样也能够带来弱分类器之间的一定独立性，从而降低Bagging后模型的方差。</p><p>再看Boosting，大家应该还记得Boosting的训练过程。在训练好一个弱分类器后，我们需要计算弱分类器的错误或者残差，作为下一个分类器的输入。这个过程本身就是在不断减小损失函数，来使模型不断逼近靶心，使得模型偏差不断降低。但Boosting的过程并不会显著降低方差，这是因为Boosting的训练过程使得各弱分类器之间是强相关的，缺乏独立性，所以不会对降低方差有作用。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E7%99%BE%E9%9D%A2%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%20%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/606386-20180722194316424-288674381.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>不难看出，方差和偏差是相辅相成，矛盾又统一的，二者并不能完全独立的存在。对于给定的学习任务和训练数据集，我们需要对模型的复杂度做合理的假设。如果模型复杂度过低，虽然方差很小，但是偏差会很高，如果模型复杂度高，虽然偏差降低了，但是方差会很高。所以需要综合考虑偏差和方差选择合适复杂度的模型进行训练。</p><h1 id="梯度提升决策树的基本原理"><a href="#梯度提升决策树的基本原理" class="headerlink" title="梯度提升决策树的基本原理"></a>梯度提升决策树的基本原理</h1><p>GBDT非常好的体现了从错误中学习的理念，基于决策树预测的残差进行迭代的学习。GBDT几乎是算法工程师的必备技能。</p><h2 id="问题1-3"><a href="#问题1-3" class="headerlink" title="问题1"></a>问题1</h2><p>问：GBDT的基本原理是什么？</p><p>答：相比于Bagging中各个弱分类器可以独立地进行训练，Boosting中的弱分类器需要依次生成。在每一轮迭代中，基于已生成的弱分类器集合（即当前模型）的预测结果，新的弱分类器会重点关注那些还没有被正确预测的样本。</p><p>Gradient Boosting是Boosting中的一大类算法，其基本思想是根据当前模型损失函数的负梯度信息来训练新加入的弱分类器，然后将训练好的弱分类器以累加的形式结合到现有模型中。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E7%99%BE%E9%9D%A2%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%20%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/20180128125923199.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>算法1描述了Gradient Boosting算法的基本流程，在每一轮迭代中，首先计算出当前模型在所有样本上的负梯度，然后以该值为目标训练一个新的弱分类器进行拟合并计算出该弱分类器的权重，最终实现对模型的更新。</p><p>采用决策树作为弱分类器的Gradient Boosting算法被称为GBDT，有时又被称为MART（Multiple Additive Regression Tree）。GBDT中使用的决策树通常为CART。</p><p>用一个很简单的例子来解释GBDT训练的过程。模型任务是预测一个人的年龄，训练集有ABCD 4个人，他们的年龄分别是14，16，24，26，特征包括了购物金额，上网时长，上网历史等。下面开始训练第一棵树，训练的过程和传统的决策树相同，简单起见，我们只进行一次分枝。训练好第一棵树后，求得每个样本预测值与真实值之间的残差。可以看到A，B，C，D的残差分别是-1，1，-1，1。这时我们就用每个样本的残差训练下一棵树，直到残差收敛到某个阈值以下，或者树的总数达到某个上限为止。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E7%99%BE%E9%9D%A2%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%20%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/WechatIMG2.jpeg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>由于GBDT是利用残差训练的，在预测的过程中，我们也需要把所有树的预测值加起来，得到最终的预测结果。</p><p>GBDT使用梯度提升作为训练方法，而在逻辑回归或者神经网络的训练过程中往往采用梯度下降（Gradient Descent）作为训练方法，两者有什么联系与区别吗？</p><h2 id="问题二"><a href="#问题二" class="headerlink" title="问题二"></a>问题二</h2><p>问：梯度提升和梯度下降的区别和联系是什么？</p><p>答： 两者都是在每一轮迭代中，利用损失函数相对于模型的负梯度方向的信息来对当前模型进行更新，只不过在梯度下降中，模型是以参数化形式表示，从而模型的更新等价于参数的更新。在梯度提升中，模型并不需要进行参数化表示，而是直接定义在函数空间中，从而大大扩展了可以使用的模型种类。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E7%99%BE%E9%9D%A2%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%20%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/WechatIMG3.jpeg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><h2 id="问题三"><a href="#问题三" class="headerlink" title="问题三"></a>问题三</h2><p>问：GBDT的优点和局限性有哪些？</p><p>答：优点：</p><ol><li>预测阶段计算速度快，树与树之间可并行化计算。</li><li>在分布稠密的数据集上，泛化能力和表达能力都很好，这使得GBDT在Kaggle的众多竞赛中，经常名列前茅。</li><li>采用决策树作为弱分类器使得GBDT模型具有较好的解释性和鲁棒性，能够自动发现特征间的高阶关系，并且也不需要对数据进行特殊的预处理如归一化等。</li></ol><p>局限性：</p><ol><li>GBDT在高维稀疏的数据集上，表现不如支持向量机或者神经网络。</li><li>GBDT在处理文本分类特征问题上，相对其他模型的优势不如它在处理数值特征时明显。</li><li>训练过程需要串行训练，只能在决策树内部采用一些局部并行的手段提高训练速度。</li></ol><h2 id="自己的一些思考"><a href="#自己的一些思考" class="headerlink" title="自己的一些思考"></a>自己的一些思考</h2><p>在GBDT中每棵树不是分类树，是回归树，对于上面的例子来说，回归树的每一个节点都会得一个预测值，这个预测值一般为该节点中所有样本的均值。然后我们分枝时穷举每一个feature的每个阈值找最好的分割点，但衡量最好的标准不再是最大熵，而是最小化均方差。最小化均方差就是每个样本的（真实值-预测值）^2 的总和 / N，均方差越小，说明错的越不离谱，那均方差最小时使用的那个特征就是分枝应选择的最佳特征。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E7%99%BE%E9%9D%A2%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%20%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/v2-a7a81cfe1abe8bfb5e5839d90ad1c0d8_b.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>提升树其实即使不断迭代、不断构造回归树进行决策，而且每一个回归的样本数据均来自上一个回归树所产生的残差。残差就是真实值 - 预测值。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E7%99%BE%E9%9D%A2%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%20%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/v2-a7dbc7661bf3e3e3ed98b446f4aab0e4_b.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>提升树的过程如下：<br>1.计算第一个节点的均值：20<br>2.穷举每个特征进行首次分枝，选取均方差最小的那个特征作为首次分枝依据，<br>3.计算每个节点的均值：15，25<br>4.计算每个节点的中每个样本的残差-1，1，-1，1<br>5.以残差为训练样本进行下一轮回归树的训练……..<br>6.累加每棵回归树的结论，得出最终的预测值。</p><p>梯度提升决策树，它和提升树的主要区别在于梯度提升决策树是利用最速下降的近似方法，即利用损失函数的负梯度在当前模型的值，作为回归问题中提升树算法的残差的近似值来拟合一个回归树。梯度提升背后的主要思想是合并许多简单的模型，比如深度较小的树，每棵树只能对部分数据做出好的预测。因此，添加的树越来越多，可以不断迭代提高性能。</p><p>关于损失函数：对于回归算法，最常见的损失函数是均方差，<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E7%99%BE%E9%9D%A2%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%20%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-23%20%E4%B8%8B%E5%8D%888.01.41.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><h1 id="XGBoost与GBDT的联系和区别"><a href="#XGBoost与GBDT的联系和区别" class="headerlink" title="XGBoost与GBDT的联系和区别"></a>XGBoost与GBDT的联系和区别</h1><h2 id="问题1-4"><a href="#问题1-4" class="headerlink" title="问题1"></a>问题1</h2><p>问：XGBoost与GBDT的联系和区别有哪些？</p><p>答：原始的GBDT算法基于经验损失函数来构造新的决策树，只是在决策树构建完成后再剪枝，而XGBoost在决策树构建阶段就加入了正则项。</p><p>除了算法上与传统的GBDT有一些不同外，XGBoost还在工程实现上做了大量的优化。总的来说，两者之间的区别和联系可以总结成以下几个方面。</p><ol><li>GBDT是机器学习算法，XGBoost是该算法的工程实现。</li><li>在使用CART作为基分类器时，XGBoost显式地加入了正则项来控制模型的复杂度，有利于防止过拟合，从而提高模型的泛化能力。</li><li>GBDT在模型训练时只使用了代价函数的一阶导数信息，XGBoost对代价函数进行二阶泰勒展开，可以同时使用一阶和二阶导数。</li><li>传统的GBDT采用CART作为基分类器，XGBoost支持多种类型的基分类器，比如线性分类器。</li><li>传统的GBDT在每轮迭代时使用全部的数据，XGBoost则采用了与随机森林相似的策略，支持对数据进行采样。</li><li>传统的GBDT没有设计对缺失值进行处理，XGBoost能够自动学习出缺失值的处理策略。</li><li>节点分裂的方式不同，gbdt是用的gini系数，xgboost是经过优化推导后的</li><li>Xgboost工具支持并行。boosting不是一种串行的结构吗?怎么并行的？注意xgboost的并行不是tree粒度的并行，xgboost也是一次迭代完才能进行下一次迭代的（第t次迭代的代价函数里包含了前面t-1次迭代的预测值）。xgboost的并行是在特征粒度上的。我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），xgboost在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行</li></ol><h1 id="N问GBDT"><a href="#N问GBDT" class="headerlink" title="N问GBDT"></a>N问GBDT</h1><p>怎样设置单棵树的停止生长条件？</p><p>(1)怎样设置单棵树的停止生长条件？<br>答：A. 节点分裂时的最小样本数<br>B. 最大深度<br>C. 最多叶子节点数<br>D. loss满足约束条件<br>(2)如何评估特征的权重大小？<br>答：a. 通过计算每个特征在训练集下的信息增益，最后计算每个特征信息增益与所有特征信息增益之和的比例为权重值。<br>b. 借鉴投票机制。用相同的gbdt参数对w每个特征训练出一个模型，然后在该模型下计算每个特征正确分类的个数，最后计算每个特征正确分类的个数与所有正确分类个数之和的比例为权重值。<br>(3)当增加样本数量时，训练时长是线性增加吗？<br>答：不是。因为生成单棵决策树时，对于损失函数极小值<br>与样本数量N不是线性相关<br>(4)当增加树的棵树时，训练时长是线性增加吗？<br>答：不是。因为每棵树的生成的时间复杂度不一样。<br>(5)当增加一个棵树叶子节点数目时，训练时长是线性增加吗？<br>答：不是。叶子节点数和每棵树的生成的时间复杂度不成正比。<br>(6)每个节点上都保存什么信息？<br>答：中间节点保存某个特征的分割值，叶结点保存预测是某个类别的概率。<br>(7)如何防止过拟合？<br>答：a. 增加样本（data bias or small data的缘故），移除噪声。<br>b. 减少特征，保留重要的特征（可以用PCA等对特征进行降维）。<br>c. 对样本进行采样（类似bagging）。就是建树的时候，不是把所有的样本都作为输入，而是选择一个子集。<br>d. 对特征进行采样。类似样本采样一样, 每次建树的时候，只对部分的特征进行切分。<br>(8) gbdt在训练和预测的时候都用到了步长，这两个步长一样么？都有什么用，如果不一样，为什么？怎么设步长的大小？（太小？太大？）在预测时，设太大对排序结果有什么影响？跟shrinking里面的步长一样么这两个步长一样么？<br>答：训练跟预测时，两个步长是一样的，也就是预测时的步长为训练时的步长，从训练的过程可以得知（更新当前迭代模型的时候）。<br>都有什么用，如果不一样，为什么？答：它的作用就是使得每次更新模型的时候，使得loss能够平稳地沿着负梯度的方向下降，不至于发生震荡。<br>那么怎么设步长的大小?<br>答：有两种方法，一种就是按策略来决定步长，另一种就是在训练模型的同时，学习步长。<br>A. 策略：<br>a 每个树步长恒定且相等，一般设较小的值；<br>b 开始的时候给步长设一个较小值，随着迭代次数动态改变，或者说衰减。<br>B. 学习：<br>因为在训练第k棵树的时候，前k-1棵树时已知的，而且求梯度的时候是利用前k-1棵树来获得。所以这个时候，就可以把步长当作一个变量来学习。<br>（太小？太大？）在预测时，对排序结果有什么影响？<br>答：如果步长过大，在训练的时候容易发生震荡，使得模型学不好，或者完全没有学好，从而导致模型精度不好。<br>而步长过小，导致训练时间过长，即迭代次数较大，从而生成较多的树，使得模型变得复杂，容易造成过拟合以及增加计算量。<br>不过步长较小的话，使训练比较稳定，总能找到一个稳定的局部最优解。<br>个人觉得过大过小的话，模型的预测值都会偏离真实情况（可能比较严重），从而导致模型精度不好。<br>跟shrinking里面的步长一样么？答：这里的步长跟shrinking里面的步长是一致的。<br>(10)gbdt中哪些部分可以并行？<br>答：A. 计算每个样本的负梯度<br>B. 分裂挑选最佳特征及其分割点时，对特征计算相应的误差及均值时<br>C. 更新每个样本的负梯度时<br>D. 最后预测过程中，每个样本将之前的所有树的结果累加的时候<br>(11) 树生长成畸形树，会带来哪些危害，如何预防？<br>答：在生成树的过程中，加入树不平衡的约束条件。这种约束条件可以是用户自定义的。<br>例如对样本集中分到某个节点，而另一个节点的样本很少的情况进行惩罚</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;集成学习有几种？有何异同？&quot;&gt;&lt;a href=&quot;#集成学习有几种？有何异同？&quot; class=&quot;headerlink&quot; title=&quot;集成学习有几种？有何异同？&quot;&gt;&lt;/a&gt;集成学习有几种？有何异同？&lt;/h1&gt;&lt;h2 id=&quot;Boosting&quot;&gt;&lt;a href=&quot;#B
      
    
    </summary>
    
      <category term="知识总结" scheme="https://github.com/zdkswd/categories/%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/"/>
    
    
      <category term="机器学习" scheme="https://github.com/zdkswd/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Scrapy</title>
    <link href="https://github.com/zdkswd/2019/02/08/Scrapy/"/>
    <id>https://github.com/zdkswd/2019/02/08/Scrapy/</id>
    <published>2019-02-08T12:50:32.000Z</published>
    <updated>2019-02-08T12:51:19.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Scrapy"><a href="#Scrapy" class="headerlink" title="Scrapy"></a>Scrapy</h1><h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><blockquote><p>pip install scrapy  </p></blockquote><h1 id="创建项目"><a href="#创建项目" class="headerlink" title="创建项目"></a>创建项目</h1><p>首先创建一个新的Scrapy项目。</p><blockquote><p>scrapy startproject tutorial  </p></blockquote><p>创建的tutorial 目录：<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/Scrapy/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-08%20%E4%B8%8B%E5%8D%885.31.25.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>这些文件是：<br>scrapy.cfg: 项目的配置文件<br>tutorial/ : 该项目的python模块。之后您将在此加入代码。<br>tutorial/ items.py: 项目中的item文件.<br>tutorial/ pipelines.py: 项目中的pipelines文件.<br>tutorial/ settings.py: 项目的设置文件.<br>tutorial/ spiders /: 放置spider代码的目录.</p><h1 id="简单流程"><a href="#简单流程" class="headerlink" title="简单流程"></a>简单流程</h1><ol><li>创建一个Scrapy项目；</li><li>定义提取的Item；</li><li>编写爬取网站的 spider 并提取 Item；</li><li>编写 Item Pipeline 来存储提取到的Item(即数据)。<h1 id="定义Item"><a href="#定义Item" class="headerlink" title="定义Item"></a>定义Item</h1>Item 是保存爬取到的数据的容器；其使用方法和python字典类似， 并且提供了额外保护机制来避免拼写错误导致的未定义字段错误。</li></ol><p>可以通过创建一个 scrapy.Item 类， 并且定义类型为 scrapy.Field 的类属性来定义一个Item。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/Scrapy/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-08%20%E4%B8%8B%E5%8D%885.43.48.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>通过定义item， 可以很方便的使用Scrapy的其他方法。而这些方法需要知道item的定义。</p><h1 id="编写第一个爬虫-Spider"><a href="#编写第一个爬虫-Spider" class="headerlink" title="编写第一个爬虫(Spider)"></a>编写第一个爬虫(Spider)</h1><p>Spider是用户编写用于从单个网站(或者一些网站)爬取数据的类。</p><p>其包含了一个用于下载的初始URL，如何跟进网页中的链接以及如何分析页面中的内容， 提取生成 item 的方法。</p><p>为了创建一个Spider，您必须继承 scrapy.Spider 类， 且定义以下三个属性:</p><ol><li>name: 用于区别Spider。 该名字必须是唯一的，您不可以为不同的Spider设定相同的名字。</li><li>start_urls: 包含了Spider在启动时进行爬取的url列表。 因此，第一个被获取到的页面将是其中之一。 后续的URL则从初始的URL获取到的数据中提取。</li><li>parse() 是spider的一个方法。 被调用时，每个初始URL完成下载后生成的 Response 对象将会作为唯一的参数传递给该函数。 该方法负责解析返回的数据(response data)，提取数据(生成item)以及生成需要进一步处理的URL的 Request 对象。</li></ol><p>以下为我们的第一个Spider代码，保存在 tutorial/spiders 目录下的 dmoz_spider.py 文件中:<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/Scrapy/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-08%20%E4%B8%8B%E5%8D%885.50.34.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><h2 id="爬取"><a href="#爬取" class="headerlink" title="爬取"></a>爬取</h2><p>进入项目的根目录，执行下列命令启动spider</p><blockquote><p>scrapy crawl dmoz  </p></blockquote><h2 id="刚才发生了什么"><a href="#刚才发生了什么" class="headerlink" title="刚才发生了什么"></a>刚才发生了什么</h2><p>Scrapy为Spider的 start_urls 属性中的每个URL创建了 scrapy.Request 对象，并将 parse 方法作为回调函数(callback)赋值给了Request。</p><p>Request对象经过调度，执行生成 scrapy.http.Response 对象并送回给spider parse() 方法。</p><h1 id="提取Item"><a href="#提取Item" class="headerlink" title="提取Item"></a>提取Item</h1><h2 id="Selectors选择器简介"><a href="#Selectors选择器简介" class="headerlink" title="Selectors选择器简介"></a>Selectors选择器简介</h2><p>从网页中提取数据有很多方法。Scrapy使用了一种基于 XPath 和 CSS 表达式机制: Scrapy Selectors 。 </p><p>这里给出XPath表达式的例子及对应的含义:<br>/ html/ head/ title: 选择HTML文档中 <head> 标签内的 <title> 元素<br>/ html/ head/ title/ text(): 选择上面提到的 <title> 元素的文字<br>/ / td: 选择所有的 <td> 元素<br>/ / div[@class=”mine”]: 选择所有具有 class=”mine” 属性的 div 元素</td></title></title></head></p><p>为了配合XPath，Scrapy除了提供了 Selector 之外，还提供了方法来避免每次从response中提取数据时生成selector的麻烦。</p><p>Selector有四个基本的方法：<br>xpath(): 传入xpath表达式，返回该表达式所对应的所有节点的selector list列表 。<br>css(): 传入CSS表达式，返回该表达式所对应的所有节点的selector list列表.<br>extract(): 序列化该节点为unicode字符串并返回list。<br>re(): 根据传入的正则表达式对数据进行提取，返回unicode字符串list列表。</p><p>进入项目的根目录，启动shell：<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/Scrapy/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-08%20%E4%B8%8B%E5%8D%886.42.54.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><p>shell的类似输出：<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/Scrapy/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-08%20%E4%B8%8B%E5%8D%886.44.12.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>当shell载入后，您将得到一个包含response数据的本地 response 变量。输入 response.body 将输出response的包体， 输出 response.headers 可以看到response的包头。</p><p>更为重要的是，当输入 response.selector 时， 您将获取到一个可以用于查询返回数据的selector(选择器)， 以及映射到 response.selector.xpath() 、 response.selector.css() 的 快捷方法(shortcut): response.xpath() 和 response.css() 。</p><p>同时，shell根据response提前初始化了变量 sel 。该selector根据response的类型自动选择最合适的分析规则(XML vs HTML)。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/Scrapy/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-08%20%E4%B8%8B%E5%8D%886.45.17.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><h2 id="提取数据"><a href="#提取数据" class="headerlink" title="提取数据"></a>提取数据</h2><p>在我们的spider中加入这段代码:<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/Scrapy/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-08%20%E4%B8%8B%E5%8D%886.47.13.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><h2 id="使用Item"><a href="#使用Item" class="headerlink" title="使用Item"></a>使用Item</h2><p>Item 对象是自定义的python字典。 您可以使用标准的字典语法来获取到其每个字段的值。(字段即是我们之前用Field赋值的属性):<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/Scrapy/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-08%20%E4%B8%8B%E5%8D%886.56.31.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>一般来说，Spider将会将爬取到的数据以 Item 对象返回。所以为了将爬取的数据返回，我们最终的代码将是:<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/Scrapy/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-08%20%E4%B8%8B%E5%8D%886.56.49.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>现在对dmoz.org进行爬取将会产生 DmozItem 对象:<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/Scrapy/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-08%20%E4%B8%8B%E5%8D%886.59.34.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><h1 id="保存爬取到的数据"><a href="#保存爬取到的数据" class="headerlink" title="保存爬取到的数据"></a>保存爬取到的数据</h1><p>最简单存储爬取的数据是使用<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/Scrapy/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-08%20%E4%B8%8B%E5%8D%887.26.04.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>该命令将采用 JSON 格式对爬取的数据进行序列化，生成 items.json 文件。</p><p>小规模的项目中，这种存储方式已经足够。 如果需要对爬取到的item做更多更为复杂的操作，您可以编写 Item Pipeline 。 </p><h1 id="一个例子"><a href="#一个例子" class="headerlink" title="一个例子"></a>一个例子</h1><p><a href="https://github.com/Jack-Cherish/python-spider/tree/master/cartoon">https://github.com/Jack-Cherish/python-spider/tree/master/cartoon</a></p><h1 id="scrapy进行分布式爬虫"><a href="#scrapy进行分布式爬虫" class="headerlink" title="scrapy进行分布式爬虫"></a>scrapy进行分布式爬虫</h1><h2 id="分布式爬虫原理"><a href="#分布式爬虫原理" class="headerlink" title="分布式爬虫原理"></a>分布式爬虫原理</h2><p>scrapy的单机架构：<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/Scrapy/4155986-5fa2c59efa050ac4.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>可以看到，scrapy单机模式，通过一个scrapy引擎通过一个调度器，将Requests队列中的request请求发给下载器，进行页面的爬取。多台主机协作的关键是共享一个爬取队列。</p><p>分布式爬虫的关键是共享一个requests队列，维护该队列的主机称为master，而从机则负责数据的抓取，数据处理和数据存储，所以分布式爬虫架构如下图所示：<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/Scrapy/4155986-52592bfa491e1573.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>选用Redis队列进行存储，Redis是一种高效的非关系型数据库，以key-value的形式存储，结构灵活，它是内存中的数据结构存储系统，处理速度快，性能好，同时，提供了队列，集合等多种存储结构，方便队列维护。</p><p>另外一个问题，如何去重？这个的意思就是如何避免多台主机访问的request都不同，即让Reques队列中的请求都是不同的，那么就需要用到Redis提供的队列结构。Redis提供集合数据结构，在Redis集合中存储每个Request的指纹，在向Request队列中加入Request时首先验证指纹是否存在。如果存在，则不加入，如果不存在，则加入。</p><h2 id="环境配置"><a href="#环境配置" class="headerlink" title="环境配置"></a>环境配置</h2><p>目前已经有专门的python库实现了分布式架构。Scrapy-Redis库改写了Scrapy的调度器，队列等组件，可以方便的实现Scrapy分布式架构。</p><p>Scrapy-Redis链接：<a href="https://github.com/rolando/scrapy-redis">https://github.com/rolando/scrapy-redis</a></p><p>由于暂时只有单机环境，所以接下来：<br><a href="https://www.cnblogs.com/hd-zg/p/6960955.html?utm_source=itdadao&amp;utm_medium=referral" target="_blank" rel="noopener">scrapy进行分布式爬虫 - thinker1017 - 博客园</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Scrapy&quot;&gt;&lt;a href=&quot;#Scrapy&quot; class=&quot;headerlink&quot; title=&quot;Scrapy&quot;&gt;&lt;/a&gt;Scrapy&lt;/h1&gt;&lt;h1 id=&quot;安装&quot;&gt;&lt;a href=&quot;#安装&quot; class=&quot;headerlink&quot; title=&quot;安装&quot;&gt;&lt;
      
    
    </summary>
    
      <category term="知识总结" scheme="https://github.com/zdkswd/categories/%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/"/>
    
    
      <category term="Python" scheme="https://github.com/zdkswd/tags/Python/"/>
    
      <category term="爬虫" scheme="https://github.com/zdkswd/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>让爬虫程序更像人类用户的行为</title>
    <link href="https://github.com/zdkswd/2019/02/08/%E8%AE%A9%E7%88%AC%E8%99%AB%E7%A8%8B%E5%BA%8F%E6%9B%B4%E5%83%8F%E4%BA%BA%E7%B1%BB%E7%94%A8%E6%88%B7%E7%9A%84%E8%A1%8C%E4%B8%BA/"/>
    <id>https://github.com/zdkswd/2019/02/08/让爬虫程序更像人类用户的行为/</id>
    <published>2019-02-08T06:27:32.000Z</published>
    <updated>2019-02-08T06:27:25.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="让爬虫程序更像人类用户的行为"><a href="#让爬虫程序更像人类用户的行为" class="headerlink" title="让爬虫程序更像人类用户的行为"></a>让爬虫程序更像人类用户的行为</h1><p><a href="https://blog.csdn.net/c406495762/article/details/72793480" target="_blank" rel="noopener">Python3网络爬虫(十一)：爬虫黑科技之让你的爬虫程序更像人类用户的行为(代理IP池等) - Jack-Cui - CSDN博客</a></p><h1 id="黑科技"><a href="#黑科技" class="headerlink" title="黑科技"></a>黑科技</h1><p>一些简单的方法让爬虫更像人类访问用户。</p><h2 id="构造合理的HTTP请求头"><a href="#构造合理的HTTP请求头" class="headerlink" title="构造合理的HTTP请求头"></a>构造合理的HTTP请求头</h2><h2 id="设置Cookie的学问"><a href="#设置Cookie的学问" class="headerlink" title="设置Cookie的学问"></a>设置Cookie的学问</h2><p>虽然 cookie 是一把双刃剑，但正确地处理 cookie 可以避免许多采集问题。网站会用 cookie 跟踪你的访问过程，如果发现了爬虫异常行为就会中断你的访问，比如特别快速地填写表单，或者浏览大量页面。虽然这些行为可以通过关闭并重新连接或者改变 IP 地址来伪装，但是如果 cookie 暴露了你的身份，再多努力也是白费。</p><p>在采集一些网站时 cookie 是不可或缺的。要在一个网站上持续保持登录状态，需要在多个页面中保存一个 cookie。有些网站不要求在每次登录时都获得一个新 cookie，只要保存一个旧的“已登录”的 cookie 就可以访问。</p><p> Cookie信息，也可以更具实际情况填写。不过requests已经封装好了很多操作，自动管理cookie，session保持连接。我们可以先访问某个目标网站，建立一个session连接之后，获取cookie。代码如下：<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%AE%A9%E7%88%AC%E8%99%AB%E7%A8%8B%E5%BA%8F%E6%9B%B4%E5%83%8F%E4%BA%BA%E7%B1%BB%E7%94%A8%E6%88%B7%E7%9A%84%E8%A1%8C%E4%B8%BA/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-08%20%E4%B8%8B%E5%8D%8812.54.51.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>使用 requests.Session 会话对象让你能够跨请求保持某些参数，它也会在同一个 Session 实例发出的所有请求之间保持 cookie， 期间使用 urllib3 的 connection pooling 功能。</p><p>因为 requests 模块不能执行 JavaScript，所以它不能处理很多新式的跟踪软件生成的 cookie，比如 Google Analytics，只有当客户端脚本执行后才设置 cookie（或者在用户浏览页面时基于网页事件产生 cookie，比如点击按钮）。要处理这些动作，需要用 Selenium 和 PhantomJS 包。</p><p>PhantomJS 是一个“无头”（headless）浏览器。它会把网站加载到内存并执行页面上的 JavaScript，但不会向用户展示网页的图形界面。将 Selenium 和 PhantomJS 结合在一起，就可以运行一个非常强大的网络爬虫了，可以处理 cookie、JavaScript、headers，以及任何你需要做的事情。</p><p>还可以调用 delete_cookie()、add_cookie() 和 delete_all_cookies() 方法来处理 cookie。另外，还可以保存 cookie 以备其他网络爬虫使用。</p><p>通过Selenium和PhantomJS，我们可以很好的处理一些需要事件执行后才能获得的cookie。</p><h2 id="正常的访问速度"><a href="#正常的访问速度" class="headerlink" title="正常的访问速度"></a>正常的访问速度</h2><p>有一些防护措施完备的网站可能会阻止你快速地提交表单，或者快速地与网站进行交互。即使没有这些安全措施，用一个比普通人快很多的速度从一个网站下载大量信息也可能让自己被网站封杀。</p><p>因此，虽然多进程程序可能是一个快速加载页面的好办法——在一个进程中处理数据，另一个进程中加载页面——但是这对编写好的爬虫来说是恐怖的策略。还是应该尽量保证一次加载页面加载且数据请求最小化。如果条件允许，尽量为每个页面访问增加一点儿时间间隔，即使你要增加两行代码：<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%AE%A9%E7%88%AC%E8%99%AB%E7%A8%8B%E5%BA%8F%E6%9B%B4%E5%83%8F%E4%BA%BA%E7%B1%BB%E7%94%A8%E6%88%B7%E7%9A%84%E8%A1%8C%E4%B8%BA/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-08%20%E4%B8%8B%E5%8D%881.07.52.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>合理控制速度是你不应该破坏的规则。过度消耗别人的服务器资源会让你置身于非法境地，更严重的是这么做可能会把一个小型网站拖垮甚至下线。拖垮网站是不道德的，是彻头彻尾的错误。所以请控制采集速度！</p><h2 id="注意隐含输入字段"><a href="#注意隐含输入字段" class="headerlink" title="注意隐含输入字段"></a>注意隐含输入字段</h2><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%AE%A9%E7%88%AC%E8%99%AB%E7%A8%8B%E5%BA%8F%E6%9B%B4%E5%83%8F%E4%BA%BA%E7%B1%BB%E7%94%A8%E6%88%B7%E7%9A%84%E8%A1%8C%E4%B8%BA/20170528154851498.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>用隐含字段阻止网络数据采集的方式主要有两种。</p><p>第一种是表单页面上的一个字段可以用服务器生成的随机变量表示。如果提交时这个值不在表单处理页面上，服务器就有理由认为这个提交不是从原始表单页面上提交的，而是由一个网络机器人直接提交到表单处理页面的。绕开这个问题的最佳方法就是，首先采集表单所在页面上生成的随机变量，然后再提交到表单处理页面。</p><p>第二种方式是“蜜罐”（honey pot）。如果表单里包含一个具有普通名称的隐含字段（设置蜜罐圈套），比如“用户名”（username）或“邮箱地址”（email address），设计不太好的网络机器人往往不管这个字段是不是对用户可见，直接填写这个字段并向服务器提交，这样就会中服务器的蜜罐圈套。服务器会把所有隐含字段的真实值（或者与表单提交页面的默认值不同的值）都忽略，而且填写隐含字段的访问用户也可能被网站封杀。</p><p>总之，有时检查表单所在的页面十分必要，看看有没有遗漏或弄错一些服务器预先设定好的隐含字段（蜜罐圈套）。如果你看到一些隐含字段，通常带有较大的随机字符串变量，那么很可能网络服务器会在表单提交的时候检查它们。另外，还有其他一些检查，用来保证这些当前生成的表单变量只被使用一次或是最近生成的（这样可以避免变量被简单地存储到一个程序中反复使用）。</p><h2 id="爬虫如何避开蜜罐"><a href="#爬虫如何避开蜜罐" class="headerlink" title="爬虫如何避开蜜罐"></a>爬虫如何避开蜜罐</h2><p>虽然在进行网络数据采集时用 CSS 属性区分有用信息和无用信息会很容易（比如，通过读取 id和 class 标签获取信息），但这么做有时也会出问题。如果网络表单的一个字段通过 CSS 设置成对用户不可见，那么可以认为普通用户访问网站的时候不能填写这个字段，因为它没有显示在浏览器上。如果这个字段被填写了，就可能是机器人干的，因此这个提交会失效。</p><p>这种手段不仅可以应用在网站的表单上，还可以应用在链接、图片、文件，以及一些可以被机器人读取，但普通用户在浏览器上却看不到的任何内容上面。访问者如果访问了网站上的一个“隐含”内容，就会触发服务器脚本封杀这个用户的 IP 地址，把这个用户踢出网站，或者采取其他措施禁止这个用户接入网站。实际上，许多商业模式就是在干这些事情。</p><p>通常元素可以通过三种不同的方式对用户进行隐藏。</p><ol><li>第一个链接是通过简单的 CSS 属性设置 display:none 进行隐藏；</li><li>电话号码字段 name=”phone” 是一个隐含的输入字段；</li><li>邮箱地址字段 name=”email” 是将元素向右移动 50 000 像素（应该会超出电脑显示器的边界）并隐藏滚动条。</li></ol><p>因为 Selenium 可以获取访问页面的内容，所以它可以区分页面上的可见元素与隐含元素。通过 is_displayed() 可以判断元素在页面上是否可见。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E8%AE%A9%E7%88%AC%E8%99%AB%E7%A8%8B%E5%BA%8F%E6%9B%B4%E5%83%8F%E4%BA%BA%E7%B1%BB%E7%94%A8%E6%88%B7%E7%9A%84%E8%A1%8C%E4%B8%BA/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-08%20%E4%B8%8B%E5%8D%881.41.18.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><h2 id="创建自己的代理IP池"><a href="#创建自己的代理IP池" class="headerlink" title="创建自己的代理IP池"></a>创建自己的代理IP池</h2><p>启用远程平台的人通常有两个目的：对更大计算能力和灵活性的需求，以及对可变 IP 地址的需求。</p><p>有一些网站会设置访问阈值，也就是说，如果一个IP访问速度超过这个阈值，那么网站就会认为，这是一个爬虫程序，而不是用户行为。为了避免远程服务器封锁IP，或者想加快爬取速度，一个可行的方法就是使用代理IP，我们需要做的就是创建一个自己的代理IP池。</p><p>思路：通过免费IP代理网站爬取IP，构建一个容量为100的代理IP池。从代理IP池中随机选取IP，在使用IP之前，检查IP是否可用。如果可用，使用该IP访问目标页面，如果不可用，舍弃该IP。当代理IP池中IP的数量小于20的时候，更新整个代理IP池，即重新从免费IP代理网站爬取IP，构建一个新的容量为100的代理IP池。</p><h1 id="检查列表"><a href="#检查列表" class="headerlink" title="检查列表"></a>检查列表</h1><p>如果一直被网站封杀找不到原因，可以从以下方面来检查。</p><h2 id="检查-JavaScript"><a href="#检查-JavaScript" class="headerlink" title="检查 JavaScript"></a>检查 JavaScript</h2><p>如果你从网络服务器收到的页面是空白的，缺少信息，或其遇到他不符合你预期的情况（或者不是你在浏览器上看到的内容），有可能是因为网站创建页面的 JavaScript 执行有问题。</p><h2 id="检查正常浏览器提交的参数"><a href="#检查正常浏览器提交的参数" class="headerlink" title="检查正常浏览器提交的参数"></a>检查正常浏览器提交的参数</h2><p>如果你准备向网站提交表单或发出 POST 请求，记得检查一下页面的内容，看看你想提交的每个字段是不是都已经填好，而且格式也正确。用 Chrome 浏览器的网络面板（快捷键 F12 打开开发者控制台，然后点击“Network”即可看到）查看发送到网站的 POST 命令，确认你的每个参数都是正确的。</p><h2 id="是否有合法的-Cookie？"><a href="#是否有合法的-Cookie？" class="headerlink" title="是否有合法的 Cookie？"></a>是否有合法的 Cookie？</h2><p>如果你已经登录网站却不能保持登录状态，或者网站上出现了其他的“登录状态”异常，请检查你的 cookie。确认在加载每个页面时 cookie 都被正确调用，而且你的 cookie 在每次发起请求时都发送到了网站上。</p><h2 id="IP-被封禁"><a href="#IP-被封禁" class="headerlink" title="IP 被封禁"></a>IP 被封禁</h2><p>如果你在客户端遇到了 HTTP 错误，尤其是 403 禁止访问错误，这可能说明网站已经把你的 IP 当作机器人了，不再接受你的任何请求。你要么等待你的 IP 地址从网站黑名单里移除，要么就换个 IP 地址。如果你确定自己并没有被封杀，那么再检查下面的内容： </p><ol><li>确认你的爬虫在网站上的速度不是特别快。快速采集是一种恶习，会对网管的服务器造成沉重的负担，还会让你陷入违法境地，也是 IP 被网站列入黑名单的首要原因。给你的爬虫增加延迟，让它们在夜深人静的时候运行。切记：匆匆忙忙写程序或收集数据都是拙劣项目管理的表现；应该提前做好计划，避免临阵慌乱。</li><li>还有一件必须做的事情：修改你的请求头！有些网站会封杀任何声称自己是爬虫的访问者。如果你不确定请求头的值怎样才算合适，就用你自己浏览器的请求头吧。</li><li>确认你没有点击或访问任何人类用户通常不能点击或接入的信息。</li><li>如果你用了一大堆复杂的手段才接入网站，考虑联系一下网管吧，告诉他们你的目的。试试发邮件到 webmaster@&lt; 域名 &gt; 或 admin@&lt; 域名 &gt;，请求网管允许你使用爬虫采集数据。管理员也是人嘛！</li></ol><p>使用免费的代理IP也是有局限的，就是不稳定。更好的方法是，花钱买一个可以动态切换IP的阿里云服务器，这样IP就可以无限动态变化了！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;让爬虫程序更像人类用户的行为&quot;&gt;&lt;a href=&quot;#让爬虫程序更像人类用户的行为&quot; class=&quot;headerlink&quot; title=&quot;让爬虫程序更像人类用户的行为&quot;&gt;&lt;/a&gt;让爬虫程序更像人类用户的行为&lt;/h1&gt;&lt;p&gt;&lt;a href=&quot;https://blog.
      
    
    </summary>
    
      <category term="知识总结" scheme="https://github.com/zdkswd/categories/%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/"/>
    
    
      <category term="Python" scheme="https://github.com/zdkswd/tags/Python/"/>
    
      <category term="爬虫" scheme="https://github.com/zdkswd/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>再议Python数据可视化技术</title>
    <link href="https://github.com/zdkswd/2019/02/08/%E5%86%8D%E8%AE%AEPython%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8A%80%E6%9C%AF/"/>
    <id>https://github.com/zdkswd/2019/02/08/再议Python数据可视化技术/</id>
    <published>2019-02-08T03:28:56.000Z</published>
    <updated>2019-02-08T03:28:28.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="再议Python数据可视化技术"><a href="#再议Python数据可视化技术" class="headerlink" title="再议Python数据可视化技术"></a>再议Python数据可视化技术</h1><h1 id="可视化视图分类"><a href="#可视化视图分类" class="headerlink" title="可视化视图分类"></a>可视化视图分类</h1><p>可视化视图可分为4类，比较，联系，构成和分布。</p><ol><li>比较：比较数据间各类别的关系，或者是它们随着时间的变化趋势，比如折线图。</li><li>联系：查看两个或两个以上变量的关系，比如散点图。</li><li>构成：每个部分占整体的百分比，或者是随着时间的百分比变化，比如饼图。</li><li>分布：关注单个变量，或者多个变量的分布情况，比如直方图。</li></ol><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E5%86%8D%E8%AE%AEPython%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8A%80%E6%9C%AF/4673a17085302cfe9177f8ee687ac675.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E5%86%8D%E8%AE%AEPython%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8A%80%E6%9C%AF/8ed2addb00a4329dd63bba669f427fd2.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;再议Python数据可视化技术&quot;&gt;&lt;a href=&quot;#再议Python数据可视化技术&quot; class=&quot;headerlink&quot; title=&quot;再议Python数据可视化技术&quot;&gt;&lt;/a&gt;再议Python数据可视化技术&lt;/h1&gt;&lt;h1 id=&quot;可视化视图分类&quot;&gt;&lt;a h
      
    
    </summary>
    
      <category term="知识总结" scheme="https://github.com/zdkswd/categories/%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/"/>
    
    
      <category term="Python" scheme="https://github.com/zdkswd/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>数据分析小总结</title>
    <link href="https://github.com/zdkswd/2019/02/07/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B0%8F%E6%80%BB%E7%BB%93/"/>
    <id>https://github.com/zdkswd/2019/02/07/数据分析小总结/</id>
    <published>2019-02-07T13:58:56.000Z</published>
    <updated>2019-02-07T14:00:32.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="数据分析小总结"><a href="#数据分析小总结" class="headerlink" title="数据分析小总结"></a>数据分析小总结</h1><h1 id="数据挖掘的十大算法"><a href="#数据挖掘的十大算法" class="headerlink" title="数据挖掘的十大算法"></a>数据挖掘的十大算法</h1><h2 id="分类算法"><a href="#分类算法" class="headerlink" title="分类算法"></a>分类算法</h2><p>C4.5，朴素贝叶斯（Naive Bayes），SVM，KNN，Adaboost，CART</p><h2 id="聚类算法"><a href="#聚类算法" class="headerlink" title="聚类算法"></a>聚类算法</h2><p>K-Means，EM</p><h2 id="关连分析"><a href="#关连分析" class="headerlink" title="关连分析"></a>关连分析</h2><p>Apriori<br> [‘eɪprɪ’ɔ:rɪ] 先验的<br>Aprion是一种挖掘关联规则( association rules)的算法,它通过挖掘频繁项集( frequent item sets)来揭示物品之间的关联关系,被广泛应用到商业挖掘和网络安全等领域中。频繁项集是指经常岀现在一起的物品的集合,关联规则暗示着两种物品之间可能存在很强的关系。</p><h2 id="连接分析"><a href="#连接分析" class="headerlink" title="连接分析"></a>连接分析</h2><p>PageRank<br>PageRank起源于论文影响力的计算方式,如果一篇文论被引入的次数越多,就代表这篇论文的影响力越强。同样 PageRank被 Google创造性地应用到了网页权重的计算中:当一个页面链岀的页面越多,说明这个页面的“参考文献”越多,当这个页面被链入的频率越高,说明这个页面被引用的次数越高。基于这个原理,我们可以得到网站的权重划分。</p><h1 id="Numpy之再回首"><a href="#Numpy之再回首" class="headerlink" title="Numpy之再回首"></a>Numpy之再回首</h1><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B0%8F%E6%80%BB%E7%BB%93/7ba74ca7776ac29a5dc94c272d72ff66.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h2 id="使用NumPy让Python科学计算更高效"><a href="#使用NumPy让Python科学计算更高效" class="headerlink" title="使用NumPy让Python科学计算更高效"></a>使用NumPy让Python科学计算更高效</h2><p>列表list的元素在系统内存中是分散存储的,而 NumPy数组存储在一个均匀连续的内存块中。这样数组计算遍历所有的元素,不像列表list还需要对內存地址进行査找,从而节省了计算资源。由之前的NumPy博客可得知详情。<br><a href="https://zdkswd.github.io/2018/10/28/Python%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E6%89%8B%E5%86%8C/" target="_blank" rel="noopener">Python数据科学手册 NumPy入门 | ZDK’s blog</a><br>然而一旦确定下来numpy数组后，就不能在增加元素了。</p><p>另外在内存访问模式中,缓存会直接把字节块从RAM加载到CPU寄存器中。因为数据连续的存储在内存中, NumPy直接利用现代CPU的矢量化指令计算,加载寄存器中的多个连续浮点数。</p><p>NumPy中的矩阵计算可以采用多线程的方式,充分利用多核CPU计算资源,大大提升了计算效率。python因为有GIL锁，因此多线程也只能使用一个处理器，但是numpy是例外，因为numpy内部是用C写的，不经过python解释器，因此它本身的矩阵运算(array operations)都可以使用多核。</p><p>除了使用 NumPy外,你还需要一些技巧来提升内存和提高计算资源的利用率。一个重要的规则就是:<strong>避免采用隐式拷贝,而是采用就地操作的方式</strong>。举个例子,如果我想让一个数值x是原来的两倍,可以直接写成x <em> =2,而不要写成y=x </em> 2。这样速度能快2倍甚至更多。</p><p>NumPy里有两个重要的对象：ndarray（N-dimensional array object）解决了多维数组问题。ufunc（universal function object）则是解决对数组进行处理的函数。</p><h3 id="ndarray对象"><a href="#ndarray对象" class="headerlink" title="ndarray对象"></a>ndarray对象</h3><p>axis为0是跨行（纵向），axis为1是跨列（横向）。</p><h3 id="创建数组"><a href="#创建数组" class="headerlink" title="创建数组"></a>创建数组</h3><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B0%8F%E6%80%BB%E7%BB%93/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-06%20%E4%B8%8B%E5%8D%885.15.05.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>运行结果<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B0%8F%E6%80%BB%E7%BB%93/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-06%20%E4%B8%8B%E5%8D%885.15.21.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><h3 id="结构数组"><a href="#结构数组" class="headerlink" title="结构数组"></a>结构数组</h3><p>如果想统计一个班级里面学生的姓名，年龄，以及语文数学英语成绩，可以用数组的下标来代表不同的字段，但是这样不显性，可以自己定义一个数据结构。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B0%8F%E6%80%BB%E7%BB%93/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-06%20%E4%B8%8B%E5%8D%885.34.17.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>运行结果<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B0%8F%E6%80%BB%E7%BB%93/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-06%20%E4%B8%8B%E5%8D%885.34.30.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>注意在定义数组时，用array中指定了结构数组的类型dtype=persontype</p><h3 id="ufunc运算"><a href="#ufunc运算" class="headerlink" title="ufunc运算"></a>ufunc运算</h3><p>它能对数组中<strong>每个元素</strong>进行函数操作，NumPy中很多ufunc函数计算速度非常快，因为都是采用C语言实现的。</p><h3 id="连续数组的创建"><a href="#连续数组的创建" class="headerlink" title="连续数组的创建"></a>连续数组的创建</h3><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B0%8F%E6%80%BB%E7%BB%93/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-06%20%E4%B8%8B%E5%8D%885.56.36.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>np. arange和np.linspace起到的作用是一样的,都是创建等差数组。这两个数组的结果x1,x2都是[1 3 5 7 9]。结果相同,但是创建的方式是不同的。</p><p>arange（）类似内置函数 range（）,通过指定<strong>初始值、终值、步长</strong>来创建等差数列的一维数组,默认是不包括终值的。</p><p>linspace是 linear space的缩写,代表线性等分向量的含义。 linspace（）通过指定<strong>初始值，终值、元素个数</strong>来创建等差数列的一维数组,默认是包括终值的。</p><h3 id="算术运算"><a href="#算术运算" class="headerlink" title="算术运算"></a>算术运算</h3><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B0%8F%E6%80%BB%E7%BB%93/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-06%20%E4%B8%8B%E5%8D%886.22.30.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>运行结果<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B0%8F%E6%80%BB%E7%BB%93/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-06%20%E4%B8%8B%E5%8D%886.22.45.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>取余函数里，既可以用remainder，也可以用mod，结果是一样的。</p><h3 id="统计函数"><a href="#统计函数" class="headerlink" title="统计函数"></a>统计函数</h3><p>统计数组矩阵的最大值函数amax（），最小值函数amin（）<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B0%8F%E6%80%BB%E7%BB%93/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-06%20%E4%B8%8B%E5%8D%886.55.53.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>运行结果<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B0%8F%E6%80%BB%E7%BB%93/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-06%20%E4%B8%8B%E5%8D%886.56.10.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><p>统计最大值与最小值之差ptp（）<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B0%8F%E6%80%BB%E7%BB%93/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-06%20%E4%B8%8B%E5%8D%886.59.14.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>运行结果<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B0%8F%E6%80%BB%E7%BB%93/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-06%20%E4%B8%8B%E5%8D%886.59.30.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><h3 id="统计数组的百分位数percentile（）"><a href="#统计数组的百分位数percentile（）" class="headerlink" title="统计数组的百分位数percentile（）"></a>统计数组的百分位数percentile（）</h3><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B0%8F%E6%80%BB%E7%BB%93/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-06%20%E4%B8%8B%E5%8D%887.00.54.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>运行结果<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B0%8F%E6%80%BB%E7%BB%93/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-06%20%E4%B8%8B%E5%8D%887.01.09.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>percentile（）代表着第p个百分位数,这里p的取值范围是0-100,如果p=0,那么就是求最小值,如果p=50就是求平均值,如果p=100就是求最大值。同样你也可以求得在axs=0和axis=1两个轴上的p%的百分位数。</p><h3 id="统计数组中的中位数median（），平均数mean（）"><a href="#统计数组中的中位数median（），平均数mean（）" class="headerlink" title="统计数组中的中位数median（），平均数mean（）"></a>统计数组中的中位数median（），平均数mean（）</h3><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B0%8F%E6%80%BB%E7%BB%93/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-06%20%E4%B8%8B%E5%8D%887.03.35.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>运行结果<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B0%8F%E6%80%BB%E7%BB%93/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-06%20%E4%B8%8B%E5%8D%887.03.52.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><h3 id="统计数组中的加权平均值average（）"><a href="#统计数组中的加权平均值average（）" class="headerlink" title="统计数组中的加权平均值average（）"></a>统计数组中的加权平均值average（）</h3><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B0%8F%E6%80%BB%E7%BB%93/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-06%20%E4%B8%8B%E5%8D%887.05.59.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>运行结果<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B0%8F%E6%80%BB%E7%BB%93/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-06%20%E4%B8%8B%E5%8D%887.06.17.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><h3 id="统计数组中的标准差std（），方差var（）"><a href="#统计数组中的标准差std（），方差var（）" class="headerlink" title="统计数组中的标准差std（），方差var（）"></a>统计数组中的标准差std（），方差var（）</h3><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B0%8F%E6%80%BB%E7%BB%93/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-06%20%E4%B8%8B%E5%8D%887.09.33.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>运行结果<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B0%8F%E6%80%BB%E7%BB%93/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-06%20%E4%B8%8B%E5%8D%887.09.55.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><h3 id="NumPy排序"><a href="#NumPy排序" class="headerlink" title="NumPy排序"></a>NumPy排序</h3><p>这些排序算法在NumPy中实现起来其实非常简单,一条语句就可以搞定。这里你可以使用sort函数,sort(a,axis=-1,kind= quicksort, order=None),默认情况下使用的是快速排序;在kind里,可以指定 quicksort、 mergesort、 heapsort分别表示快速排序、合并排序、堆排序。同样axis默认是-1,即沿着数组的最后一个轴迸行排序,也可以取不同的axis轴,或者axis=None代表采用扁平化的方式作为一个向量进行排序。另外 <strong>order</strong>字段对于结构化的数组可以指定按照某个字段进行排序。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B0%8F%E6%80%BB%E7%BB%93/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-06%20%E4%B8%8B%E5%8D%887.18.49.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>运行结果<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B0%8F%E6%80%BB%E7%BB%93/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-06%20%E4%B8%8B%E5%8D%887.19.07.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><h2 id="练习题"><a href="#练习题" class="headerlink" title="练习题"></a>练习题</h2><p>统计全班的成绩<br>假设一个团队里有5名学员,成绩如下表所示。你可以用 NumPy统计下这些人在语文、英语、数学中的平均成绩、最小成绩、最大成绩、方差、标准差。然后把这些人的总成绩排序,得出名次进行成绩输出。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B0%8F%E6%80%BB%E7%BB%93/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-06%20%E4%B8%8B%E5%8D%887.21.38.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br><a href="https://github.com/zdkswd/PythonCode/blob/master/%E7%BB%9F%E8%AE%A1%E5%85%A8%E7%8F%AD%E7%9A%84%E6%88%90%E7%BB%A9.py">PythonCode/统计全班的成绩.py at master · zdkswd/PythonCode · GitHub</a></p><h1 id="Pandas之再回首"><a href="#Pandas之再回首" class="headerlink" title="Pandas之再回首"></a>Pandas之再回首</h1><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B0%8F%E6%80%BB%E7%BB%93/74884960677548b08acdc919c13460cd.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>在NumPy中数据结构是围绕ndarray展开的，在Pandas中，核心数据结构是Series和DataFrame。Pandas的基础是NumPy。</p><h2 id="数据结构：Series和DataFrame"><a href="#数据结构：Series和DataFrame" class="headerlink" title="数据结构：Series和DataFrame"></a>数据结构：Series和DataFrame</h2><p><strong>Series是个定长的字典序列</strong>，说是定长是因为在存储时，相当于两个ndarray，这也是和字典结构最大的不同，因为在字典结构中元素的个数是不固定的。</p><p>Series有两个基本属性，Index和values。在Series结构中，index默认是0，1，2递增的整数序列，当然也可以自己来指定索引，index=[‘a’, ‘b’, ‘c’, ‘d’]。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B0%8F%E6%80%BB%E7%BB%93/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-07%20%E4%B8%8A%E5%8D%8810.47.12.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>运行结果<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B0%8F%E6%80%BB%E7%BB%93/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-07%20%E4%B8%8A%E5%8D%8810.47.49.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><p>还可以用字典方式来创建Series。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B0%8F%E6%80%BB%E7%BB%93/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-07%20%E4%B8%8A%E5%8D%8810.52.37.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>运行结果<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B0%8F%E6%80%BB%E7%BB%93/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-07%20%E4%B8%8A%E5%8D%8810.52.55.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><h3 id="DataFrame类型数据结构类似数据库表"><a href="#DataFrame类型数据结构类似数据库表" class="headerlink" title="DataFrame类型数据结构类似数据库表"></a>DataFrame类型数据结构类似数据库表</h3><p>数据库表<br>DataFrame是由相同索引的Series组成的字典类型。</p><blockquote><p>import pandas as pd<br>from pandas import Series, DataFrame<br>data = {‘Chinese’: [66, 95, 93, 90,80],’English’: [65, 85, 92, 88, 90],’Math’: [30, 98, 96, 77, 90]}<br>df1= DataFrame(data)<br>df2 = DataFrame(data, index=[‘ZhangFei’, ‘GuanYu’, ‘ZhaoYun’, ‘HuangZhong’, ‘DianWei’], columns=[‘English’, ‘Math’, ‘Chinese’])<br>print df1<br>print df2  </p></blockquote><p>df1的输出：<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B0%8F%E6%80%BB%E7%BB%93/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-07%20%E4%B8%8A%E5%8D%8811.39.55.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>df2的输出：<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B0%8F%E6%80%BB%E7%BB%93/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-07%20%E4%B8%8A%E5%8D%8811.40.22.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><h3 id="数据导入和输出"><a href="#数据导入和输出" class="headerlink" title="数据导入和输出"></a>数据导入和输出</h3><p>xlsx,csv</p><h2 id="数据清洗"><a href="#数据清洗" class="headerlink" title="数据清洗"></a>数据清洗</h2><p>数据清洗是数据准备中必不可少的环节，Pandas也为我们提供了数据清洗的工具。</p><h3 id="删除DataFrame中不必要的行或列"><a href="#删除DataFrame中不必要的行或列" class="headerlink" title="删除DataFrame中不必要的行或列"></a>删除DataFrame中不必要的行或列</h3><p>Pandas中drop（）方法来删除我们不想要的列或行。<br>将语文这列删掉<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B0%8F%E6%80%BB%E7%BB%93/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-07%20%E4%B8%8A%E5%8D%8811.49.14.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>将张飞这行删掉<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B0%8F%E6%80%BB%E7%BB%93/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-07%20%E4%B8%8A%E5%8D%8811.49.33.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><h3 id="重命名列名columns，让列表名更容易识别"><a href="#重命名列名columns，让列表名更容易识别" class="headerlink" title="重命名列名columns，让列表名更容易识别"></a>重命名列名columns，让列表名更容易识别</h3><p>例如把列名Chinese改为YuWen，English改为YingYu。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B0%8F%E6%80%BB%E7%BB%93/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-07%20%E4%B8%8A%E5%8D%8811.51.35.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><h3 id="去重复的值"><a href="#去重复的值" class="headerlink" title="去重复的值"></a>去重复的值</h3><p>数据采集可能存在重复的行,这时只要使用 drop_duplicates就会自动把重复的行去掉。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B0%8F%E6%80%BB%E7%BB%93/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-07%20%E4%B8%8A%E5%8D%8811.52.28.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><h3 id="格式问题"><a href="#格式问题" class="headerlink" title="格式问题"></a>格式问题</h3><h4 id="更改数据格式"><a href="#更改数据格式" class="headerlink" title="更改数据格式"></a>更改数据格式</h4><p>这是个比较常用的操作,因为很多时候数据格式不规范,我们可以使用 astype函数来规范数据格式,比如我们把 Chinese字段的值改成str类型,或者int64可以这么写。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B0%8F%E6%80%BB%E7%BB%93/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-07%20%E4%B8%8A%E5%8D%8811.55.14.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><h4 id="数据间的空格"><a href="#数据间的空格" class="headerlink" title="数据间的空格"></a>数据间的空格</h4><p>有时候我们先把格式转成了str类型,是为了方便对数据进行操作,这时想要删除数据间的空格,我们就可以使用strip函数。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B0%8F%E6%80%BB%E7%BB%93/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-07%20%E4%B8%8B%E5%8D%8812.06.53.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>想要删除特殊符号，同样可以使用strip函数，比如删除美元符号。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B0%8F%E6%80%BB%E7%BB%93/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-07%20%E4%B8%8B%E5%8D%8812.18.49.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><h4 id="大小写转换"><a href="#大小写转换" class="headerlink" title="大小写转换"></a>大小写转换</h4><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B0%8F%E6%80%BB%E7%BB%93/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-07%20%E4%B8%8B%E5%8D%8812.19.29.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h4 id="查找空值"><a href="#查找空值" class="headerlink" title="查找空值"></a>查找空值</h4><p>数据量大的情况下，有些字段存在空值NaN的可能，这时需要Pandas中的isnull函数进行查找。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B0%8F%E6%80%BB%E7%BB%93/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-07%20%E4%B8%8B%E5%8D%8812.21.17.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>如果想看哪些地方存在空值NaN，可以针对数据表df进行df.isnull()<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B0%8F%E6%80%BB%E7%BB%93/5b52bca4eb6f00d51f72dcc5c6ce2afe.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>如果想要知道哪列存在空值，可以使用df.isnull().any()<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B0%8F%E6%80%BB%E7%BB%93/89cb71afc4f54a11ce1d4d05cd46bb03.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><h2 id="使用apply函数对数据进行清洗"><a href="#使用apply函数对数据进行清洗" class="headerlink" title="使用apply函数对数据进行清洗"></a>使用apply函数对数据进行清洗</h2><p>apply是Pandas中<strong>自由度非常高的函数</strong>，使用频率也非常高。<br>比如想对name列的数值都进行大写转化：<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B0%8F%E6%80%BB%E7%BB%93/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-07%20%E4%B8%8B%E5%8D%8812.59.06.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>可以定义个函数，在apply中进行使用。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B0%8F%E6%80%BB%E7%BB%93/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-07%20%E4%B8%8B%E5%8D%881.00.06.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>还可以定义更复杂的函数<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B0%8F%E6%80%BB%E7%BB%93/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-07%20%E4%B8%8B%E5%8D%881.07.27.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>其中axis=1代表按照列为轴进行操作,axis=0代表按照行为轴进行操作,args是传递的两个参数,即n=2,m=3,在plus函数中使用到了n和m,从而生成新的df。</p><h2 id="数据统计"><a href="#数据统计" class="headerlink" title="数据统计"></a>数据统计</h2><p>在数据清洗后，就要对数据进行统计了。Pandas和NumPy一样都有常用的统计函数，如果遇到空值NaN，会自动排除。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B0%8F%E6%80%BB%E7%BB%93/343ba98c1322dc0c013e07c87b157a00.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B0%8F%E6%80%BB%E7%BB%93/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-07%20%E4%B8%8B%E5%8D%889.39.43.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B0%8F%E6%80%BB%E7%BB%93/e4a7a208a11d60dbcda6f3dbaff9a583.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h2 id="数据表合并"><a href="#数据表合并" class="headerlink" title="数据表合并"></a>数据表合并</h2><p>创建两个DataFrame。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B0%8F%E6%80%BB%E7%BB%93/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-07%20%E4%B8%8B%E5%8D%889.41.32.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>使用的是merge（）函数，有五种形式。</p><h3 id="基于指定列进行连接"><a href="#基于指定列进行连接" class="headerlink" title="基于指定列进行连接"></a>基于指定列进行连接</h3><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B0%8F%E6%80%BB%E7%BB%93/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-07%20%E4%B8%8B%E5%8D%889.42.44.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B0%8F%E6%80%BB%E7%BB%93/220ce1ea19c8f6f2668d3a8122989c2f.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h3 id="inner内连接"><a href="#inner内连接" class="headerlink" title="inner内连接"></a>inner内连接</h3><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B0%8F%E6%80%BB%E7%BB%93/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-07%20%E4%B8%8B%E5%8D%889.44.50.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B0%8F%E6%80%BB%E7%BB%93/220ce1ea19c8f6f2668d3a8122989c2f%202.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h3 id="left左连接"><a href="#left左连接" class="headerlink" title="left左连接"></a>left左连接</h3><p>左连接是第一个DataFrame为主进行连接，第二个DataFrame作为补充。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B0%8F%E6%80%BB%E7%BB%93/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-07%20%E4%B8%8B%E5%8D%889.46.05.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B0%8F%E6%80%BB%E7%BB%93/9091a7406d5aa7a2980328d587fb42ac.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h3 id="right右连接"><a href="#right右连接" class="headerlink" title="right右连接"></a>right右连接</h3><p>右连接是以第二个DataFrame为主进行的连接，第一个DataFrame作为补充。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B0%8F%E6%80%BB%E7%BB%93/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-07%20%E4%B8%8B%E5%8D%889.47.34.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B0%8F%E6%80%BB%E7%BB%93/10f9f22f66f3745381d85d760f857baf.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h3 id="outer外连接"><a href="#outer外连接" class="headerlink" title="outer外连接"></a>outer外连接</h3><p>外连接相当于求两个DataFrame的并集。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B0%8F%E6%80%BB%E7%BB%93/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-07%20%E4%B8%8B%E5%8D%889.48.28.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B0%8F%E6%80%BB%E7%BB%93/6737f6d4d66af0d75734cd140b5d198c.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h2 id="用SQL方式打开Pandas"><a href="#用SQL方式打开Pandas" class="headerlink" title="用SQL方式打开Pandas"></a>用SQL方式打开Pandas</h2><p>pandasql。<br>pandas中的主要函数是sqdf,它接收两个参数:一个SQL查询语句,还有一组环境变量globals（）或 locals（）。这样我们就可以在 Python里,直接用SQL语句中对 Data Frame进行操作,举个例子。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B0%8F%E6%80%BB%E7%BB%93/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-07%20%E4%B8%8B%E5%8D%889.50.22.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>运行结果<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B0%8F%E6%80%BB%E7%BB%93/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-07%20%E4%B8%8B%E5%8D%889.51.58.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>其中lambda是用来定义一个匿名函数的。具体形式为：<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/media/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B0%8F%E6%80%BB%E7%BB%93/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-07%20%E4%B8%8B%E5%8D%889.53.16.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>argument_list 是参数列表，expression是关于参数的表达式，会根据expression表达式计算结果进行输出返回。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;数据分析小总结&quot;&gt;&lt;a href=&quot;#数据分析小总结&quot; class=&quot;headerlink&quot; title=&quot;数据分析小总结&quot;&gt;&lt;/a&gt;数据分析小总结&lt;/h1&gt;&lt;h1 id=&quot;数据挖掘的十大算法&quot;&gt;&lt;a href=&quot;#数据挖掘的十大算法&quot; class=&quot;header
      
    
    </summary>
    
      <category term="知识总结" scheme="https://github.com/zdkswd/categories/%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/"/>
    
    
      <category term="Python" scheme="https://github.com/zdkswd/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>Linux命令行</title>
    <link href="https://github.com/zdkswd/2019/02/05/Linux%E5%91%BD%E4%BB%A4%E8%A1%8C/"/>
    <id>https://github.com/zdkswd/2019/02/05/Linux命令行/</id>
    <published>2019-02-05T11:33:32.000Z</published>
    <updated>2019-02-05T13:20:50.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Linux命令行"><a href="#Linux命令行" class="headerlink" title="Linux命令行"></a>Linux命令行</h1><p>为了更好的使用实验室的服务器，特此来进行总结。</p><h2 id="后台运行"><a href="#后台运行" class="headerlink" title="后台运行"></a>后台运行</h2><p>在当前终端运行一个脚本时，该终端会不占用而无法进行其他工作。比如在训练模型是想要查看GPU的使用情况，或者断开与远程服务器的连接但保持训练的进行。这时我们就需要借助 tmux 这个工具。tmux 是一个终端选择器（terminal multiplexer），它可以让用户在一个终端窗口中控制多个终端。对于连接远程服务器的工作有很大的帮助。</p><blockquote><pre><code>$ tmux                      # create a new terminal session  </code></pre></blockquote><p>在创建了新的 terminal session 之后，窗口会显示新的终端界面。想要退出新的终端界面，则可以在使用 ctrl+b 前缀之后再按 d （表示detached），这样就返回最初的终端。现在可以查看一共创建过多少个 terminal session：</p><blockquote><pre><code>$ tmux ls                   # list all terminal sessions  </code></pre></blockquote><p>键入以上的命令之后就可以看到下面的一段显示。</p><blockquote><p>0: 1 windows (created Mon Apr  2 17:19:35 2018) [100x34]<br>1: 1 windows (created Mon Apr  2 19:01:14 2018) [100x34]  </p></blockquote><p>每一行就是一个 terminal session， 最前面的就是它们的索引。我们可以通过这些索引来进入我们想要的 session。</p><blockquote><p>$ tmux attach -t 0  </p></blockquote><p>这样就可以进入上面编号为0的 session 查看程序的执行进度或者执行其他操作。运用 tmux 能够保证进程在断开与服务器连接之后依然正常运行。</p><h2 id="GPU使用"><a href="#GPU使用" class="headerlink" title="GPU使用"></a>GPU使用</h2><blockquote><p>nvidia-smi 可以查看GPU使用情况<br>export CUDA_VISIBLE_DEVICES=0 指定可用的GPU资源，防止占用所有GPU<br>的显存  </p></blockquote><h2 id="远程查看server端的log"><a href="#远程查看server端的log" class="headerlink" title="远程查看server端的log"></a>远程查看server端的log</h2><p>tensorboard的log输出的位置。</p><blockquote><p>$ tensorboard –log path_to_log_files  </p></blockquote><p>然后在本地的浏览器中输入server的地址再加上port的编号即可。</p><h2 id="screen"><a href="#screen" class="headerlink" title="screen"></a>screen</h2><p>在SSH远程连接被断开或者关闭的时候，系统里面的下载程序还能继续运行。<br>解决方法：<br>1.安装screen<br>实验室服务器已安装此步跳过。<br>2 创建screen会话，自定义screen虚拟终端的名称，test可以改为想要的名称</p><blockquote><p>screen -S  test</p></blockquote><p>3 查看之前的会话虚拟终端</p><blockquote><p>screen -r test</p></blockquote><p>查看所有screen会话</p><blockquote><p>screen -ls</p></blockquote><p>保存当前的screen会话<br>按键盘上的ctrl＋a，然后再按d</p><p>退出screen</p><blockquote><p>exit</p></blockquote><p>删除会话</p><blockquote><p>screen -wipe test</p></blockquote><h2 id="Linux查看物理CPU个数、核数、逻辑CPU个数"><a href="#Linux查看物理CPU个数、核数、逻辑CPU个数" class="headerlink" title="Linux查看物理CPU个数、核数、逻辑CPU个数"></a>Linux查看物理CPU个数、核数、逻辑CPU个数</h2><p>查看物理CPU个数</p><blockquote><p>cat /proc/cpuinfo| grep “physical id”| sort| uniq| wc -l</p></blockquote><p>查看每个物理CPU中core的个数(即核数)</p><blockquote><p>cat /proc/cpuinfo| grep “cpu cores”| uniq</p></blockquote><p>查看逻辑CPU的个数</p><blockquote><p>cat /proc/cpuinfo| grep “processor”| wc -l</p></blockquote><p>查看CPU信息（型号）</p><blockquote><p>cat /proc/cpuinfo | grep name | cut -f2 -d: | uniq -c</p></blockquote><p>注，根据之前博客中提到的，/proc目录本身是一个虚拟文件系统，放置的数据都是在内存当中，例如系统核心，进程信息，周边设备的状态及网络状态。这个目录下的数据都在内存当中。</p><p>物理cpu个数、核数、逻辑cpu数的概念<br><strong>物理cpu数</strong>：主板上实际插入的cpu数量，可以数不重复的 physical id 有几个（physical id）。<br><strong>cpu核数</strong>：单块CPU上面能处理数据的芯片组的数量，如双核、四核等。（cpu cores）<br><strong>逻辑cpu数</strong>：一般情况下，逻辑cpu=物理CPU个数×每颗核数，如果不相等的话，则表示服务器的CPU支持超线程技术（HT：简单来说，它可使处理器中的1 颗内核如2 颗内核那样在操作系统中发挥作用。这样一来，操作系统可使用的执行资源扩大了一倍，大幅提高了系统的整体性能，此时逻辑cpu=物理CPU个数×每颗核数x2）</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Linux命令行&quot;&gt;&lt;a href=&quot;#Linux命令行&quot; class=&quot;headerlink&quot; title=&quot;Linux命令行&quot;&gt;&lt;/a&gt;Linux命令行&lt;/h1&gt;&lt;p&gt;为了更好的使用实验室的服务器，特此来进行总结。&lt;/p&gt;
&lt;h2 id=&quot;后台运行&quot;&gt;&lt;a h
      
    
    </summary>
    
      <category term="知识总结" scheme="https://github.com/zdkswd/categories/%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/"/>
    
    
      <category term="Linux" scheme="https://github.com/zdkswd/tags/Linux/"/>
    
  </entry>
  
</feed>
